[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nTime\nLocation (Room)\n\n\n\n\n25.10.2023\nWednesdays, 11:30 - 13:00\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nDepending on the students‚Äô feedback in the survey before the course starts, the course can be held in (incl.¬†presentations, pitches, discussions, etc.) English or German.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer."
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nTime\nLocation (Room)\n\n\n\n\n25.10.2023\nWednesdays, 11:30 - 13:00\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nDepending on the students‚Äô feedback in the survey before the course starts, the course can be held in (incl.¬†presentations, pitches, discussions, etc.) English or German.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer."
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nIn this seminar, students are introduced to working with digital behavioral data (DBD). DBD refer to digital traces of human behavior that are knowingly or unknowingly left in online environments (e.g.¬†social media, messengers, entertainment media, or digital collaboration tools). These rich data is increasingly available to social scientific research in the public interest, but can also be used to derive strategic insights for business decisions.\nStudents learn how to work with DBD alongside the entire research process, from data collection, preprocessing and analysis, to reporting and provision (e.g.¬†via open science tools). Students first get a comprehensive overview of the ways in which DBD can be collected (e.g., API scraping, usage logging, mock-up virtual environments, or data donations), as well as the requirements for data protection, research ethics, and data quality. Afterwards, students practice and apply their newly acquired knowledge in small projects on use cases from media and communication research. In doing so, they learn important computer-based methods with which large digital behavioral data sets (e.g.¬†texts, images, usage behavior logs) can be processed and analyzed. By completing this module, participants will get an up-to-date overview and practical insights into how the potential of observational data (digital traces) can be used to better understand the behavior of media users in digital environments."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will\n\noverview and understand central opportunities of DBD and accompanying challenges for data collection and preprocessing\nevaluate the strengths and weaknesses of different ways of collecting DBD\nget to know and understand central requirements for data protection, research ethics, and data quality\nget to know and overview key computational social science methods to analyze DBD\npractice and apply knowledge on DBD, statistics, and data analysis in small projects of their own"
  },
  {
    "objectID": "course-syllabus.html#recommended-prerequisites",
    "href": "course-syllabus.html#recommended-prerequisites",
    "title": "Syllabus",
    "section": "Recommended prerequisites",
    "text": "Recommended prerequisites\n\nInterest in social scientific perspectives on media, communication, and digital technologies.\nBasic knowledge of working with statistical software such as Stata, R, Python, or SPSS is required.\nStudents are recommended, but not required, to also visit the lecture Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing."
  },
  {
    "objectID": "course-syllabus.html#organization-of-the-course",
    "href": "course-syllabus.html#organization-of-the-course",
    "title": "Syllabus",
    "section": "Organization of the course",
    "text": "Organization of the course\nRegistration for the course takes place via  StudOn. There you will receive the first information and instructions. Please make sure that\n\nyou complete the short survey before the seminar begins.\nyou check if you have received the invitation and joined  Zulip.\n\nAll slides, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website. I will regularly send out course announcements by e-mail or Zulip, make sure to check one or the other of these regularly."
  },
  {
    "objectID": "course-syllabus.html#preliminary-schedule",
    "href": "course-syllabus.html#preliminary-schedule",
    "title": "Syllabus",
    "section": "(Preliminary) Schedule",
    "text": "(Preliminary) Schedule\n\n\n\n\n\n\nImportant information\n\n\n\n\nPlease note that this is a provisional schedule. Part of the Kick-Off session is the presentation, discussion and voting on different project ideas.\nAll sessions marked with a üî® are hands-on sessions actively working with R.\nAll sessions marked with a üìö or üì¶ are presentation sessions where a group of students will give a detailed presentation.\n\n\n\n\n\n\nSession\nDatum\nTopic\n\n\n\n\n\n\nIntroduction\n\n\n1\n25.10.2023\nKick-Off\n\n\n-\n01.11.2023\nüéÉ Holiday (No Lecture)\n\n\n2\n08.11.2023\nDBD: Overview\n\n\n3\n15.11.2023\nüî® Working with R\n\n\n\nüìÇ Project 1\nAnalysis of media content\n\n\n4\n22.11.2023\nüìö Digital disconnection\n\n\n5\n29.11.2023\nüì¶ Data collection methods\n\n\n6\n06.12.2023\nüî® Text as data\n\n\n7\n13.12.2023\nüìä Presentation & Discussion\n\n\n8\n20.12.2023\nBuffer Session\n\n\n-\n-\nüéÑChristmas Break (No Lecture)\n\n\n\nüìÇ Project 2\nAnalysis of media usage\n\n\n9\n11.01.2024\nüìö Media habits & routines\n\n\n10\n18.01.2024\nüì¶ Data donations methods\n\n\n11\n25.01.2024\nüî® Working data logs\n\n\n12\n02.02.2024\nüìä Presentation & Discussion\n\n\n13\n08.02.2024\nüèÅ Recap, Evaluation & Discussion\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor an even more detailed overview of the course schedule as well as the linked content of the individual sessions (e.g.¬†slides or literature for the respective presentation), please see Schedule.\n\n\nThe course consists of three main parts:\n\nPart I: Introduction\nThe first three sessions form the (theoretical) basis for the course.\n\nThe kick-off session is mainly for getting to know each other and organizing the course.¬†\nThe second session is to give you an extended introduction DBD, including challenges and important frameworks.¬†\nThe third session is about practical work with R and RStudio. ¬†\n\n\n\nPart II: Analysis of media content (Project 1)\n\nThe focus of the second section is the observation and analysis of (written) social media content (e.g.¬†post and/or hashtags) with the help of different methods (e.g.¬†topic modeling, sentiment analysis etc.).\n\n\n\nPart III: Analysis of media usage (Project 2)\n\nThe third part is about the effects of media use. Here you will learn how to process and analyze logging data and how to link it to survey data with the help of a survey conducted on the participants of the course during the first months of the semester."
  },
  {
    "objectID": "course-syllabus.html#sessions",
    "href": "course-syllabus.html#sessions",
    "title": "Syllabus",
    "section": "Sessions",
    "text": "Sessions\nThe goal of the sessions is to be as interactive as possible. In general, the sessions consist of two parts. In the first part (¬± 30 - 45 minutes) at the beginning of the session, there are usually presentations (including discussion), which are more or less detailed depending on the stage of the project. The second part (¬± 45 - 60 minutes) consists of a group activity (with concluding discussion), which should either be about deepening the presentation content or about independent work on one‚Äôs own or the group project.\nMy role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. You are expected to bring a laptop to each class so that you can take part in the in-session exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone.\n\nWhere to ask questions\n\nIf you have a question during the lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nAny general questions about session content, assignments or about the project should be posted on Zulip, so that everyone can benefit from the answers. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question, I encourage you to respond!\nE-mails should be reserved for personal matters."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nIn order to obtain credits and a grade, participants are required to\n\nattend regularly (at least 80% of the sessions) and participate actively. A maximum of two sessions can be missed without excuse. Absence in further sessions can only be excused in case of illness (i. e. with a medical certificate).\ncomplete various assignments as part of a portfolio. The type and scope of the assignments depends on the number of participants and the project(s). Detailed information can be found in the section Assignments."
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\n\n\n\n\n\n\nTL;DR\n\n\n\nDo not cheat!\n\n\nFor general information on formatting, style, citation, appendices, wording of the affidavit, etc., see our Guide to Academic Writing.\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course‚Äôs policy is that you may make use of any online resources (e.g.¬†StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\nPolicy on use of generative artificial intelligence (AI):\nYou should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course1: (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n‚úÖ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n‚ùå AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content."
  },
  {
    "objectID": "course-syllabus.html#recommended-textbooks",
    "href": "course-syllabus.html#recommended-textbooks",
    "title": "Syllabus",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of Computational Social Science, Volume 1: Theory, Case Studies and Ethics (1st ed.). Routledge. https://doi.org/10.4324/9781003024583\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of computational social science, volume 2. Routledge. https://doi.org/10.4324/9781003025245\nHaim, M. (2023). Computational Communication Science: Eine Einf√ºhrung. Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-40171-9\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press."
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D‚Ü©Ô∏é"
  },
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\nüîó R for Data Science, 2nd Edition\nüîó Data Visualization: A Practical Introduction\nüîó Tidy modeling with R\nüîó Text Mining with R"
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann"
  },
  {
    "objectID": "exercise/exercise-10.html",
    "href": "exercise/exercise-10.html",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-10.html#background",
    "href": "exercise/exercise-10.html#background",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-10.html#preparation",
    "href": "exercise/exercise-10.html#preparation",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-10.html#import-and-process-the-data",
    "href": "exercise/exercise-10.html#import-and-process-the-data",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\nGet twitter data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nDTM/DFM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  mutate(across(text, ~str_replace_all(., \"#digitaldetox\", \"\"))) %&gt;% \n  select(-c(detox_dy, retweet_dy)) %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\n\n\nGet topic model (data)\n\n# TPM data\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))\n\n# Base data with topics \ntweets_detox_topics &lt;- qs::qread(here(\"local_data/tweets-digital-detox-topics.qs\"))"
  },
  {
    "objectID": "exercise/exercise-10.html#exercises",
    "href": "exercise/exercise-10.html#exercises",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "üìã Exercises",
    "text": "üìã Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the ‚ÄúRun all chunks above‚Äù-button  of the next chunk.\nYou can choose to solve some exercises using either the tidytext or quanteda functions. As the work steps differ, please select the tab with your preferred package.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\nüìã Exercise 1: Hashtag co-occurence\n\n\n\n\n\n\nObjective(s)\n\n\n\nRecreate the network plot from the session without the token #digitaldetox\n\n\n\nwith tidytextwith quanteda\n\n\n\nCreate new dataset tweets_dfm_cleaned\n\nBased on the dataset tweets_detox,\n\nUse mutate() and create the variables\n\ntext that removes the hashtag \"#digitaldetox\" using str_remove_all() and\nhashtag that extracts hashtags from the variable text with the help of str_extract_all() (and the pattern \"#\\\\S+\").\n\nTokenize the data by using only unnest() on the variable hashtag.\nSummarize occurrences using count(tweet_id, hashtags).\nConvert to DFM using cast_dfm(tweet_id, hashtags, n).\n\nSave this transformation by creating a new dataset with the name tweets_dfm_cleaned.\n\nCreate new dataset top_hashtags_tidy\n\nBased on the dataset tweets_detox,\n\nRepeat steps 1. & 2. from before.\nSummarize occurrences using count(hashtags, sort = TRUE).\nExtract top 50 hashtags by using slice_head(n = 50).\nConvert to string vector by using pull() .\n\nSave this transformation by creating a new dataset with the name top_hashtags_tidy.\n\nVisualize co-occurence\n\nBased on the dataset tweets_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network()\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n\n\nCreate new dataset quanteda_dfm_cleaned:\n\nBased on the dataset quanteda_dfm,\n\nUse quanteda::dfm_select(pattern = \"#*\") to create a DFM containing only hashtags.\nUse quanteda::dfm_remove(pattern = \"#digitaldetox\") to removes the hashtag \"#digitaldetox\".\n\nSave this transformation by creating a new dataset with the name quanteda_dfm_cleaned.\n\nCreate new dataset top_hashtags_quanteda\n\nBased on the dataset quanteda_dfm_cleaned,\n\nUse topfeatures(50) o extract the top 50 most common hashtags.\nUse names() to only store the names (not the values).\n\nSave this transformation by creating a new dataset with the name top_hashtags_quanteda.\n\nVisualize co-occurence\n\nBased on the dataset quanteda_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network() .\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n\n\n\n# Create new dataset tweets_dfm_cleaned |¬†quanteda_dfm_cleaned\n\n# Create new dataset top_hashtags_quanteda |top_hashtags_quanteda\n\n# Visualize co-occurence\n\n\n\nüìã Exercise 2: Understanding Topic 9\n\n\n\n\n\n\nObjective(s)\n\n\n\nTake a closer look at another topic from the topic model used in the session by examining the most representative tweets and the users who post the most tweets on that topic.\n\n\n\n2.1: Explore top tweets\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nArrange the selected tweets in descending order based on top_gamma values using arrange(-top_gamma).\nExtract the top 10 tweets using slice_head(n = 10).\nSelect only relevant columns (tweet_id, user_username, created_at, text, top_gamma) using select().\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\n\n2.1 Explore top users\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nCount the number of tweets per user using count(user_username, sort = TRUE).\nCalculate the proportion of tweets for each user by creating a new column prop using mutate(prop = round(n/sum(n)*100, 2)).\nExtract the top 15 users with the highest engagement using slice_head(n = 15).\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\n\n\nüìã Exercise 3: Expolore different topic model\n\n\n\n\n\n\nObjective(s)\n\n\n\nIn the session, three models were considered for closer examination. Choose one of the other topics and recreate all the steps for the initial exploration of the topic model (as in the session).\n\n\n\n3.1 Initial exploration\n\nExplore a different topic model (estimation)\n\nBased on the dataset stm_results,\n\nUse the filter(k == X) to select a different topic model (estimation) of your choice (by defining X).\nTip: Use stm_results$k to see available options for X\nUse pull(mdl) %&gt;% .[[1]] to select the topic model (estimation) with the specified number of topics.\n\nSave this transformation by creating a new dataset with the name tpm_new.\n\nVisual exploration of tpm_new\n\nVisualize the summary of the selected topic model using plot(type = \"summary\") .\n\n\n\n# Explore a different topic model (estimation)\ntpm_new &lt;- stm_results |&gt;\n   filter(k == ) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\n\n\n\n3.2 Document/Word-topic relations\n\nCalculate mean gamma values\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"gamma\") on tpm_new to get the gamma values.\nCalculate the mean gamma values for each topic using group_by() and summarise().\nUse arrange() to sort topics (descending) by gamma\n\nSave this transformation by creating a new dataset with the name top_gamma_new.\n\nIdentify top terms of each topic\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"beta\") on tpm_new to get the beta values.\nArrange terms within each topic in descending order based on beta values using group_by(), arrange(-beta), and top_n(10, wt = beta).\nSelect relevant columns (topic, term) using select().\nSummarize the top 10 terms for each topic using summarise(terms_beta = toString(term), .groups = \"drop\").\n\nSave this transformation by creating a new dataset with the name top_beta_new.\n\nCombine top topics and top terms\n\nJoin the dataframes top_gamma_new and top_beta_new based on the ‚Äútopic‚Äù column using left_join().\nWithin mutate(), - Adjusted topic names by topic = paste0(\"Topic \", topic) and - Reorder the dataset with topic = reorder(topic, gamma)\nSave this transformation by creating a new dataset with the name top_topics_terms_new.\n\nPreview the results\n\nDisplay a table preview of the top topics and terms with rounded gamma values using gt()\n\n\n\n# Calculate mean gamma values\n\n# Identify top terms of each topic\n\n# Combine top topics and top terms\n\n# Preview the esults"
  },
  {
    "objectID": "exercise/showcase-10.html",
    "href": "exercise/showcase-10.html",
    "title": "Showcase 10: üî® Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides"
  },
  {
    "objectID": "exercise/showcase-10.html#background",
    "href": "exercise/showcase-10.html#background",
    "title": "Showcase 10: üî® Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/showcase-10.html#preparation",
    "href": "exercise/showcase-10.html#preparation",
    "title": "Showcase 10: üî® Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/showcase-10.html#import-and-process-the-data",
    "href": "exercise/showcase-10.html#import-and-process-the-data",
    "title": "Showcase 10: üî® Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n# Topic models \nstm_k0 &lt;- qs::qread(here(\"local_data/stm_k0.qs\"))\nstm_exploration &lt;- qs::qread(here(\"local_data/stm_exploration.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))"
  },
  {
    "objectID": "exercise/showcase-10.html#text-as-data-in-r-part-ii",
    "href": "exercise/showcase-10.html#text-as-data-in-r-part-ii",
    "title": "Showcase 10: üî® Automatic analysis of text in R",
    "section": "Text as data in R (Part II)",
    "text": "Text as data in R (Part II)\n\nStep-by-step DTM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\nDocument-feature matrix of: 46,670 documents, 87,172 features (99.99% sparse) and 0 docvars.\n                     features\ndocs                  bite detox digital digitaldetox enjoy fly happitizer\n  1000009901563838465    1     1       1            1     2   1          1\n  1000038819520008193    0     0       0            1     0   0          0\n  1000042717492187136    0     0       0            1     0   0          0\n  1000043574673715203    0     0       0            1     0   0          0\n  1000053895035531264    0     1       1            1     0   0          0\n  1000075155891281925    0     0       0            1     0   0          0\n                     features\ndocs                  happitizers https inspiration\n  1000009901563838465           1     1           1\n  1000038819520008193           0     1           0\n  1000042717492187136           0     1           0\n  1000043574673715203           0     2           0\n  1000053895035531264           0     2           0\n  1000075155891281925           0     1           0\n[ reached max_ndoc ... 46,664 more documents, reached max_nfeat ... 87,162 more features ]\n\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\nDocument-feature matrix of: 46,670 documents, 49,123 features (99.98% sparse) and 35 docvars.\n            features\ndocs         put blackberry iphone read pew report teens distracted driving\n  5777201122   1          1      1    1   1      1     1          1       1\n  4814687834   0          0      0    0   0      0     0          0       0\n  4813781509   0          0      0    0   0      0     0          0       0\n  3351604894   0          0      0    0   0      0     0          0       0\n  3350930292   0          0      0    0   0      0     0          0       0\n  3349372574   0          0      0    0   0      0     0          0       0\n            features\ndocs         #digitaldetox\n  5777201122             1\n  4814687834             1\n  4813781509             1\n  3351604894             1\n  3350930292             1\n  3349372574             1\n[ reached max_ndoc ... 46,664 more documents, reached max_nfeat ... 49,113 more features ]\n\n\n\n\n\n\n\nNetwork of hashtags\n\ntidytextquanteda\n\n\n\n# Extract hashtags\ntweets_hashtags &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) \n\n# Extract most common hashtags\ntop50_hashtags_tidy &lt;- tweets_hashtags %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n# Extract DFM with only hashtags\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Extract most common hashtags \ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of hashtags\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n\n\n\n\nTopic modeling\n\nPreparation\n\nPruning\n\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim(\n    min_docfreq = 0.0001, \n    max_docfreq = .099, \n    docfreq_type = \"prop\"\n  )\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n    convert(to = \"stm\")\n\n\n\n\n\n\n\nConverting the trimmmed DFM to an stm object will result in an errormessage\n\nWarning message:\nIn dfm2stm(x, docvars, omit_empty = TRUE) : Dropped 46,670 empty document(s)\n\nThis is due to the fact, that some tweets are ‚Äúempty‚Äù or do not match with any feature after the pruning. To successfully match the stm results with the original data, the ‚Äúempty‚Äù tweets need to be dropped. To identify the empty cases, run\n\n# Check if tweet contains feature\nempty_docs &lt;- Matrix::rowSums(as(quanteda_dfm_trim, \"Matrix\")) == 0 \n\n# Create vector for empty tweet identification \nempty_docs_ids &lt;- quanteda_dfm_trim@docvars$docname[empty_docs]\n\n\n# Optional: Print indices of empty documents\nif (any(empty_docs)) {\n  cat(\"Indices of empty documents:\", which(empty_docs), \"\\n\")\n  \n  # Print corresponding docnames\n  cat(\"Docnames of empty documents:\", quanteda_dfm_trim@docvars$docname[empty_docs], \"\\n\")\n}\n\n\n\n\n\n\n\nSelect model\n\nbased on k = 0\n\n# Estimate model\nstm_k0 &lt;- stm(\n    quanteda_stm$documents, \n    quanteda_stm$vocab, \n    K = 0, \n    max.em.its = 50,\n    init.type = \"Spectral\", \n    seed = 42 \n  )\n\n\n# Preview\nstm_k0\n\nA topic model with 72 topics, 46574 documents and a 8268 word dictionary.\n\n\n\n\nbased on different model diagnostics\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) # use multiple sessions \n\n# Estimate multiple models\nstm_exploration &lt;- tibble(k = seq(from = 5, to = 85, by = 5)) %&gt;% \n    mutate(mdl = furrr::future_map(k, ~stm::stm(\n      documents =  quanteda_stm$documents,\n      vocab = quanteda_stm$vocab, \n      K = ., \n      seed = 42,\n      max.em.its = 1000,\n      init.type = \"Spectral\",\n      verbose = FALSE),\n      .options = furrr::furrr_options(seed = 42))\n  )\n\n\nstm_exploration$mdl\n\n[[1]]\nA topic model with 5 topics, 46574 documents and a 8268 word dictionary.\n\n[[2]]\nA topic model with 10 topics, 46574 documents and a 8268 word dictionary.\n\n[[3]]\nA topic model with 15 topics, 46574 documents and a 8268 word dictionary.\n\n[[4]]\nA topic model with 20 topics, 46574 documents and a 8268 word dictionary.\n\n[[5]]\nA topic model with 25 topics, 46574 documents and a 8268 word dictionary.\n\n[[6]]\nA topic model with 30 topics, 46574 documents and a 8268 word dictionary.\n\n[[7]]\nA topic model with 35 topics, 46574 documents and a 8268 word dictionary.\n\n[[8]]\nA topic model with 40 topics, 46574 documents and a 8268 word dictionary.\n\n[[9]]\nA topic model with 45 topics, 46574 documents and a 8268 word dictionary.\n\n[[10]]\nA topic model with 50 topics, 46574 documents and a 8268 word dictionary.\n\n[[11]]\nA topic model with 55 topics, 46574 documents and a 8268 word dictionary.\n\n[[12]]\nA topic model with 60 topics, 46574 documents and a 8268 word dictionary.\n\n[[13]]\nA topic model with 65 topics, 46574 documents and a 8268 word dictionary.\n\n[[14]]\nA topic model with 70 topics, 46574 documents and a 8268 word dictionary.\n\n[[15]]\nA topic model with 75 topics, 46574 documents and a 8268 word dictionary.\n\n[[16]]\nA topic model with 80 topics, 46574 documents and a 8268 word dictionary.\n\n[[17]]\nA topic model with 85 topics, 46574 documents and a 8268 word dictionary.\n\n\n\n\n\nExploration\n\n# Create heldout\nheldout &lt;- make.heldout(\n  quanteda_stm$documents,\n  quanteda_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_exploration %&gt;%\n  mutate(exclusivity = map(mdl, exclusivity),\n         semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n         eval_heldout = map(mdl, eval.heldout, heldout$missing),\n         residual = map(mdl, checkResiduals, quanteda_stm$documents),\n         bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n         lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n         lbound = bound + lfact,\n         iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))\n\n\n# Preview \nstm_results\n\n# A tibble: 17 √ó 10\n       k mdl    exclusivity semantic_coherence eval_heldout residual       bound\n   &lt;dbl&gt; &lt;list&gt; &lt;list&gt;      &lt;list&gt;             &lt;list&gt;       &lt;list&gt;         &lt;dbl&gt;\n 1     5 &lt;STM&gt;  &lt;dbl [5]&gt;   &lt;dbl [5]&gt;          &lt;named list&gt; &lt;named list&gt; -3.25e6\n 2    10 &lt;STM&gt;  &lt;dbl [10]&gt;  &lt;dbl [10]&gt;         &lt;named list&gt; &lt;named list&gt; -3.21e6\n 3    15 &lt;STM&gt;  &lt;dbl [15]&gt;  &lt;dbl [15]&gt;         &lt;named list&gt; &lt;named list&gt; -3.17e6\n 4    20 &lt;STM&gt;  &lt;dbl [20]&gt;  &lt;dbl [20]&gt;         &lt;named list&gt; &lt;named list&gt; -3.12e6\n 5    25 &lt;STM&gt;  &lt;dbl [25]&gt;  &lt;dbl [25]&gt;         &lt;named list&gt; &lt;named list&gt; -3.09e6\n 6    30 &lt;STM&gt;  &lt;dbl [30]&gt;  &lt;dbl [30]&gt;         &lt;named list&gt; &lt;named list&gt; -3.09e6\n 7    35 &lt;STM&gt;  &lt;dbl [35]&gt;  &lt;dbl [35]&gt;         &lt;named list&gt; &lt;named list&gt; -3.08e6\n 8    40 &lt;STM&gt;  &lt;dbl [40]&gt;  &lt;dbl [40]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n 9    45 &lt;STM&gt;  &lt;dbl [45]&gt;  &lt;dbl [45]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n10    50 &lt;STM&gt;  &lt;dbl [50]&gt;  &lt;dbl [50]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n11    55 &lt;STM&gt;  &lt;dbl [55]&gt;  &lt;dbl [55]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n12    60 &lt;STM&gt;  &lt;dbl [60]&gt;  &lt;dbl [60]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n13    65 &lt;STM&gt;  &lt;dbl [65]&gt;  &lt;dbl [65]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n14    70 &lt;STM&gt;  &lt;dbl [70]&gt;  &lt;dbl [70]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n15    75 &lt;STM&gt;  &lt;dbl [75]&gt;  &lt;dbl [75]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n16    80 &lt;STM&gt;  &lt;dbl [80]&gt;  &lt;dbl [80]&gt;         &lt;named list&gt; &lt;named list&gt; -3.03e6\n17    85 &lt;STM&gt;  &lt;dbl [85]&gt;  &lt;dbl [85]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n# ‚Ñπ 3 more variables: lfact &lt;dbl&gt;, lbound &lt;dbl&gt;, iterations &lt;dbl&gt;\n\n# Visualize\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n         y = NULL,\n         title = \"Model diagnostics by number of topics\"\n    ) +\n    theme_pubr() +\n    # add highlights \n    geom_vline(aes(xintercept =  5), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 10), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#C77CFF\", alpha = .5) \n\n\n\n\n\n# Models for comparison\nmodels_for_comparison = c(5, 10, 40)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr() \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()\n\n\n\n\n\n\n\nUnderstanding\n\nSelect model for further analysis\n\nn_topics &lt;- 10\n\ntpm &lt;- stm_results |&gt;\n   filter(k == n_topics) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n\n\nOverview\n\ntpm %&gt;% plot(type = \"summary\")\n\n\n\ntpm %&gt;% plot(type = \"hist\")\n\n\n\n\n\n\nDocument/Word-topic relations\n\n# Create data\ntop_gamma &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- tpm %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      topic\n      terms_beta\n      gamma\n    \n  \n  \n    Topic 10\nphone, via, much, #screentime, know, check, people, phones, #parenting, online\n0.122\n    Topic 9\nhelp, read, devices, use, world, tech, family, taking, kids, sleep\n0.114\n    Topic 3\nneed, great, week, going, listen, #unplugging, discuss, @icphenomenallyu, away, well\n0.111\n    Topic 2\n#unplug, just, weekend, #travel, unplug, enjoy, #nature, really, join, nature\n0.103\n    Topic 8\nnew, technology, smartphone, retreat, work, addiction, year, health, internet, without\n0.099\n    Topic 7\ncan, #mindfulness, feel, #switchoff, #digitalwellbeing, #wellness, things, #phonefree, #disconnecttoreconnect, #digitalminimalism\n0.098\n    Topic 4\namp, day, take, #mentalhealth, go, try, #wellbeing, now, give, every\n0.092\n    Topic 1\nsocial, media, get, life, back, like, good, #socialmedia, find, see\n0.091\n    Topic 5\nus, today, days, next, put, happy, may, facebook, share, hour\n0.089\n    Topic 6\none, break, tips, make, screen, love, #technology, free, looking, getting\n0.080\n  \n  \n  \n\n\n\n\n\n\nDocument-topic-relations\n\n# Prepare for merging\ntopic_gammas &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas &lt;- tpm %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(., topic_gammas, by = join_by(document))\n\n# Merge with original data\ntweets_detox_topics &lt;- tweets_detox %&gt;% \n  filter(!(tweet_id %in% empty_docs_ids)) %&gt;% \n  bind_cols(gammas) %&gt;% \n  select(-document)\n\n# Preview\ntweets_detox_topics \n\n# A tibble: 46,574 √ó 50\n   tweet_id   user_username text         created_at          in_reply_to_user_id\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;              \n 1 5777201122 pblackshaw    Put down yo‚Ä¶ 2009-11-16 22:03:12 &lt;NA&gt;               \n 2 4814687834 andrewgerrard @Dawn_Wylie‚Ä¶ 2009-10-12 18:48:30 18003723           \n 3 4813781509 pblackshaw    Is Social M‚Ä¶ 2009-10-12 17:55:57 &lt;NA&gt;               \n 4 3351604894 KurtyD        Google gear‚Ä¶ 2009-08-16 23:20:53 &lt;NA&gt;               \n 5 3350930292 KurtyD        Wifi on the‚Ä¶ 2009-08-16 22:31:22 &lt;NA&gt;               \n 6 3349372574 KurtyD        Adios amigo‚Ä¶ 2009-08-16 20:37:03 &lt;NA&gt;               \n 7 2722418665 evelynanne    Ok, I'm hea‚Ä¶ 2009-07-19 14:31:16 &lt;NA&gt;               \n 8 1959962952 halriley      I'm thinkin‚Ä¶ 2009-05-29 14:11:36 &lt;NA&gt;               \n 9 1638612063 deconst       Interesting‚Ä¶ 2009-04-28 13:03:14 &lt;NA&gt;               \n10 1626172818 deconst       Back from #‚Ä¶ 2009-04-27 03:55:25 &lt;NA&gt;               \n# ‚Ñπ 46,564 more rows\n# ‚Ñπ 45 more variables: author_id &lt;chr&gt;, lang &lt;chr&gt;, possibly_sensitive &lt;lgl&gt;,\n#   conversation_id &lt;chr&gt;, user_created_at &lt;chr&gt;, user_protected &lt;lgl&gt;,\n#   user_name &lt;chr&gt;, user_verified &lt;lgl&gt;, user_description &lt;chr&gt;,\n#   user_location &lt;chr&gt;, user_url &lt;chr&gt;, user_profile_image_url &lt;chr&gt;,\n#   user_pinned_tweet_id &lt;chr&gt;, retweet_count &lt;int&gt;, like_count &lt;int&gt;,\n#   quote_count &lt;int&gt;, user_tweet_count &lt;int&gt;, user_list_count &lt;int&gt;, ‚Ä¶\n\n\n\n\nTopic distribution\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\n\nTopic distribution over time\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(year, fill = top_topic)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(year, fill = top_topic)) +\n  geom_bar(position = \"fill\") +\n  theme_pubr() \n\n\n\n\n\n\nFocus on specific topics\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      tweet_id\n      user_username\n      created_at\n      text\n      top_gamma\n    \n  \n  \n    1496707135794827266\nbeckygrantstr\n2022-02-24 04:42:43\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/EcV7yKn4WX\n0.8192142\n    1496343978672898048\nbeckygrantstr\n2022-02-23 04:39:39\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/o9e6WQIpm5\n0.8192142\n    1495981434665947137\nbeckygrantstr\n2022-02-22 04:39:02\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/rQgcPltrMt\n0.8192142\n    1499612427503247360\nbeckygrantstr\n2022-03-04 05:07:18\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/DQRQlN1iyq\n0.8192142\n    1499249315381977089\nbeckygrantstr\n2022-03-03 05:04:26\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/L7ly66J2fq\n0.8192142\n    1498886117394980865\nbeckygrantstr\n2022-03-02 05:01:13\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/uJPIMYY1Id\n0.8192142\n    1498522796611321864\nbeckygrantstr\n2022-03-01 04:57:30\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/z4Fv2bmeag\n0.8192142\n    1498159674008510465\nbeckygrantstr\n2022-02-28 04:54:35\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/SdZFAJy6kZ\n0.8192142\n    1497796525929472003\nbeckygrantstr\n2022-02-27 04:51:34\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/OKLMzwfQUA\n0.8192142\n    1497433419512524802\nbeckygrantstr\n2022-02-26 04:48:42\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/yTnqhTPV8h\n0.8192142\n  \n  \n  \n\n\n\n\n\n\nTop Users\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 15)\n\n# A tibble: 15 √ó 3\n   user_username       n  prop\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt;\n 1 TimeToLogOff     2384 28.8 \n 2 punkt             390  4.72\n 3 petitstvincent    175  2.12\n 4 tanyagoodin       135  1.63\n 5 ditox_unplug      111  1.34\n 6 OurHourOff         99  1.2 \n 7 beckygrantstr      63  0.76\n 8 CreeEscape         56  0.68\n 9 ConsciDigital      51  0.62\n10 phubboo            50  0.6 \n11 digitaldetoxing    47  0.57\n12 smudgedlippy       40  0.48\n13 detox_india        32  0.39\n14 winiepuh           27  0.33\n15 iamscentered       23  0.28"
  },
  {
    "objectID": "exercise/showcase-09.html",
    "href": "exercise/showcase-09.html",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "",
    "text": "Open session slides"
  },
  {
    "objectID": "exercise/showcase-09.html#background",
    "href": "exercise/showcase-09.html#background",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/showcase-09.html#preparation",
    "href": "exercise/showcase-09.html#preparation",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nsource(here::here(\"slides/schedule.R\"))\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/showcase-09.html#import-and-process-the-data",
    "href": "exercise/showcase-09.html#import-and-process-the-data",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/showcase-09.html#digital-disconnection-on-fa-brands-twitter-not-ùïè",
    "href": "exercise/showcase-09.html#digital-disconnection-on-fa-brands-twitter-not-ùïè",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Digital disconnection on  (not ùïè)",
    "text": "Digital disconnection on  (not ùïè)\n\nThe structure of the data set & available variables\n\ntweets_correct %&gt;% glimpse()\n\nRows: 361,408\nColumns: 37\n$ tweet_id               &lt;chr&gt; \"7223508762\", \"7222271007\", \"7219735500\", \"7216‚Ä¶\n$ user_username          &lt;chr&gt; \"Princessbride24\", \"winnerandy\", \"the_enthusias‚Ä¶\n$ text                   &lt;chr&gt; \"@Ali_Sweeney detox same week as the digital cl‚Ä¶\n$ created_at             &lt;dttm&gt; 2009-12-31 05:25:55, 2009-12-31 04:44:20, 2009‚Ä¶\n$ in_reply_to_user_id    &lt;chr&gt; \"23018333\", NA, NA, NA, NA, NA, NA, \"19475829\",‚Ä¶\n$ author_id              &lt;chr&gt; \"16157429\", \"14969949\", \"16217478\", \"18001568\",‚Ä¶\n$ lang                   &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\",‚Ä¶\n$ possibly_sensitive     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ conversation_id        &lt;chr&gt; \"7222577237\", \"7222271007\", \"7219735500\", \"7216‚Ä¶\n$ user_created_at        &lt;chr&gt; \"2008-09-06T15:13:58.000Z\", \"2008-06-01T07:34:3‚Ä¶\n$ user_protected         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ user_name              &lt;chr&gt; \"Sam\", \"Andrew\", \"The Enthusiast\", \"üåªBahSunüåª\"‚Ä¶\n$ user_verified          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ user_description       &lt;chr&gt; \"As you wish.\", \"Life is a playground, so enjoy‚Ä¶\n$ user_location          &lt;chr&gt; \"Houston\", \"California\", \"Melbourne, Australia\"‚Ä¶\n$ user_url               &lt;chr&gt; \"https://t.co/hchLJvesW1\", NA, \"http://t.co/bLK‚Ä¶\n$ user_profile_image_url &lt;chr&gt; \"https://pbs.twimg.com/profile_images/587051289‚Ä¶\n$ user_pinned_tweet_id   &lt;chr&gt; NA, NA, NA, NA, \"1285575168010735621\", NA, NA, ‚Ä¶\n$ retweet_count          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ like_count             &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ quote_count            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ user_tweet_count       &lt;int&gt; 3905, 18808, 4494, 35337, 284181, 18907, 39562,‚Ä¶\n$ user_list_count        &lt;int&gt; 7, 12, 101, 49, 540, 82, 91, 12, 117, 46, 192, ‚Ä¶\n$ user_followers_count   &lt;int&gt; 103, 481, 2118, 693, 17677, 2527, 8784, 100, 26‚Ä¶\n$ user_following_count   &lt;int&gt; 370, 1488, 391, 317, 16470, 320, 11538, 120, 25‚Ä¶\n$ sourcetweet_type       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_id         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_text       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_lang       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_author_id  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ year                   &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n$ month                  &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,‚Ä¶\n$ day                    &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 29, 28, 28, 27, 27,‚Ä¶\n$ hour                   &lt;int&gt; 5, 4, 3, 1, 22, 2, 0, 20, 21, 1, 19, 8, 8, 3, 9‚Ä¶\n$ minute                 &lt;int&gt; 25, 44, 22, 42, 49, 31, 39, 23, 45, 47, 31, 28,‚Ä¶\n$ retweet_dy             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,‚Ä¶\n$ detox_dy               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n\n\n\n\nBonus: Distribution statistics\n\ntweets_correct %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n361408\n\n\nNumber of columns\n37\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n19\n\n\nlogical\n5\n\n\nnumeric\n12\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntweet_id\n0\n1.00\n9\n19\n0\n361408\n0\n\n\nuser_username\n0\n1.00\n2\n15\n0\n185679\n0\n\n\ntext\n0\n1.00\n13\n938\n0\n343311\n0\n\n\nin_reply_to_user_id\n321844\n0.11\n2\n19\n0\n30993\n0\n\n\nauthor_id\n0\n1.00\n3\n19\n0\n185679\n0\n\n\nlang\n0\n1.00\n2\n3\n0\n60\n0\n\n\nconversation_id\n0\n1.00\n9\n19\n0\n354482\n0\n\n\nuser_created_at\n0\n1.00\n24\n24\n0\n185594\n0\n\n\nuser_name\n0\n1.00\n0\n50\n50\n178328\n0\n\n\nuser_description\n0\n1.00\n0\n353\n26741\n166383\n0\n\n\nuser_location\n66727\n0.82\n1\n147\n0\n51237\n65\n\n\nuser_url\n103146\n0.71\n12\n58\n0\n115594\n0\n\n\nuser_profile_image_url\n0\n1.00\n0\n226\n44\n182266\n0\n\n\nuser_pinned_tweet_id\n232501\n0.36\n4\n19\n0\n56397\n0\n\n\nsourcetweet_type\n349949\n0.03\n6\n6\n0\n1\n0\n\n\nsourcetweet_id\n349949\n0.03\n11\n19\n0\n10480\n0\n\n\nsourcetweet_text\n349949\n0.03\n1\n755\n0\n10283\n0\n\n\nsourcetweet_lang\n349949\n0.03\n2\n3\n0\n42\n0\n\n\nsourcetweet_author_id\n349949\n0.03\n2\n19\n0\n7419\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npossibly_sensitive\n0\n1\n0.01\nFAL: 359068, TRU: 2340\n\n\nuser_protected\n0\n1\n0.00\nFAL: 361408\n\n\nuser_verified\n0\n1\n0.06\nFAL: 340005, TRU: 21403\n\n\nretweet_dy\n0\n1\n0.02\nFAL: 354735, TRU: 6673\n\n\ndetox_dy\n0\n1\n0.16\nFAL: 303396, TRU: 58012\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nretweet_count\n0\n1\n0.77\n21.53\n0\n0\n0\n0.00\n9035\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nlike_count\n0\n1\n3.17\n219.89\n0\n0\n0\n1.00\n98181\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nquote_count\n0\n1\n0.09\n11.07\n0\n0\n0\n0.00\n5213\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_tweet_count\n0\n1\n60860.29\n173889.51\n1\n2933\n12371\n41843.00\n9641956\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_list_count\n0\n1\n375.59\n2643.28\n0\n7\n46\n171.00\n216734\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_followers_count\n0\n1\n37077.25\n532883.29\n0\n274\n1157\n4724.25\n61075617\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_following_count\n0\n1\n2952.88\n18952.23\n0\n221\n808\n2067.00\n1407439\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n0\n1\n2016.96\n2.73\n2007\n2015\n2017\n2019.00\n2022\n‚ñÅ‚ñÇ‚ñá‚ñá‚ñÖ\n\n\nmonth\n0\n1\n6.36\n3.49\n1\n3\n7\n9.00\n12\n‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá\n\n\nday\n0\n1\n15.50\n8.95\n1\n7\n15\n23.00\n31\n‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ\n\n\nhour\n0\n1\n12.62\n6.08\n0\n8\n13\n17.00\n23\n‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÖ\n\n\nminute\n0\n1\n27.09\n17.93\n0\n11\n27\n43.00\n59\n‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ncreated_at\n0\n1\n2007-11-19 09:14:10\n2022-12-31 23:18:38\n2017-04-26 08:15:03\n352271\n\n\n\n\n\n\n\nDistribution of tweets with reference to digital detox over time\n\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = retweet_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Is the tweet a retweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#1DA1F2\", \"#004389\")) +\n    theme_pubr() +    \n    # add annotations\n    annotate(\n      \"text\", \n      x = 14, y = 55000,\n      label = \"Increase of character limit from 140 to 280\") + \n    geom_curve(\n      data = data.frame(\n        x = 14.2965001234837,y = 53507.2283841571,\n        xend = 11.5275706534335, yend = 45412.4966032138),\n      mapping = aes(x = x, y = y, xend = xend, yend = yend),\n      angle = 127L,\n      curvature = 0.28,\n      arrow = arrow(30L, unit(0.1, \"inches\"), \"last\", \"closed\"),\n      inherit.aes = FALSE)\n\n\n\n\n\n\nTweets with reference to ditigal detox by (participating) users\n\ntweets_correct %&gt;% \n    group_by(author_id) %&gt;% \n    summarize(n = n()) %&gt;% \n    mutate(\n        n_grp = case_when(\n                     n &lt;=    1 ~ 1,\n            n &gt;   1 & n &lt;=  10 ~ 2,\n            n &gt;  10 & n &lt;=  25 ~ 3,\n            n &gt;  25 & n &lt;=  50 ~ 4,\n            n &gt;  50 & n &lt;=  75 ~ 5,\n            n &gt;  75 & n &lt;= 100 ~ 6,\n            n &gt; 100 ~ 7\n        ), \n        n_grp_fct = factor(\n            n_grp,\n            levels = c(1:7),\n            labels = c(\n            \" 1\",\n            \" 2 - 10\", \"11 - 25\",\n            \"26 - 50\", \"50 - 75\",\n            \"75 -100\", \"&gt; 100\")\n        )\n    ) %&gt;% \n    sjmisc::frq(n_grp_fct) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Tweets by users with a least on tweet\"\n    #    ) %&gt;% \n    cols_label(\n      val = \"Number of tweets by user\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Number of tweets by user\n      n\n      %raw\n      %cum\n    \n  \n  \n     1\n143364\n77.21\n77.21\n     2 - 10\n39981\n21.53\n98.74\n    11 - 25\n1647\n0.89\n99.63\n    26 - 50\n435\n0.23\n99.86\n    50 - 75\n88\n0.05\n99.91\n    75 -100\n55\n0.03\n99.94\n    &gt; 100\n109\n0.06\n100.00\n    NA\n0\n0.00\nNA\n  \n  \n  \n\n\n\n\n\n\nTweets with reference to ditigal detox by language\n\ntweets_correct %&gt;% \n    sjmisc::frq(\n        lang, \n        sort.frq = c(\"desc\"), \n        min.frq = 2000\n    ) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Language of the collected tweets\"\n    #   ) %&gt;% \n    cols_label(\n      val = \"Twitter language code\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Twitter language code\n      n\n      %raw\n      %cum\n    \n  \n  \n    en\n274351\n75.91\n75.91\n    fr\n20407\n5.65\n81.56\n    es\n16248\n4.50\n86.05\n    de\n14091\n3.90\n89.95\n    pt\n7127\n1.97\n91.92\n    it\n5999\n1.66\n93.58\n    da\n3490\n0.97\n94.55\n    in\n3095\n0.86\n95.41\n    ja\n2428\n0.67\n96.08\n    n &lt; 2000\n14172\n3.92\n100.00\n    NA\n0\n0.00\nNA"
  },
  {
    "objectID": "exercise/showcase-09.html#text-as-data-in-r-part-i",
    "href": "exercise/showcase-09.html#text-as-data-in-r-part-i",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Text as data in R (Part I)",
    "text": "Text as data in R (Part I)\n\nBuild a subsample: Tweets containing #digitaldetox\n\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = detox_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"#digitaldetox used in tweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#1DA1F2\")) +\n    theme_pubr() \n\n\n\n\n\n\n\n\n\n\n\nBonus: distribution statistics\n\ntweets_detox %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n46670\n\n\nNumber of columns\n37\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n19\n\n\nlogical\n5\n\n\nnumeric\n12\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntweet_id\n0\n1.00\n10\n19\n0\n46670\n0\n\n\nuser_username\n0\n1.00\n3\n15\n0\n15383\n0\n\n\ntext\n0\n1.00\n18\n873\n0\n45039\n0\n\n\nin_reply_to_user_id\n43310\n0.07\n3\n19\n0\n2617\n0\n\n\nauthor_id\n0\n1.00\n3\n19\n0\n15383\n0\n\n\nlang\n0\n1.00\n2\n2\n0\n1\n0\n\n\nconversation_id\n0\n1.00\n10\n19\n0\n46235\n0\n\n\nuser_created_at\n0\n1.00\n24\n24\n0\n15383\n0\n\n\nuser_name\n0\n1.00\n1\n50\n0\n15252\n0\n\n\nuser_description\n0\n1.00\n0\n227\n1089\n14612\n0\n\n\nuser_location\n5596\n0.88\n1\n113\n0\n5834\n2\n\n\nuser_url\n5863\n0.87\n18\n44\n0\n11413\n0\n\n\nuser_profile_image_url\n0\n1.00\n59\n151\n0\n15317\n0\n\n\nuser_pinned_tweet_id\n21644\n0.54\n9\n19\n0\n4756\n0\n\n\nsourcetweet_type\n44990\n0.04\n6\n6\n0\n1\n0\n\n\nsourcetweet_id\n44990\n0.04\n18\n19\n0\n1620\n0\n\n\nsourcetweet_text\n44990\n0.04\n23\n402\n0\n1620\n0\n\n\nsourcetweet_lang\n44990\n0.04\n2\n3\n0\n14\n0\n\n\nsourcetweet_author_id\n44990\n0.04\n4\n19\n0\n1227\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npossibly_sensitive\n0\n1\n0.00\nFAL: 46559, TRU: 111\n\n\nuser_protected\n0\n1\n0.00\nFAL: 46670\n\n\nuser_verified\n0\n1\n0.03\nFAL: 45200, TRU: 1470\n\n\nretweet_dy\n0\n1\n0.00\nFAL: 46670\n\n\ndetox_dy\n0\n1\n1.00\nTRU: 46670\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nretweet_count\n0\n1\n0.70\n30.82\n0\n0.00\n0.0\n0\n6618\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nlike_count\n0\n1\n2.88\n220.54\n0\n0.00\n0.0\n1\n47308\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nquote_count\n0\n1\n0.06\n1.62\n0\n0.00\n0.0\n0\n340\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_tweet_count\n0\n1\n27905.41\n76117.39\n1\n1809.00\n10199.0\n16045\n1268964\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_list_count\n0\n1\n208.86\n557.75\n0\n15.00\n109.0\n163\n25072\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_followers_count\n0\n1\n7627.12\n74560.68\n0\n519.25\n2410.0\n5032\n7977533\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuser_following_count\n0\n1\n2223.94\n6345.92\n0\n398.00\n1749.5\n1989\n475935\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n0\n1\n2017.40\n2.24\n2009\n2016.00\n2017.0\n2019\n2022\n‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ\n\n\nmonth\n0\n1\n6.43\n3.48\n1\n3.00\n7.0\n9\n12\n‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá\n\n\nday\n0\n1\n15.58\n8.77\n1\n8.00\n16.0\n23\n31\n‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ\n\n\nhour\n0\n1\n12.50\n5.52\n0\n9.00\n13.0\n16\n23\n‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÉ\n\n\nminute\n0\n1\n24.62\n17.97\n0\n7.00\n25.0\n39\n59\n‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÉ\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ncreated_at\n0\n1\n2009-01-04 12:10:04\n2022-12-31 21:23:34\n2017-09-08 10:45:05\n46448\n\n\n\n\n\n\n\nTransform data to tidy text\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n    # Remove HTML entities\n    mutate(text = str_remove_all(text, remove_reg)) %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n    select(tweet_id, user_username, text) %&gt;% \n    print(n = 5)\n\n# A tibble: 639,459 √ó 3\n  tweet_id   user_username text      \n  &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;     \n1 5777201122 pblackshaw    blackberry\n2 5777201122 pblackshaw    iphone    \n3 5777201122 pblackshaw    read      \n4 5777201122 pblackshaw    pew       \n5 5777201122 pblackshaw    report    \n# ‚Ñπ 639,454 more rows\n\n\n\n\nSummarize all tokens over all tweets\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized %&gt;% \n    print(n = 15)\n\n# A tibble: 87,172 √ó 2\n   text             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 t.co         57530\n 2 https        52890\n 3 digitaldetox 46642\n 4 digital       8521\n 5 detox         6623\n 6 time          6000\n 7 http          4674\n 8 phone         4213\n 9 unplug        4021\n10 day           2939\n11 life          2548\n12 social        2449\n13 mindfulness   2408\n14 media         2264\n15 technology    2065\n# ‚Ñπ 87,157 more rows\n\n\n\n\nVisualization of Top 100 token\n\ntweets_summarized %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 30) +\n    theme_minimal()"
  },
  {
    "objectID": "exercise/showcase-09.html#modeling-realtionships-between-words",
    "href": "exercise/showcase-09.html#modeling-realtionships-between-words",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Modeling realtionships between words",
    "text": "Modeling realtionships between words\n\nCount word pairs within tweets\n\n# Create word paris\ntweets_word_pairs &lt;- tweets_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        tweet_id,\n        sort = TRUE)\n\n# Preview\ntweets_word_pairs %&gt;% \n    print(n = 15)\n\n# A tibble: 3,466,216 √ó 3\n   item1        item2            n\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;\n 1 t.co         digitaldetox 40640\n 2 digitaldetox t.co         40640\n 3 t.co         https        36605\n 4 https        t.co         36605\n 5 https        digitaldetox 36486\n 6 digitaldetox https        36486\n 7 digital      digitaldetox  7791\n 8 digitaldetox digital       7791\n 9 t.co         digital       7365\n10 digital      t.co          7365\n11 https        digital       6856\n12 digital      https         6856\n13 detox        digitaldetox  6004\n14 digitaldetox detox         6004\n15 t.co         detox         5672\n# ‚Ñπ 3,466,201 more rows\n\n\n\n\nSummarize and correlate tokens within tweets\n\n# Create word correlation\ntweets_pairs_corr &lt;- tweets_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        tweet_id, \n        sort = TRUE)\n\n# Preview\ntweets_pairs_corr %&gt;% \n    print(n = 15)\n\n# A tibble: 41,820 √ó 3\n   item1           item2           correlation\n   &lt;chr&gt;           &lt;chr&gt;                 &lt;dbl&gt;\n 1 jocelyn         brewer                0.999\n 2 brewer          jocelyn               0.999\n 3 jocelyn         discusses             0.983\n 4 discusses       jocelyn               0.983\n 5 discusses       brewer                0.982\n 6 brewer          discusses             0.982\n 7 icphenomenallyu discuss               0.981\n 8 discuss         icphenomenallyu       0.981\n 9 taniamulry      wealth                0.979\n10 wealth          taniamulry            0.979\n11 jocelyn         nutrition             0.968\n12 nutrition       jocelyn               0.968\n13 nutrition       brewer                0.967\n14 brewer          nutrition             0.967\n15 discusses       nutrition             0.951\n# ‚Ñπ 41,805 more rows\n\n\n\n\nDisplay correlates for specific token\n\ntweets_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\"detox\", \"digital\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#04316A\")) +\n  theme_pubr()"
  },
  {
    "objectID": "exercise/showcase-09.html#dictionary-based-approach-sentiment-analysis",
    "href": "exercise/showcase-09.html#dictionary-based-approach-sentiment-analysis",
    "title": "Showcase 09: üî® Text as data in R",
    "section": "Dictionary based approach: Sentiment analysis",
    "text": "Dictionary based approach: Sentiment analysis\n\nMost commmon positive and negative words\n\ntweets_sentiment_count &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\ntweets_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007D29\")) +\n  theme_pubr()\n\n\n\n\n\nLink word sentiment to tidy data\n\ntweets_sentiment &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(tweet_id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\ntweets_sentiment \n\n# A tibble: 24,372 √ó 4\n   tweet_id            positive negative sentiment\n   &lt;chr&gt;                  &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 1000009901563838465        3        0         3\n 2 1000038819520008193        1        0         1\n 3 1000042717492187136        4        0         4\n 4 1000043574673715203        1        1         0\n 5 1000075155891281925        4        0         4\n 6 1000086637987139590        1        0         1\n 7 1000094334660825088        1        0         1\n 8 1000133715194920960        1        0         1\n 9 1000255467434729472        0        1        -1\n10 1000271209353895938        1        1         0\n# ‚Ñπ 24,362 more rows\n\n\n\n\nOverall distribution sentiment by tweets\n\ntweets_sentiment %&gt;% \n  ggplot(aes(as.factor(sentiment))) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"Sentiment (sum) of tweet\", \n    y = \"Number of tweets\"\n  ) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nDevelopment of tweet sentiment over the years\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Proportion of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "exercise/exercise-10_solution.html",
    "href": "exercise/exercise-10_solution.html",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#background",
    "href": "exercise/exercise-10_solution.html#background",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#preparation",
    "href": "exercise/exercise-10_solution.html#preparation",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#import-and-process-the-data",
    "href": "exercise/exercise-10_solution.html#import-and-process-the-data",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\nGet twitter data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nDTM/DFM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  mutate(across(text, ~str_replace_all(., \"#digitaldetox\", \"\"))) %&gt;% \n  select(-c(detox_dy, retweet_dy)) %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\n\n\nGet topic model (data)\n\n# TPM data\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))\n\n# Base data with topics \ntweets_detox_topics &lt;- qs::qread(here(\"local_data/tweets-digital-detox-topics.qs\"))"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#exercises",
    "href": "exercise/exercise-10_solution.html#exercises",
    "title": "Exercise 10: üî® Automatic analysis of text in R",
    "section": "üìã Exercises",
    "text": "üìã Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the ‚ÄúRun all chunks above‚Äù-button  of the next chunk.\nYou can choose to solve some exercises using either the tidytext or quanteda functions. As the work steps differ, please select the tab with your preferred package.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\nüìã Exercise 1: Hashtag co-occurence\n\n\n\n\n\n\nObjective(s)\n\n\n\nRecreate the network plot from the session without the token #digitaldetox\n\n\n\nwith tidytextwith quanteda\n\n\n\nCreate new dataset tweets_dfm_cleaned\n\nBased on the dataset tweets_detox,\n\nUse mutate() and create the variables\n\ntext that removes the hashtag \"#digitaldetox\" using str_remove_all() and\nhashtag that extracts hashtags from the variable text with the help of str_extract_all() (and the pattern \"#\\\\S+\").\n\nTokenize the data by using only unnest() on the variable hashtag.\nSummarize occurrences using count(tweet_id, hashtags).\nConvert to DFM using cast_dfm(tweet_id, hashtags, n).\n\nSave this transformation by creating a new dataset with the name tweets_dfm_cleaned.\n\nCreate new dataset top_hashtags_tidy\n\nBased on the dataset tweets_detox,\n\nRepeat steps 1. & 2. from before.\nSummarize occurrences using count(hashtags, sort = TRUE).\nExtract top 50 hashtags by using slice_head(n = 50).\nConvert to string vector by using pull() .\n\nSave this transformation by creating a new dataset with the name top_hashtags_tidy.\n\nVisualize co-occurence\n\nBased on the dataset tweets_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network()\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n# Create new dataset `tweets_dfm_cleaned`\ntweets_dfm_cleaned &lt;- tweets_detox %&gt;% \n  # Edit data\n  mutate(\n    text = str_remove_all(text, \"#digitaldetox\"), \n    hashtags = str_extract_all(text, \"#\\\\S+\")\n    ) %&gt;%\n  # Tokenize\n  unnest(hashtags) %&gt;% \n  # Summarize\n  count(tweet_id, hashtags) %&gt;% \n  # Transform to DFM\n  cast_dfm(tweet_id, hashtags, n)\n\n# Create new dataset `top_hashtags_tidy`\ntop_hashtags_tidy &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize co-occurence\ntweets_dfm_cleaned %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\nCreate new dataset quanteda_dfm_cleaned:\n\nBased on the dataset quanteda_dfm,\n\nUse quanteda::dfm_select(pattern = \"#*\") to create a DFM containing only hashtags.\nUse quanteda::dfm_remove(pattern = \"#digitaldetox\") to removes the hashtag \"#digitaldetox\".\n\nSave this transformation by creating a new dataset with the name quanteda_dfm_cleaned.\n\nCreate new dataset top_hashtags_quanteda\n\nBased on the dataset quanteda_dfm_cleaned,\n\nUse topfeatures(50) o extract the top 50 most common hashtags.\nUse names() to only store the names (not the values).\n\nSave this transformation by creating a new dataset with the name top_hashtags_quanteda.\n\nVisualize co-occurence\n\nBased on the dataset quanteda_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network() .\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n# Create new dataset `quanteda_dfm_cleaned`\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Create new dataset `top_hashtags_quanteda`\ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize co-occurence\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n# Create new dataset tweets_dfm_cleaned |¬†quanteda_dfm_cleaned\n\n# Create new dataset top_hashtags_quanteda |top_hashtags_quanteda\n\n# Visualize co-occurence\n\n\n\nüìã Exercise 2: Understanding Topic 9\n\n\n\n\n\n\nObjective(s)\n\n\n\nTake a closer look at another topic from the topic model used in the session by examining the most representative tweets and the users who post the most tweets on that topic.\n\n\n\n2.1: Explore top tweets\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nArrange the selected tweets in descending order based on top_gamma values using arrange(-top_gamma).\nExtract the top 10 tweets using slice_head(n = 10).\nSelect only relevant columns (tweet_id, user_username, created_at, text, top_gamma) using select().\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 9) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt()\n\n\n\n2.1 Explore top users\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nCount the number of tweets per user using count(user_username, sort = TRUE).\nCalculate the proportion of tweets for each user by creating a new column prop using mutate(prop = round(n/sum(n)*100, 2)).\nExtract the top 15 users with the highest engagement using slice_head(n = 15).\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 9) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 15)\n\n\n\n\nüìã Exercise 3: Expolore different topic model\n\n\n\n\n\n\nObjective(s)\n\n\n\nIn the session, three models were considered for closer examination. Choose one of the other topics and recreate all the steps for the initial exploration of the topic model (as in the session).\n\n\n\n3.1 Initial exploration\n\nExplore a different topic model (estimation)\n\nBased on the dataset stm_results,\n\nUse the filter(k == X) to select a different topic model (estimation) of your choice (by defining X).\nTip: Use stm_results$k to see available options for X\nUse pull(mdl) %&gt;% .[[1]] to select the topic model (estimation) with the specified number of topics.\n\nSave this transformation by creating a new dataset with the name tpm_new.\n\nVisual exploration of tpm_new\n\nVisualize the summary of the selected topic model using plot(type = \"summary\") .\n\n\n\n# Explore a different topic model (estimation)\ntpm_new &lt;- stm_results |&gt;\n   filter(k == ) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\n\n\n# Explore a different topic model (estimation)\nn_topics &lt;- 40\n\ntpm_new &lt;- stm_results |&gt;\n   filter(k == n_topics) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\nplot(tpm_new, type = \"summary\")\n\n\n\n3.2 Document/Word-topic relations\n\nCalculate mean gamma values\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"gamma\") on tpm_new to get the gamma values.\nCalculate the mean gamma values for each topic using group_by() and summarise().\nUse arrange() to sort topics (descending) by gamma\n\nSave this transformation by creating a new dataset with the name top_gamma_new.\n\nIdentify top terms of each topic\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"beta\") on tpm_new to get the beta values.\nArrange terms within each topic in descending order based on beta values using group_by(), arrange(-beta), and top_n(10, wt = beta).\nSelect relevant columns (topic, term) using select().\nSummarize the top 10 terms for each topic using summarise(terms_beta = toString(term), .groups = \"drop\").\n\nSave this transformation by creating a new dataset with the name top_beta_new.\n\nCombine top topics and top terms\n\nJoin the dataframes top_gamma_new and top_beta_new based on the ‚Äútopic‚Äù column using left_join().\nWithin mutate(), - Adjusted topic names by topic = paste0(\"Topic \", topic) and - Reorder the dataset with topic = reorder(topic, gamma)\nSave this transformation by creating a new dataset with the name top_topics_terms_new.\n\nPreview the results\n\nDisplay a table preview of the top topics and terms with rounded gamma values using gt()\n\n\n\n# Calculate mean gamma values\n\n# Identify top terms of each topic\n\n# Combine top topics and top terms\n\n# Preview the esults\n\n\n# Calculate mean gamma values\ntop_gamma_new &lt;- tpm_new %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Identify top terms of each topic\ntop_beta_new &lt;- tpm_new %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\n# Combine top topics and top terms\ntop_topics_terms_new &lt;- top_beta_new %&gt;% \n  dplyr::left_join(top_gamma_new, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Preview the esults\ntop_topics_terms_new %&gt;% \n  gt()"
  },
  {
    "objectID": "sessions/session-09.html",
    "href": "sessions/session-09.html",
    "title": "Session 9",
    "section": "",
    "text": "üñ•Ô∏è Session 09\nüß∑ Showcase"
  },
  {
    "objectID": "sessions/session-09.html#participate",
    "href": "sessions/session-09.html#participate",
    "title": "Session 9",
    "section": "",
    "text": "üñ•Ô∏è Session 09\nüß∑ Showcase"
  },
  {
    "objectID": "sessions/session-09.html#practice",
    "href": "sessions/session-09.html#practice",
    "title": "Session 9",
    "section": "Practice",
    "text": "Practice\nüìã Exercise 09"
  },
  {
    "objectID": "sessions/session-09.html#suggested-readings",
    "href": "sessions/session-09.html#suggested-readings",
    "title": "Session 9",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\nSilge, E. H., & Julia (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-06.html",
    "href": "sessions/session-06.html",
    "title": "Session 6",
    "section": "",
    "text": "üñ•Ô∏è Session 06"
  },
  {
    "objectID": "sessions/session-06.html#participate",
    "href": "sessions/session-06.html#participate",
    "title": "Session 6",
    "section": "",
    "text": "üñ•Ô∏è Session 06"
  },
  {
    "objectID": "sessions/session-06.html#mandatory-literature",
    "href": "sessions/session-06.html#mandatory-literature",
    "title": "Session 6",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nNassen, L.-M., Vandebosch, H., Poels, K., & Karsay, K. (2023). Opt-out, abstain, unplug. A systematic review of the voluntary digital disconnection literature. Telematics and Informatics, 81, 101980. https://doi.org/10.1016/j.tele.2023.101980\nRadtke, T., Apel, T., Schenkel, K., Keller, J., & Von Lindern, E. (2022). Digital detox: An effective solution in the smartphone era? A systematic literature review. Mobile Media & Communication, 10(2), 190‚Äì215. https://doi.org/10.1177/20501579211028647\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007"
  },
  {
    "objectID": "sessions/session-06.html#other-additional-readings",
    "href": "sessions/session-06.html#other-additional-readings",
    "title": "Session 6",
    "section": "Other additional readings",
    "text": "Other additional readings\n\nMoe, H., & Madsen, O. J. (2021). Understanding digital disconnection beyond media studies. Convergence: The International Journal of Research into New Media Technologies, 27(6), 1584‚Äì1598. https://doi.org/10.1177/13548565211048969\nSchmuck, D. (2020). Does Digital Detox Work? Exploring the Role of Digital Detox Applications for Problematic Smartphone Use and Well-Being of Young Adults Using Multigroup Analysis. Cyberpsychology, Behavior, and Social Networking, 23(8), 526‚Äì532. https://doi.org/10.1089/cyber.2019.0578\nSyvertsen, T., & Enli, G. (2019). Digital detox: Media resistance and the promise of authenticity. Convergence: The International Journal of Research into New Media Technologies, 26(5-6), 1269‚Äì1283. https://doi.org/10.1177/1354856519847325\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-04.html",
    "href": "sessions/session-04.html",
    "title": "Session 4",
    "section": "",
    "text": "üîç Kontrolle der Ergebnisse des Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-04.html#prepare",
    "href": "sessions/session-04.html#prepare",
    "title": "Session 4",
    "section": "",
    "text": "üîç Kontrolle der Ergebnisse des Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-04.html#participate",
    "href": "sessions/session-04.html#participate",
    "title": "Session 4",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 04"
  },
  {
    "objectID": "sessions/session-04.html#mandatory-literature",
    "href": "sessions/session-04.html#mandatory-literature",
    "title": "Session 4",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nGardner, B. (2022). Habit and behavioural complexity: habitual instigation and execution as predictors of simple and complex behaviours. Current Research in Behavioral Sciences, 3, 100081. https://doi.org/10.1016/j.crbeha.2022.100081\nMeier, A., & Reinecke, L. (2021). Computer-Mediated Communication, Social Media, and Mental Health: A Conceptual and Empirical Meta-Review. Communication Research, 48(8), 1182‚Äì1209. https://doi.org/gjf96r\nNaab, T. K., & Schnauber, A. (2014). Habitual Initiation of Media Use and a Response-Frequency Measure for Its Examination. Media Psychology, 19(1), 126‚Äì155. https://doi.org/10.1080/15213269.2014.951055\nSchnauber-Stockmann, A., & Mangold, F. (2020). Day-to-day routines of media platform use in the digital age: A structuration perspective. Communication Monographs, 87(4), 464‚Äì483. https://doi.org/10.1080/03637751.2020.1758336\nSchnauber-Stockmann, A., Scharkow, M., & Breuer, J. (2022). Routines and the Predictability of Day-to-Day Web Use. Media Psychology, 1‚Äì23. https://doi.org/10.1080/15213269.2022.2121286"
  },
  {
    "objectID": "sessions/session-04.html#other-readings",
    "href": "sessions/session-04.html#other-readings",
    "title": "Session 4",
    "section": "Other readings",
    "text": "Other readings\n\nAnderson, I. A., & Wood, W. (2020). Habits and the electronic herd: The psychology behind social media‚Äôs successes and failures. Consumer Psychology Review, 4(1), 83‚Äì99. https://doi.org/10.1002/arcp.1063\nAnderson, I. A., & Wood, W. (2023). Social motivations‚Äô limited influence on habitual behavior: Tests from social media engagement. Motivation Science, 9(2), 107‚Äì119. https://doi.org/10.1037/mot0000292\nBayer, J. B., Anderson, I. A., & Tokunaga, R. S. (2022). Building and breaking social media habits. Current Opinion in Psychology, 45, 101303. https://doi.org/10.1016/j.copsyc.2022.101303\nBayer, J. B., & LaRose, R. (2018). Technology habits: Progress, problems, and prospects (pp. 111‚Äì130). Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0_7\nCarden, L., & Wood, W. (2018). Habit formation and change. Current Opinion in Behavioral Sciences, 20, 117‚Äì122. https://doi.org/10.1016/j.cobeha.2017.12.009\nKatevas, K., Arapakis, I., & Pielot, M. (2018). Typical phone use habits. Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services. https://doi.org/10.1145/3229434.3229441\nKruglanski, A. W., & Szumowska, E. (2020). Habitual Behavior Is Goal-Driven. Perspectives on Psychological Science, 15(5), 1256‚Äì1271. https://doi.org/10.1177/1745691620917676\nMazar, A., & Wood, W. (2018). Defining habit in psychology (pp. 13‚Äì29). Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0_2\nMeier, A. (2021). Studying problems, not problematic usage: Do mobile checking habits increase procrastination and decrease well-being? Mobile Media & Communication, 10(2), 272‚Äì293. https://doi.org/10.1177/20501579211029326\nMeier, A., Beyens, I., Siebers, T., Pouwels, J. L., & Valkenburg, P. M. (2023). Habitual social media and smartphone use are linked to task delay for some, but not all, adolescents. Journal of Computer-Mediated Communication, 28(3). https://doi.org/10.1093/jcmc/zmad008\nNaab, T. K., Karnowski, V., & Schl√ºtz, D. (2018). Reporting Mobile Social Media Use: How Survey and Experience Sampling Measures Differ. Communication Methods and Measures, 13(2), 126‚Äì147. https://doi.org/10.1080/19312458.2018.1555799\nOulasvirta, A., Rattenbury, T., Ma, L., & Raita, E. (2011). Habits make smartphone use more pervasive. Personal and Ubiquitous Computing, 16(1), 105‚Äì114. https://doi.org/10.1007/s00779-011-0412-2\nSchnauber, A. (2017). Medienselektion im alltag. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-15441-7\nSchnauber-Stockmann, A., Meier, A., & Reinecke, L. (2018). Procrastination out of Habit? The Role of Impulsive Versus Reflective Media Selection in Procrastinatory Media Use. Media Psychology, 21(4), 640‚Äì668. https://doi.org/10.1080/15213269.2018.1476156\nSchnauber-Stockmann, A., & Naab, T. K. (2018). The process of forming a mobile media habit: results of a longitudinal study in a real-world setting. Media Psychology, 22(5), 714‚Äì742. https://doi.org/10.1080/15213269.2018.1513850\nVerplanken, B. (Ed.). (2018). The psychology of habit. Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0\nWood, W., & R√ºnger, D. (2016). Psychology of Habit. Annual Review of Psychology, 67(1), 289‚Äì314. https://doi.org/10.1146/annurev-psych-122414-033417\nWood, W., Mazar, A., & Neal, D. T. (2021). Habits and Goals in Human Behavior: Separate but Interacting Systems. Perspectives on Psychological Science, 17(2), 590‚Äì605. https://doi.org/10.1177/1745691621994226\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-02.html",
    "href": "sessions/session-02.html",
    "title": "Session 2",
    "section": "",
    "text": "‚úçÔ∏è Start working on Basiskurs R/RStudio. Dealine for the course certificate (via mail to christoph.adrian@fau.de) is 13.11.2023\n\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100‚Äì118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kƒ±cƒ±man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013"
  },
  {
    "objectID": "sessions/session-02.html#prepare",
    "href": "sessions/session-02.html#prepare",
    "title": "Session 2",
    "section": "",
    "text": "‚úçÔ∏è Start working on Basiskurs R/RStudio. Dealine for the course certificate (via mail to christoph.adrian@fau.de) is 13.11.2023\n\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100‚Äì118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kƒ±cƒ±man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013"
  },
  {
    "objectID": "sessions/session-02.html#participate",
    "href": "sessions/session-02.html#participate",
    "title": "Session 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 02\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "slides/slides-09.html#schedule",
    "href": "slides/slides-09.html#schedule",
    "title": "üî® Text as data in R",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis üé•\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-09.html#looking-at-the-discourse",
    "href": "slides/slides-09.html#looking-at-the-discourse",
    "title": "üî® Text as data in R",
    "section": "Looking at the discourse",
    "text": "Looking at the discourse\nThe theoretical background: digital disconnection\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk (on Twitter) about digital detox/disconnection:\nIs social media a üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\nSource: dup-magazin.de\n\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú"
  },
  {
    "objectID": "slides/slides-09.html#social-media-as-or",
    "href": "slides/slides-09.html#social-media-as-or",
    "title": "üî® Text as data in R",
    "section": "Social Media as üíä, üëπ or üç© ?",
    "text": "Social Media as üíä, üëπ or üç© ?\nRelationship between social media use, digital well-being and digital disconnection\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      \n      \n        Social Media as a ‚Ä¶\n      \n    \n    \n      Drug\n      Demon\n      Donut\n    \n  \n  \n    What is at stake?\n\nAddiction/health\n\nDistraction\n\nWell-being\n\n    Root cause of problem\n\nIndividual susceptibility\n\nAddictive design\n\nInadequate fit\n\n    User agency\n\nAgency is limited due to innate susceptibilities\n\nAgency needs to be reclaimed from social media platforms\n\nUser has agency, but it is challenged by person-, technology- and context-specific elements\n\n    Focus of disconnection\n\nComplete abstinence, re-training of the ‚Äòfaulty brain‚Äô to break the dopamine link\n\nRemoving/weakening the distracting potential of tech, using persuasive design to support exerting social media self-control\n\nDisconnection interventions tailored to persons and/or contexts to ‚Äòoptimize the balance‚Äô between benefits and drawbacks of connectivity, mindful use\n\n    Digital disconnection examples\n\nDigital detox, cognitive behavioral therapy\n\nMuting phone, disabling notifications, putting phone in grey-scale, using apps that reward abstinence (e.g., Forest)\n\nLocative disconnection, disconnection apps that extensive tailoring to persons and contexts, mindfulness training\n\n  \n  \n  \n\n\n\n\n(Vanden Abeele et al., 2022)"
  },
  {
    "objectID": "slides/slides-09.html#fa-brands-twitter-digital-detox-digitaldetox",
    "href": "slides/slides-09.html#fa-brands-twitter-digital-detox-digitaldetox",
    "title": "üî® Text as data in R",
    "section": " digital detox, #digitaldetox, ‚Ä¶",
    "text": "digital detox, #digitaldetox, ‚Ä¶\nInformation about the data collection process\n\nGoal: Collect all tweets (until 31.12.2022) that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nActual collection was done in the beginning of 2023 (before the takeover by Musk and the associated change in API access)\nAccess via Twitter Academic Research Product Track v2 API with the help of the academictwitteR package (Barrie & Ho, 2021)\n\n\nacademictwitteR::build_query(\n  query = c(\"digital detox\", \"#digitaldetox\", \"digital_detox\"),\n  is_retweet = FALSE\n  )"
  },
  {
    "objectID": "slides/slides-09.html#a-quick-glimpse",
    "href": "slides/slides-09.html#a-quick-glimpse",
    "title": "üî® Text as data in R",
    "section": "A quick glimpse",
    "text": "A quick glimpse\nThe structure of the data set & available variables\n\ntweets_correct %&gt;% glimpse()\n\nRows: 361,408\nColumns: 37\n$ tweet_id               &lt;chr&gt; \"7223508762\", \"7222271007\", \"7219735500\", \"7216‚Ä¶\n$ user_username          &lt;chr&gt; \"Princessbride24\", \"winnerandy\", \"the_enthusias‚Ä¶\n$ text                   &lt;chr&gt; \"@Ali_Sweeney detox same week as the digital cl‚Ä¶\n$ created_at             &lt;dttm&gt; 2009-12-31 05:25:55, 2009-12-31 04:44:20, 2009‚Ä¶\n$ in_reply_to_user_id    &lt;chr&gt; \"23018333\", NA, NA, NA, NA, NA, NA, \"19475829\",‚Ä¶\n$ author_id              &lt;chr&gt; \"16157429\", \"14969949\", \"16217478\", \"18001568\",‚Ä¶\n$ lang                   &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\",‚Ä¶\n$ possibly_sensitive     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ conversation_id        &lt;chr&gt; \"7222577237\", \"7222271007\", \"7219735500\", \"7216‚Ä¶\n$ user_created_at        &lt;chr&gt; \"2008-09-06T15:13:58.000Z\", \"2008-06-01T07:34:3‚Ä¶\n$ user_protected         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ user_name              &lt;chr&gt; \"Sam\", \"Andrew\", \"The Enthusiast\", \"üåªBahSunüåª\"‚Ä¶\n$ user_verified          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶\n$ user_description       &lt;chr&gt; \"As you wish.\", \"Life is a playground, so enjoy‚Ä¶\n$ user_location          &lt;chr&gt; \"Houston\", \"California\", \"Melbourne, Australia\"‚Ä¶\n$ user_url               &lt;chr&gt; \"https://t.co/hchLJvesW1\", NA, \"http://t.co/bLK‚Ä¶\n$ user_profile_image_url &lt;chr&gt; \"https://pbs.twimg.com/profile_images/587051289‚Ä¶\n$ user_pinned_tweet_id   &lt;chr&gt; NA, NA, NA, NA, \"1285575168010735621\", NA, NA, ‚Ä¶\n$ retweet_count          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ like_count             &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ quote_count            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ user_tweet_count       &lt;int&gt; 3905, 18808, 4494, 35337, 284181, 18907, 39562,‚Ä¶\n$ user_list_count        &lt;int&gt; 7, 12, 101, 49, 540, 82, 91, 12, 117, 46, 192, ‚Ä¶\n$ user_followers_count   &lt;int&gt; 103, 481, 2118, 693, 17677, 2527, 8784, 100, 26‚Ä¶\n$ user_following_count   &lt;int&gt; 370, 1488, 391, 317, 16470, 320, 11538, 120, 25‚Ä¶\n$ sourcetweet_type       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_id         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_text       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_lang       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ sourcetweet_author_id  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ year                   &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n$ month                  &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,‚Ä¶\n$ day                    &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 29, 28, 28, 27, 27,‚Ä¶\n$ hour                   &lt;int&gt; 5, 4, 3, 1, 22, 2, 0, 20, 21, 1, 19, 8, 8, 3, 9‚Ä¶\n$ minute                 &lt;int&gt; 25, 44, 22, 42, 49, 31, 39, 23, 45, 47, 31, 28,‚Ä¶\n$ retweet_dy             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,‚Ä¶\n$ detox_dy               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE‚Ä¶"
  },
  {
    "objectID": "slides/slides-09.html#biggest-attention-in-2016-steady-decline-thereafter",
    "href": "slides/slides-09.html#biggest-attention-in-2016-steady-decline-thereafter",
    "title": "üî® Text as data in R",
    "section": "Biggest attention in 2016, steady decline thereafter",
    "text": "Biggest attention in 2016, steady decline thereafter\nDistribution of tweets with reference to digital detox over time\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = retweet_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Is the tweet a retweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#1DA1F2\", \"#004389\")) +\n    theme_pubr() +    \n    # add annotations\n    annotate(\n      \"text\", \n      x = 14, y = 55000,\n      label = \"Increase of character limit from 140 to 280\") + \n    geom_curve(\n      data = data.frame(\n        x = 14.2965001234837,y = 53507.2283841571,\n        xend = 11.5275706534335, yend = 45412.4966032138),\n      mapping = aes(x = x, y = y, xend = xend, yend = yend),\n      angle = 127L,\n      curvature = 0.28,\n      arrow = arrow(30L, unit(0.1, \"inches\"), \"last\", \"closed\"),\n      inherit.aes = FALSE)"
  },
  {
    "objectID": "slides/slides-09.html#sporadic-tweeting-on-the-topic-mainly-in-english",
    "href": "slides/slides-09.html#sporadic-tweeting-on-the-topic-mainly-in-english",
    "title": "üî® Text as data in R",
    "section": "Sporadic tweeting on the topic, mainly in English",
    "text": "Sporadic tweeting on the topic, mainly in English\nTweets with reference to ditigal detox by (participating) users and language\n\n\nExpand for full code\n# Number of tweets by user \ntweets_correct %&gt;% \n    group_by(author_id) %&gt;% \n    summarize(n = n()) %&gt;% \n    mutate(\n        n_grp = case_when(\n                     n &lt;=    1 ~ 1,\n            n &gt;   1 & n &lt;=  10 ~ 2,\n            n &gt;  10 & n &lt;=  25 ~ 3,\n            n &gt;  25 & n &lt;=  50 ~ 4,\n            n &gt;  50 & n &lt;=  75 ~ 5,\n            n &gt;  75 & n &lt;= 100 ~ 6,\n            n &gt; 100 ~ 7\n        ), \n        n_grp_fct = factor(\n            n_grp,\n            levels = c(1:7),\n            labels = c(\n            \" 1\",\n            \" 2 - 10\", \"11 - 25\",\n            \"26 - 50\", \"50 - 75\",\n            \"75 -100\", \"&gt; 100\")\n        )\n    ) %&gt;% \n    sjmisc::frq(n_grp_fct) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Tweets by users with a least on tweet\"\n    #    ) %&gt;% \n    cols_label(\n      val = \"Number of tweets by user\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n# Language of Tweets\ntweets_correct %&gt;% \n    sjmisc::frq(\n        lang, \n        sort.frq = c(\"desc\"), \n        min.frq = 2000\n    ) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Language of the collected tweets\"\n    #   ) %&gt;% \n    cols_label(\n      val = \"Twitter language code\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n\n\n\n  \n    \n    \n      Number of tweets by user\n      n\n      %raw\n      %cum\n    \n  \n  \n     1\n143364\n77.21\n77.21\n     2 - 10\n39981\n21.53\n98.74\n    11 - 25\n1647\n0.89\n99.63\n    26 - 50\n435\n0.23\n99.86\n    50 - 75\n88\n0.05\n99.91\n    75 -100\n55\n0.03\n99.94\n    &gt; 100\n109\n0.06\n100.00\n    NA\n0\n0.00\nNA\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      Twitter language code\n      n\n      %raw\n      %cum\n    \n  \n  \n    en\n274351\n75.91\n75.91\n    fr\n20407\n5.65\n81.56\n    es\n16248\n4.50\n86.05\n    de\n14091\n3.90\n89.95\n    pt\n7127\n1.97\n91.92\n    it\n5999\n1.66\n93.58\n    da\n3490\n0.97\n94.55\n    in\n3095\n0.86\n95.41\n    ja\n2428\n0.67\n96.08\n    n &lt; 2000\n14172\n3.92\n100.00\n    NA\n0\n0.00\nNA"
  },
  {
    "objectID": "slides/slides-09.html#deciding-on-the-right-method",
    "href": "slides/slides-09.html#deciding-on-the-right-method",
    "title": "üî® Text as data in R",
    "section": "Deciding on the right method",
    "text": "Deciding on the right method\nDifferent approaches of computational analysis of text\n\nNo method that is the one, but specific applications for specific methods\nApart from analyzing basic word and text metrics, there are three variants of automatic text analysis:\n\nüîé Dictionary Approaches (e.g.¬†Sentiment Analysis)\nUnsupervised Text Analysis (e.g.¬†Topic Modeling)\nSupervised Text Analysis (e.g.¬†ML Classifier)"
  },
  {
    "objectID": "slides/slides-09.html#building-a-shared-vocabulary",
    "href": "slides/slides-09.html#building-a-shared-vocabulary",
    "title": "üî® Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nImportant terms & definitions\n\n\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. ‚ÄúHello‚Äù, ‚Äú123‚Äù, and ‚Äú-‚Äù are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. ‚ÄúThe weather looks good‚Äù is an example of a sentence, and the tokens of the sentence are [‚ÄúThe‚Äù, ‚Äúweather‚Äù, ‚Äúlooks‚Äù, ‚Äúgood].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word‚Äôs id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/slides-09.html#explore-tweets-with-digitaldetox",
    "href": "slides/slides-09.html#explore-tweets-with-digitaldetox",
    "title": "üî® Text as data in R",
    "section": "Explore tweets with #digitaldetox",
    "text": "Explore tweets with #digitaldetox\nWorking through a typical text analysis using tidy data principles\n\n(Silge & Robinson, 2017)\nBut: Tidy data has a specific structure:\n\nEach variable is a column\nEach observation is a row\nEach type of observational unit is a table.\n\nThus the tidy text format is defined as a table with one-token-per-row (Silge & Robinson, 2017)."
  },
  {
    "objectID": "slides/slides-09.html#focusing-on-digitaldetox",
    "href": "slides/slides-09.html#focusing-on-digitaldetox",
    "title": "üî® Text as data in R",
    "section": "Focusing on #digitaldetox",
    "text": "Focusing on #digitaldetox\nBuild a subsample\n\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = detox_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"#digitaldetox used in tweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#1DA1F2\")) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#tokenization-of-the-tweets",
    "href": "slides/slides-09.html#tokenization-of-the-tweets",
    "title": "üî® Text as data in R",
    "section": "Tokenization of the tweets",
    "text": "Tokenization of the tweets\nTransform data to tidy text\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n    # Remove HTML entities\n    mutate(text = str_remove_all(text, remove_reg)) %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n    select(tweet_id, user_username, text) %&gt;% \n    print(n = 5)\n\n# A tibble: 639,459 √ó 3\n  tweet_id   user_username text      \n  &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;     \n1 5777201122 pblackshaw    blackberry\n2 5777201122 pblackshaw    iphone    \n3 5777201122 pblackshaw    read      \n4 5777201122 pblackshaw    pew       \n5 5777201122 pblackshaw    report    \n# ‚Ñπ 639,454 more rows"
  },
  {
    "objectID": "slides/slides-09.html#count-token-frequency",
    "href": "slides/slides-09.html#count-token-frequency",
    "title": "üî® Text as data in R",
    "section": "Count token frequency",
    "text": "Count token frequency\nSummarize all tokens over all tweets\n\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized %&gt;% \n    print(n = 15)\n\n\n# A tibble: 87,172 √ó 2\n   text             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 t.co         57530\n 2 https        52890\n 3 digitaldetox 46642\n 4 digital       8521\n 5 detox         6623\n 6 time          6000\n 7 http          4674\n 8 phone         4213\n 9 unplug        4021\n10 day           2939\n11 life          2548\n12 social        2449\n13 mindfulness   2408\n14 media         2264\n15 technology    2065\n# ‚Ñπ 87,157 more rows"
  },
  {
    "objectID": "slides/slides-09.html#the-unavoidable-word-cloud",
    "href": "slides/slides-09.html#the-unavoidable-word-cloud",
    "title": "üî® Text as data in R",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\nVisualization of Top 100 token\n\ntweets_summarized %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 30) +\n    theme_minimal()"
  },
  {
    "objectID": "slides/slides-09.html#more-than-just-single-words",
    "href": "slides/slides-09.html#more-than-just-single-words",
    "title": "üî® Text as data in R",
    "section": "More than just single words",
    "text": "More than just single words\nModeling realtionships between words: n-grams and correlations\n\n\nMany interesting text analyses are based on the relationships between words\n\nwhether examining which words tend to follow others immediately (n-grams),\nor that tend to co-occur within the same documents (correlation)\n\n\n\n\n\n(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/slides-09.html#combinations-of-words",
    "href": "slides/slides-09.html#combinations-of-words",
    "title": "üî® Text as data in R",
    "section": "Combinations of words",
    "text": "Combinations of words\nCount word pairs within tweets\n\n\n# Create word paris\ntweets_word_pairs &lt;- tweets_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        tweet_id,\n        sort = TRUE)\n\n# Preview\ntweets_word_pairs %&gt;% \n    print(n = 15)\n\n\n# A tibble: 3,466,216 √ó 3\n   item1        item2            n\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;\n 1 t.co         digitaldetox 40640\n 2 digitaldetox t.co         40640\n 3 t.co         https        36605\n 4 https        t.co         36605\n 5 https        digitaldetox 36486\n 6 digitaldetox https        36486\n 7 digital      digitaldetox  7791\n 8 digitaldetox digital       7791\n 9 t.co         digital       7365\n10 digital      t.co          7365\n11 https        digital       6856\n12 digital      https         6856\n13 detox        digitaldetox  6004\n14 digitaldetox detox         6004\n15 t.co         detox         5672\n# ‚Ñπ 3,466,201 more rows"
  },
  {
    "objectID": "slides/slides-09.html#correlation-of-words",
    "href": "slides/slides-09.html#correlation-of-words",
    "title": "üî® Text as data in R",
    "section": "Correlation of words",
    "text": "Correlation of words\nSummarize and correlate tokens within tweets\n\n\n# Create word correlation\ntweets_pairs_corr &lt;- tweets_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        tweet_id, \n        sort = TRUE)\n\n# Preview\ntweets_pairs_corr %&gt;% \n    print(n = 15)\n\n\n# A tibble: 41,820 √ó 3\n   item1           item2           correlation\n   &lt;chr&gt;           &lt;chr&gt;                 &lt;dbl&gt;\n 1 jocelyn         brewer                0.999\n 2 brewer          jocelyn               0.999\n 3 jocelyn         discusses             0.983\n 4 discusses       jocelyn               0.983\n 5 discusses       brewer                0.982\n 6 brewer          discusses             0.982\n 7 icphenomenallyu discuss               0.981\n 8 discuss         icphenomenallyu       0.981\n 9 taniamulry      wealth                0.979\n10 wealth          taniamulry            0.979\n11 jocelyn         nutrition             0.968\n12 nutrition       jocelyn               0.968\n13 nutrition       brewer                0.967\n14 brewer          nutrition             0.967\n15 discusses       nutrition             0.951\n# ‚Ñπ 41,805 more rows"
  },
  {
    "objectID": "slides/slides-09.html#correlates-of-detox-and-digital",
    "href": "slides/slides-09.html#correlates-of-detox-and-digital",
    "title": "üî® Text as data in R",
    "section": "Correlates of detox and digital",
    "text": "Correlates of detox and digital\nDisplay correlates for specific token\n\n\ntweets_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\"detox\", \"digital\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#04316A\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#lets-talk-about-sentiments",
    "href": "slides/slides-09.html#lets-talk-about-sentiments",
    "title": "üî® Text as data in R",
    "section": "Let‚Äôs talk about sentiments",
    "text": "Let‚Äôs talk about sentiments\nDictionary based approach of text analysis\n\n(Silge & Robinson, 2017)\n\n\n\n\n\nAtteveldt et al. (2021) argue that sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries."
  },
  {
    "objectID": "slides/slides-09.html#the-meaning-of-positivenegative",
    "href": "slides/slides-09.html#the-meaning-of-positivenegative",
    "title": "üî® Text as data in R",
    "section": "The meaning of ‚Äúpositive/negative‚Äù",
    "text": "The meaning of ‚Äúpositive/negative‚Äù\nMost commmon positive and negative words\n\n\ntweets_sentiment_count &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\ntweets_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007D29\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#enrich-the-original-data",
    "href": "slides/slides-09.html#enrich-the-original-data",
    "title": "üî® Text as data in R",
    "section": "Enrich the original data",
    "text": "Enrich the original data\nLink word sentiment to tidy data\n\ntweets_sentiment &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(tweet_id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\ntweets_sentiment \n\n# A tibble: 24,372 √ó 4\n   tweet_id            positive negative sentiment\n   &lt;chr&gt;                  &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 1000009901563838465        3        0         3\n 2 1000038819520008193        1        0         1\n 3 1000042717492187136        4        0         4\n 4 1000043574673715203        1        1         0\n 5 1000075155891281925        4        0         4\n 6 1000086637987139590        1        0         1\n 7 1000094334660825088        1        0         1\n 8 1000133715194920960        1        0         1\n 9 1000255467434729472        0        1        -1\n10 1000271209353895938        1        1         0\n# ‚Ñπ 24,362 more rows"
  },
  {
    "objectID": "slides/slides-09.html#slightly-more-positive-tweets-than-negative",
    "href": "slides/slides-09.html#slightly-more-positive-tweets-than-negative",
    "title": "üî® Text as data in R",
    "section": "Slightly more positive tweets than negative",
    "text": "Slightly more positive tweets than negative\nOverall distribution sentiment by tweets\n\ntweets_sentiment %&gt;% \n  ggplot(aes(as.factor(sentiment))) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"Sentiment (sum) of tweet\", \n    y = \"Number of tweets\"\n  ) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#a-trend-towards-positivity",
    "href": "slides/slides-09.html#a-trend-towards-positivity",
    "title": "üî® Text as data in R",
    "section": "A trend towards positivity?",
    "text": "A trend towards positivity?\nDevelopment of tweet sentiment over the years\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Proportion of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "slides/slides-09.html#and-now-you-clean-and-repeat",
    "href": "slides/slides-09.html#and-now-you-clean-and-repeat",
    "title": "üî® Text as data in R",
    "section": "üß™ And now ‚Ä¶ you: Clean and repeat!",
    "text": "üß™ And now ‚Ä¶ you: Clean and repeat!\nRedo the tidy text analysis pipeline with cleaned data\n\n\n\n\n\n\nObjective of this exercise\n\n\n\nBrush up basic knowledge of working with R and the tidyverse\nGet to know the typical steps of tidy text analysis, from tokenisation and summarisation to visualisation.\n\n\n\n\nNext steps\n\nDownload files provided on StudOn or shared drives for the sessions\nUnzip the archive at a destination of your choice.\nDouble click on the Exercise-Text_as_data.Rproj to open the RStudio project. This ensures that all dependencies are working correctly.\nOpen the exercise.qmd file and follow the instructions.\nTip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs)."
  },
  {
    "objectID": "slides/slides-09.html#references",
    "href": "slides/slides-09.html#references",
    "title": "üî® Text as data in R",
    "section": "References",
    "text": "References\n\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\n\n\nBarrie, C., & Ho, J. (2021). academictwitteR: An r package to access the twitter academic research product track v2 API endpoint. Journal of Open Source Software, 6(62), 3272. https://doi.org/10.21105/joss.03272\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\n\n\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-03.html#seminarplan",
    "href": "slides/slides-03.html#seminarplan",
    "title": "üî® Working with R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-03.html#kurzes-organisatorische-update",
    "href": "slides/slides-03.html#kurzes-organisatorische-update",
    "title": "üî® Working with R",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Pr√ºfungsleistungen\n\nAlle Zertifikat vom R-Basiskurs erhalten üéâ\nInformationen zum Kursablauf wurden geupdatet, Update zu Pr√ºfungleistungen folgt noch\nF√ºr alle Pr√ºfungleistungen gilt: üá©üá™ ist immer m√∂glich, aber gerne üá¨üáß\nüó£Ô∏è 2. Pr√§sentationsgruppe: Denken Sie bitte\n\nan die Zusendung des Entwurf der Pr√§sentationsfolien bis sp√§testens n√§chste Woche Dienstag 11:00!\ndas Feedbackgespr√§ch am Mittwoch im Anschluss an das Seminar."
  },
  {
    "objectID": "slides/slides-03.html#buliding-best-practice",
    "href": "slides/slides-03.html#buliding-best-practice",
    "title": "üî® Working with R",
    "section": "Buliding best practice",
    "text": "Buliding best practice\nWillkommen (zur√ºck) zu R\n\n\n\nHow most academics learn R:\n\n\n\n\n\n\n\nHow you should learn R:\n\nVersuchen Sie R nicht systematisch zu lernen, sondern spezifisch anzuwenden.\nOrganisieren Sie Ihre Arbeit in R (mit Projekten)\nSchreiben Sie lesbaren und nachvollziehbaren Code!\nFragen Sie nach!"
  },
  {
    "objectID": "slides/slides-03.html#ein-repository-voller-daten",
    "href": "slides/slides-03.html#ein-repository-voller-daten",
    "title": "üî® Working with R",
    "section": "Ein Repository voller Daten",
    "text": "Ein Repository voller Daten\nBeispiel f√ºr √úbung durch Anwendung:  tidytuesday (social data project)\n\n\n\nData is posted to social media every Monday morning.\nExplore the data, watching out for interesting relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "slides/slides-03.html#beispiele-f√ºr-tidytuesday",
    "href": "slides/slides-03.html#beispiele-f√ºr-tidytuesday",
    "title": "üî® Working with R",
    "section": "Beispiele f√ºr #tidytuesday",
    "text": "Beispiele f√ºr #tidytuesday"
  },
  {
    "objectID": "slides/slides-03.html#everything-you-need-in-one-place",
    "href": "slides/slides-03.html#everything-you-need-in-one-place",
    "title": "üî® Working with R",
    "section": "Everything you need in one place",
    "text": "Everything you need in one place\nOrganisation der Arbeit mit RStudio-Projekten\n\n\n\n\n\n\n\n\nEmpfehlungen:\n\nF√ºr jedes Projekt ein RStudio-Projekt.\nSicherung und Organisation von Daten, Skripte und Ouput an einem Ort, z.B. mit Unterst√ºtzung durch R-Pakete wie z.B. prodigenr\nVerwenden Sie immer nur relative, keine absoluten Pfade. Empfehlung: here R-Paket"
  },
  {
    "objectID": "slides/slides-03.html#versionskontrolle-als-k√ºr",
    "href": "slides/slides-03.html#versionskontrolle-als-k√ºr",
    "title": "üî® Working with R",
    "section": "Versionskontrolle als K√ºr",
    "text": "Versionskontrolle als K√ºr\nCrashkurs zu Git(Hub)\n\n MalikaIhle\nVersionkontrolle f√ºr Code, gesichert in der Cloud\nVollst√§ndige R√ºckverfolgbarkeit von (gesicherten) √Ñnderungen\nGreat effort, great return."
  },
  {
    "objectID": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "href": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "title": "üî® Working with R",
    "section": "Run chunks, not (whole) scripts",
    "text": "Run chunks, not (whole) scripts\nOutputorientiertes Coding mit Quarto"
  },
  {
    "objectID": "slides/slides-03.html#rscript-rmarkdown-quarto",
    "href": "slides/slides-03.html#rscript-rmarkdown-quarto",
    "title": "üî® Working with R",
    "section": "RScript ‚â§ RMarkdown ‚â§ Quarto",
    "text": "RScript ‚â§ RMarkdown ‚â§ Quarto\nDer Weg vom Code zum Output\n\n\nGrundidee von Quarto : ein Quelldokument kann in eine Vielzahl von Ausgabeformaten umgewandelt werden\nMarkdown-Syntax f√ºr Text, verschiedene Programmiersprachen (wie z.B. R und Python) in einem Dokument"
  },
  {
    "objectID": "slides/slides-03.html#develop-your-style",
    "href": "slides/slides-03.html#develop-your-style",
    "title": "üî® Working with R",
    "section": "Develop your style",
    "text": "Develop your style\nWichtigkeit der Codeformatierung und -dokumentierung\n\n# Strive for \nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHTFTS &lt;- flights |&gt; filter(air_time &lt; 60)\n\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\n\n\nDie Entwicklung (oder Aneignung) eines Codestils ist wichtig!\nWas sich zun√§chst willk√ºrlich anf√ºhlt, hilft Ihnen mit der Zeit sehr\nUnterst√ºtzung durch den tidyverse style guide bzw. die Pakete styler oder lintr"
  },
  {
    "objectID": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "href": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "title": "üî® Working with R",
    "section": "Empfehlung: tidyverse is your friend!",
    "text": "Empfehlung: tidyverse is your friend!\nVerschiedenen Paketen f√ºr alle Schritte eines Projektes\n\nQuelle: RStudio"
  },
  {
    "objectID": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "href": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "title": "üî® Working with R",
    "section": "The friend of your friend: easystats",
    "text": "The friend of your friend: easystats\nFokus auf die Analyse\n\nQuelle: L√ºdecke et al. (2022)"
  },
  {
    "objectID": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "href": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "title": "üî® Working with R",
    "section": "Am Anfang steht die Theorie",
    "text": "Am Anfang steht die Theorie\nTypischer ‚Äúdata science process‚Äù als Kontext der Sitzung\n\nQuelle: Wickham et al. (2023)\n\nAdditional steps (add if necessary):"
  },
  {
    "objectID": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "href": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "title": "üî® Working with R",
    "section": "Age difference in years between move love interests",
    "text": "Age difference in years between move love interests\nDatengrundlage f√ºr die Beispiele: Hollywood Age Gap ( |  )\n\n\n\n\n\n\n\n\n\n\n‚ÄúAn informational site showing the age gap between movie love interests.‚Äù\nCommunity-Projekt\n\nGuidlines for participation/submission:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "slides/slides-03.html#explore-adapt-repeat",
    "href": "slides/slides-03.html#explore-adapt-repeat",
    "title": "üî® Working with R",
    "section": "Explore ‚ûû Adapt ‚ûû Repeat ‚ü≥",
    "text": "Explore ‚ûû Adapt ‚ûû Repeat ‚ü≥\nProzess der Datenaufbereitung\n\n\nnimmt in der Regel den Gro√üteil der Zeit der Datenanalyse in Anspruch\nh√§ufig bedarf es der mehrfachen Wiederholung dreier Schritte:\n\nder (explorativen) Erkundung,\nder Standartdisierung und\nder (erneuten) Bereinung der Daten"
  },
  {
    "objectID": "slides/slides-03.html#drei-stufen-der-datenqualit√§t",
    "href": "slides/slides-03.html#drei-stufen-der-datenqualit√§t",
    "title": "üî® Working with R",
    "section": "Drei Stufen der Datenqualit√§t",
    "text": "Drei Stufen der Datenqualit√§t\nTypische Strategien zur Datenbereinigung nach Pearson (2018)\n\n\n\n\n\n\nQuelle: Jonge & Loo (2013)\n\n\n\n\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nWie viele F√§lle sind enthalten? Wie viele Variablen?\nWie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\nUntersuchung deskriptiver Statistiken f√ºr jede Variable;\nExplorative Visualisierung;\nVerschiedene Verfahren zur Suche nach Anomalien in den Daten;\nUntersuchung der Beziehungen zwischen Schl√ºsselvariablen mit Hilfe von Scatterplots/Boxplots/Mosaic-Plots;\nDokumentation des Vorgehens und der Ergebnisse (z.B. mit .rmd-Dokument). Dient als Grundlage f√ºr die anschlie√üende Analyse und Erl√§uterung der Ergebnisse."
  },
  {
    "objectID": "slides/slides-03.html#direkter-download-via-url",
    "href": "slides/slides-03.html#direkter-download-via-url",
    "title": "üî® Working with R",
    "section": "Direkter Download via URL",
    "text": "Direkter Download via URL\nDatenimport und -preview\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nüîç Wie viele F√§lle sind enthalten? Wie viele Variablen?\nüîç Wie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") \nage_gaps \n\n# A tibble: 1,177 √ó 12\n   `Movie Name`       `Release Year` Director    `Age Difference` `Actor 1 Name`\n   &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         \n 1 Harold and Maude             1971 Hal Ashby                 52 Bud Cort      \n 2 Venus                        2006 Roger Mich‚Ä¶               50 Peter O'Toole \n 3 The Quiet American           2002 Phillip No‚Ä¶               49 Michael Caine \n 4 The Big Lebowski             1998 Joel Coen                 45 David Huddles‚Ä¶\n 5 Beginners                    2010 Mike Mills                43 Christopher P‚Ä¶\n 6 Poison Ivy                   1992 Katt Shea                 42 Tom Skerritt  \n 7 Whatever Works               2009 Woody Allen               40 Larry David   \n 8 Entrapment                   1999 Jon Amiel                 39 Sean Connery  \n 9 Husbands and Wives           1992 Woody Allen               38 Woody Allen   \n10 Magnolia                     1999 Paul Thoma‚Ä¶               38 Jason Robards \n# ‚Ñπ 1,167 more rows\n# ‚Ñπ 7 more variables: `Actor 1 Gender` &lt;chr&gt;, `Actor 1 Birthdate` &lt;date&gt;,\n#   `Actor 1 Age` &lt;dbl&gt;, `Actor 2 Name` &lt;chr&gt;, `Actor 2 Gender` &lt;chr&gt;,\n#   `Actor 2 Birthdate` &lt;chr&gt;, `Actor 2 Age` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-03.html#let-the-cleaning-beginn",
    "href": "slides/slides-03.html#let-the-cleaning-beginn",
    "title": "üî® Working with R",
    "section": "Let the cleaning beginn",
    "text": "Let the cleaning beginn\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\nüîç Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps %&lt;&gt;% janitor::clean_names()\nage_gaps %&gt;% glimpse()\n\nRows: 1,177\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"‚Ä¶\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992‚Ä¶\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel‚Ä¶\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, ‚Ä¶\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David‚Ä¶\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma‚Ä¶\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192‚Ä¶\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, ‚Ä¶\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", ‚Ä¶\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", ‚Ä¶\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0‚Ä¶\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, ‚Ä¶"
  },
  {
    "objectID": "slides/slides-03.html#building-the-habits",
    "href": "slides/slides-03.html#building-the-habits",
    "title": "üî® Working with R",
    "section": "Building the habits!",
    "text": "Building the habits!\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nReminder\n\nVer√§nderungen nicht im selben Datensatz speichern\nVerst√§ndliche Benennung & Kommentierung der Daten\nBearbeitungsschritte kommentieren"
  },
  {
    "objectID": "slides/slides-03.html#kontrolle-der-lageparameter",
    "href": "slides/slides-03.html#kontrolle-der-lageparameter",
    "title": "üî® Working with R",
    "section": "Kontrolle der Lageparameter",
    "text": "Kontrolle der Lageparameter\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nüîç Wie viele eindeutige Werte hat jede Variable?\nüîç Welcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\nüîç Gibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct %&gt;% sjmisc::descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1177      0 2000.74 16.67 0.49 2004\n age_difference numeric age_difference 1177      0   10.48  8.53 0.25    8\n    actor_1_age numeric    actor_1_age 1177      0   39.97 10.90 0.32   39\n    actor_2_age numeric    actor_2_age 1177      0   31.27  8.50 0.25   30\n trimmed          range iqr  skew\n 2003.65 88 (1935-2023)  15 -1.68\n    9.41      52 (0-52)  12  1.19\n   39.41     64 (17-81)  15  0.53\n   30.42     64 (17-81)   9  1.39"
  },
  {
    "objectID": "slides/slides-03.html#lets-start-exploring",
    "href": "slides/slides-03.html#lets-start-exploring",
    "title": "üî® Working with R",
    "section": "Let‚Äôs start exploring!",
    "text": "Let‚Äôs start exploring!\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(age_difference)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#a-recent-past",
    "href": "slides/slides-03.html#a-recent-past",
    "title": "üî® Working with R",
    "section": "A recent past ‚Ä¶",
    "text": "A recent past ‚Ä¶\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,177 √ó 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ‚Ñπ 1,167 more rows"
  },
  {
    "objectID": "slides/slides-03.html#or-still-present",
    "href": "slides/slides-03.html#or-still-present",
    "title": "üî® Working with R",
    "section": "‚Ä¶ or still present?",
    "text": "‚Ä¶ or still present?\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name)\n\n# A tibble: 12 √ó 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal‚Ä¶\n 2 Oppenheimer                         20         2023 Cillian Mur‚Ä¶ Florence Pu‚Ä¶\n 3 The Northman                        20         2022 Alexander S‚Ä¶ Anya Taylor‚Ä¶\n 4 The Lost City                       16         2022 Channing Ta‚Ä¶ Sandra Bull‚Ä¶\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb‚Ä¶\n 6 Everything Everywhere ‚Ä¶              9         2022 Ke Huy Quan  Michelle Ye‚Ä¶\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co‚Ä¶\n 8 Oppenheimer                          7         2023 Cillian Mur‚Ä¶ Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc‚Ä¶ Zo√´ Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi‚Ä¶ Reese Withe‚Ä¶\n11 Your Place or Mine                   2         2023 Ashton Kutc‚Ä¶ Reese Withe‚Ä¶\n12 You People                           1         2023 Jonah Hill   Lauren Lond‚Ä¶"
  },
  {
    "objectID": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "href": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "title": "üî® Working with R",
    "section": "(Durchschnitts-)Unterschied nach Jahren",
    "text": "(Durchschnitts-)Unterschied nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#verteilung-nach-jahren",
    "href": "slides/slides-03.html#verteilung-nach-jahren",
    "title": "üî® Working with R",
    "section": "Verteilung nach Jahren",
    "text": "Verteilung nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\"\n  ) +\n  # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))"
  },
  {
    "objectID": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "href": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "title": "üî® Working with R",
    "section": "Ein Blick auf die Korrelation",
    "text": "Ein Blick auf die Korrelation\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1175) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.68 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1177"
  },
  {
    "objectID": "slides/slides-03.html#mit-kanonen-auf-spatzen-schie√üen",
    "href": "slides/slides-03.html#mit-kanonen-auf-spatzen-schie√üen",
    "title": "üî® Working with R",
    "section": "Mit Kanonen auf Spatzen schie√üen",
    "text": "Mit Kanonen auf Spatzen schie√üen\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n# Sch√§tzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n\n\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1175) |      p\n------------------------------------------------------------------------\n(Intercept)  |      234.30 | 29.15 | [177.11, 291.50] |    8.04 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.68 | &lt; .001\n\n\n\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8334.623 | 8334.643 | 8349.835 | 0.048 |     0.047 | 8.324 | 8.331"
  },
  {
    "objectID": "slides/slides-03.html#convenience-wrapper",
    "href": "slides/slides-03.html#convenience-wrapper",
    "title": "üî® Working with R",
    "section": "Convenience wrapper",
    "text": "Convenience wrapper\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1175) = 58.96, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 234.30 (95% CI [177.11, 291.50], t(1175) = 8.04, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1175) = -7.68, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "slides/slides-03.html#try---fail---repeat",
    "href": "slides/slides-03.html#try---fail---repeat",
    "title": "üî® Working with R",
    "section": "Try - fail - repeat",
    "text": "Try - fail - repeat\nKurzes Fazit der heutigen Sitzung\n\n\n\n\nWenn R, dann mit RStudio + Quarto!\nAnschauen - nachmachen - ausprobieren\nKeep it tidy\n(Gute) Routinen bilden\n‚ÄúThere is almost always a package for that ‚Ä¶‚Äù"
  },
  {
    "objectID": "slides/slides-03.html#literatur",
    "href": "slides/slides-03.html#literatur",
    "title": "üî® Working with R",
    "section": "Literatur",
    "text": "Literatur\n\n\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\n\n\nL√ºdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Th√©riault, R., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. CRAN. https://easystats.github.io/easystats/\n\n\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group.\n\n\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: import, tidy, transform, visualize, and model data (2nd edition). O‚ÄôReilly.\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-01.html#und-nun-zu-ihnen",
    "href": "slides/slides-01.html#und-nun-zu-ihnen",
    "title": "Kick-Off",
    "section": "Und nun zu Ihnen!",
    "text": "Und nun zu Ihnen!\nVorstellungsrunde\n\nWie hei√üen Sie?\nWas und wo haben Sie im Bachelor studiert?\nWas studieren Sie aktuell?\nWelches soziale Netzwerk/Medium haben Sie letzte Woche am meisten genutzt und warum?\n\n\n\nHintegrund und Vorwissen\nMediennutzung"
  },
  {
    "objectID": "slides/slides-01.html#was-verstehen-sie-unter-digital-behavioral-data",
    "href": "slides/slides-01.html#was-verstehen-sie-unter-digital-behavioral-data",
    "title": "Kick-Off",
    "section": "Was verstehen Sie unter Digital Behavioral Data?",
    "text": "Was verstehen Sie unter Digital Behavioral Data?\nBitte an Umfrage teilnehmen\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/alhdxw6x3a6e\nTemporary Access Code: 2677 1451"
  },
  {
    "objectID": "slides/slides-01.html#ergebnis",
    "href": "slides/slides-01.html#ergebnis",
    "title": "Kick-Off",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "href": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "title": "Kick-Off",
    "section": "Ein Definitionsversuch von DBD",
    "text": "Ein Definitionsversuch von DBD\nnach Weller (2021)\n\n\n‚Ä¶ fasst eine Vielzahl von m√∂glichen Datenquellen zusammen, die verschiedene Arten von Aktivit√§ten aufzeichnen\n‚Ä¶ k√∂nnen dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-01.html#lernziele",
    "href": "slides/slides-01.html#lernziele",
    "title": "Kick-Off",
    "section": "Lernziele",
    "text": "Lernziele\nDie Studierenden werden ‚Ä¶\n\neinen √úberblick √ºber die zentralen M√∂glichkeiten von DBD und die damit verbundenen Herausforderungen bei der Datenerhebung und -aufbereitung bekommen\nlernen die St√§rken und Schw√§chen verschiedener Methoden zur Erhebung von DBD bewerten\nzentrale Anforderungen an Datenschutz, Forschungsethik und Datenqualit√§t kennen und verstehen lernen\nzentrale sozialwissenschaftliche Methoden zur Analyse von DBD kennenlernen\ndas Wissen √ºber DBD, Statistik und Datenanalyse in eigenen kleinen Projekten zu √ºben und anzuwenden"
  },
  {
    "objectID": "slides/slides-01.html#aber-was-ist-mit-.",
    "href": "slides/slides-01.html#aber-was-ist-mit-.",
    "title": "Kick-Off",
    "section": "Aber was ist mit ‚Ä¶. ?",
    "text": "Aber was ist mit ‚Ä¶. ?\nKurzes FAQ mit h√§ufig gestellten Fragen\n\nWelchen Vorkenntnisse sind f√ºr den Kurs vorausgesetzt? Interesse an sozialwissenschaftlichen Perspektiven auf Medien, Kommunikation und digitale Technologien & Grundkenntnisse in der Arbeit mit Statistikprogrammen (z.B. R, Python, Stata, SPSS)\nWerden wir praktisch mit Statistikprogrammen arbeiten? Ja. Dazu werden wir R bzw. RStudio nutzen.\n\nDeswegen: Bitte üíª mitbringen!\n\nWerden wir die mathematische Grundlagen der vorgestellten Methoden lernen? Ja und Nein. Der Kurs konzentriert sich in erster Linie auf die Anwendung; einige mathematische Parameter der vorgestellten Methoden werden jedoch f√ºr die Anwendung ben√∂tigt und deswegen kurz er√∂rtert.\n\n\n\nLaptop mit RStudio f√ºr alle m√∂glich?"
  },
  {
    "objectID": "slides/slides-01.html#different-tools-for-different-tasks",
    "href": "slides/slides-01.html#different-tools-for-different-tasks",
    "title": "Kick-Off",
    "section": "Different tools for different tasks",
    "text": "Different tools for different tasks\nKursorganisation & -kommunikation\n\nGithub-Kursseite: Informationen zu Kurs (Semesterplan, Syllabus, Pr√ºfungleistungen etc). & Sitzungen (Slides, Literatur und ggf. √úbungsmaterial)\nZulip: Wichtige Ank√ºndigungen, asynchrone Unterhaltungen, Fragen zum Kurs & zu R\nStudOn: Kursmaterialien, ggf. Beispieldatens√§tze und Pr√§sentationsaufnahmen\nE-Mail: pers√∂nliche Anliegen\n\n\n\nKurze Vorf√ºhrung der Webseite\nZulip Frage/Probleme bei Registrierung?\nOptional: Github/OSF"
  },
  {
    "objectID": "slides/slides-01.html#what-is-expected",
    "href": "slides/slides-01.html#what-is-expected",
    "title": "Kick-Off",
    "section": "What is expected",
    "text": "What is expected\nLeistungsanforderungen & Pr√ºfungsleistungen\n\n\n\n\n\n\n\nRegelm√§√üige Teilnahme\n\nmindestens 80% der Sitzungen\nmax. 2 unentschuldigte Fehltermine\n\n\n\n\n\n\n\nPortfolio\n\nVerschiedene Teilleistungen (ins. 100 Punkte)\n\n\n\n\n\nFrage: Syllabus gelesen?\nFrage: Assignments angeschaut?"
  },
  {
    "objectID": "slides/slides-01.html#schritt-f√ºr-schritt-zum-ziel",
    "href": "slides/slides-01.html#schritt-f√ºr-schritt-zum-ziel",
    "title": "Kick-Off",
    "section": "Schritt f√ºr Schritt zum Ziel",
    "text": "Schritt f√ºr Schritt zum Ziel\nDas Portfolio im √úberblick\n\n\nAusf√ºhrliche Informationen zu den einzelnen Portfolio-Elementen finden Sie auf der Kursseite unter Assignments.\n\n\n\nNur Short Report am Ende des Semesters\nGgf. wird Bewertungsschema angepasst\n‚ÄúIdeal-Konzept‚Äù vs.¬†Realit√§t (bedingt durch Diskrepanz zwischen Anmeldungen & Teilnahme)"
  },
  {
    "objectID": "slides/slides-01.html#pr√§sentation",
    "href": "slides/slides-01.html#pr√§sentation",
    "title": "Kick-Off",
    "section": "üë• Pr√§sentation",
    "text": "üë• Pr√§sentation\nüí° Theoretische & methodische Grundlage f√ºr das Mini-Projekt\n\nUmfang: maximal 30 Minuten\nZiel: √úberblick √ºber das Thema der zentralen Texte geben, z.B. zentrale Begriffe, Definitionen und Merkmale der jeweiligen Plattform, Methode und/oder des Tools\nLiteratur wird zur Verf√ºgung gestellt\n\n\nBessonderheit: vorheriges Feedbackgespr√§ch"
  },
  {
    "objectID": "slides/slides-01.html#project-topic-ideas",
    "href": "slides/slides-01.html#project-topic-ideas",
    "title": "Kick-Off",
    "section": "üë• Project topic idea(s)",
    "text": "üë• Project topic idea(s)\nüí° Projektidee vorstellen & weiterentwickeln\n\nUmfang: maximal 10 Minuten & 5 Slides\nZiel: Idee f√ºr Gruppenprojekt pr√§sentieren, offene Fragen kl√§ren und Zeit f√ºr Diskussion & Feedback\nRaum f√ºr Fragen und Austausch zwischen den verschiedenen Projektgruppen\n\n\nBessonderheit: Einzige Leistung, die f√ºr beide Projekte erbracht werden muss."
  },
  {
    "objectID": "slides/slides-01.html#project-proposal",
    "href": "slides/slides-01.html#project-proposal",
    "title": "Kick-Off",
    "section": "üë• Project proposal",
    "text": "üë• Project proposal\nüí° Erster Entwurf des Written short report\n\nUmfang: mindestens 500 W√∂rter\nZiel: Forschungsfrage (weiter-)entwickeln und verschriftlichen sowie fr√ºhzeitige Entwicklung einer spezifischen Analysestrategie\nFokus auf drei ‚ÄúAbschnitte‚Äù: Einleitung, Datengrundlage und methodisches Vorgehen\n\n\nBessonderheit: Grundlage f√ºr das Peer Review\n\n\n(Teilweise) W√§hrend des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#peer-review",
    "href": "slides/slides-01.html#peer-review",
    "title": "Kick-Off",
    "section": "üë§ Peer Review",
    "text": "üë§ Peer Review\nüí° Feedback f√ºr Bericht geben & bekommmen\n\nUmfang: Durcharbeiten eines Peer-Review-Formulars\nZiel(e):\n\nLernen, andere Projekte zu bewerten und konstruktives Feedback zu schreiben\nZus√§tzliches Feedback √ºber das eigene Projekte erhalten, dass f√ºr den finalen Written short report ber√ºcksichtigt werden kann\n\n\n\nBesonderheit: Individuelles Assigment!\n\n\n(Teilweise) W√§hrend des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#short-report",
    "href": "slides/slides-01.html#short-report",
    "title": "Kick-Off",
    "section": "üë• Short Report",
    "text": "üë• Short Report\nüí° Zusammenf√ºhrung der einzelnen Teileistungen\n\n\nUmfang:\n\n750 bis 1000 W√∂rter pro Person umfassen.\nBei einem Gruppenbericht skaliert die Anzahl der W√∂rter mit einem Faktor von 0,8 pro Person (z. B. sollte eine Zweiergruppe 1200 bis 1600 W√∂rter schreiben, eine Dreiergruppe 1800 bis 2400 W√∂rter).\n\nZiel(e):\n\nmindestens eine der vorgestellten Methoden oder Daten verwendet, um ein Thema Ihrer Wahl zu erforschen.\nKenntnisse der in diesem Kurs behandelten Themen (und dar√ºber hinaus, wenn Sie m√∂chten!) unter Beweis stellen und diese auf einen Datensatz anwenden, um ihn auf sinnvolle Weise zu analysieren.\n\n\n\nBesonderheit: Abgabe als Quarto-Dokument (& PDF)\n\n\nFinale Abgabe erst nach Ende des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#drei-themenbl√∂cke",
    "href": "slides/slides-01.html#drei-themenbl√∂cke",
    "title": "Kick-Off",
    "section": "Drei Themenbl√∂cke",
    "text": "Drei Themenbl√∂cke\nStruktur und Aufbau des Seminars"
  },
  {
    "objectID": "slides/slides-01.html#about-r",
    "href": "slides/slides-01.html#about-r",
    "title": "Kick-Off",
    "section": "About R ‚Ä¶",
    "text": "About R ‚Ä¶"
  },
  {
    "objectID": "slides/slides-01.html#trust-the-process",
    "href": "slides/slides-01.html#trust-the-process",
    "title": "Kick-Off",
    "section": "Trust the process",
    "text": "Trust the process\nDer Einsatz von R bzw. RStudio im Kurs\nWarum?\n\nKostenlose Software mit vielen n√ºtzlichen und beginner-friendly Tutorials\nR or Python? Both!\n\nIm Kurs:\n\nBestehende R-Kenntnisse sind f√∂rderlich, aber nicht zwigend notwendig, wichtiger sind praktische Erfahrung im syntaxbasierten Arbeiten\nLearn to code by example: Code von Sitzungen & Beispielen wird bereitgestellt (ggf. durch Showcases)\nPflicht: Basiskurs R/RStudio der FAU\nN√ºtzliche Quellen auf Kursseite unter Computing"
  },
  {
    "objectID": "slides/slides-01.html#theorie-meets-praxis",
    "href": "slides/slides-01.html#theorie-meets-praxis",
    "title": "Kick-Off",
    "section": "Theorie meets Praxis",
    "text": "Theorie meets Praxis\nProjektarbeit in Kleingruppen\nDurchf√ºhrung von zwei Miniprojekte (üìÅ) mit je vier Sitzungen:\n\nüìö Theoretische Grundlage aus der Kommunikationswissenschaft\nüì¶ Zentrale Methode der Datenerhebung im Kontext des Miniprojektes\nüî® Vorstellung & Anwendung von Methoden & Analysestrategie\nüìä Vorstellung & Diskussion von Projektideen\n\nüí° Idee:\n\nWissen aneignen ‚ûû anwenden ‚ûû teilen/pr√§sentieren ‚ûû diskutieren üîÑ"
  },
  {
    "objectID": "slides/slides-01.html#typische-session",
    "href": "slides/slides-01.html#typische-session",
    "title": "Kick-Off",
    "section": "Typische Session",
    "text": "Typische Session\nf√ºr üìö, üì¶ & üî®: Erst Pr√§sentation, dann Vertiefung\n\n\nPr√§sentation (ca. 30-45 Min)\n\nUmfasst eine bzw. Ihre Pr√§sentation (inkl. Zeit f√ºr Fragen und Diskussionen)\nOption auf weitere, offenere Diskussion im Kurs\n\n\nGroup Activity (ca. 45 - 60 Min)\n\nkleine Gruppenarbeiten zur Vertiefung\nvariiert abh√§ngig vom Thema der jeweiligen Sitzung\nBeispiele:\n\nAnwendung von Tool/Methode mit anschlie√üender kritschen Diskussion\nErstellung eines einfachen Forschungs- oder Analysedesign"
  },
  {
    "objectID": "slides/slides-01.html#typische-session-1",
    "href": "slides/slides-01.html#typische-session-1",
    "title": "Kick-Off",
    "section": "Typische Session",
    "text": "Typische Session\nf√ºr üìä: Pitch ‚ûû Diskussion ‚ûû Repeat\n\n\nProject topic idea(s) (ca 5-10 Min)\n\nkurzer √úberblick √ºber Thema, Forschungsfrage oder Motivation & ausgew√§hlte Daten(teil)stichprobe (2 Folien),\nkurze Beschreibung von Methode und (geplanter) Analyse (1 Folie)\nErgebnisse und/oder eine Herausforderung aufzeigen, die im Kurs diskutiert werden soll (2 Folien).\n\n\nFragen & Diskussion (ca 5-10 Min)\n\nZeit f√ºr Fragen, entweder von der Gruppe an den Kurs oder umgekehrt.\n\n\n\n\nüîÅ f√ºr jede Gruppe\n\n\n\nStatus bzw. erster Ergebnisse der Project Proposal & Short Repots (max. 5 Folien)"
  },
  {
    "objectID": "slides/slides-01.html#vorl√§ufiger-seminarplan",
    "href": "slides/slides-01.html#vorl√§ufiger-seminarplan",
    "title": "Kick-Off",
    "section": "(Vorl√§ufiger) Seminarplan",
    "text": "(Vorl√§ufiger) Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nDBD: Overview\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüìÇ Project 1\n\nAnalysis of media content\n\n\n    4\n\n22.11.2023\n\nüìö Digital disconnection\n\n\n    5\n\n29.11.2023\n\nüì¶ Data collection methods\n\n\n    6\n\n06.12.2023\n\nüî® Text as data\n\nChristoph Adrian\n\n    7\n\n13.12.2023\n\nüìä Presentation & Discussion\n\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project 2\n\nAnalysis of media usage\n\n\n    9\n\n10.01.2024\n\nüìö Media habits & routines\n\n\n    10\n\n17.01.2024\n\nüì¶ Data donation methods\n\n\n    11\n\n24.01.2024\n\nüî® Working data logs\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-01.html#talking-about-disconnecting",
    "href": "slides/slides-01.html#talking-about-disconnecting",
    "title": "Kick-Off",
    "section": "Talking about disconnecting",
    "text": "Talking about disconnecting\nProjekt 1: #digitaldetox auf sozialen Medien\n\n\n\n\n\n\nQuelle: dup-magazin.de\n\n\n\n\nProjektaufbau\n\nTheoretische Hintergrund: Digital disconnection\nAnalyse von Social Media Post mit #digitaldetox\nArchiv-Daten (Twitter) oder eigene Datenerhebung\nFokus auf Inhalt (Diskurs, andere Hashtags) oder ‚ÄúAkteure‚Äù/Accounts"
  },
  {
    "objectID": "slides/slides-01.html#studying-problems-not-problematic-usage",
    "href": "slides/slides-01.html#studying-problems-not-problematic-usage",
    "title": "Kick-Off",
    "section": "Studying problems, not problematic usage?",
    "text": "Studying problems, not problematic usage?\nProjekt 2: Mediennutzungsgewohnheiten und Wohlbefinden\n\n\n\n\n\n\n\n\n\n\nProjektaufbau\n\nHabitualisierte / routinem√§√üige Mediennutzung\nErhebung, Aufbereitung und Analyse von Logging-Daten\nEigene Erhebung, Data Download Packages, API-Zugang\nFokus auf Datenerhebungs & -aufbereitungsprozess"
  },
  {
    "objectID": "slides/slides-01.html#please-state-your-preference",
    "href": "slides/slides-01.html#please-state-your-preference",
    "title": "Kick-Off",
    "section": "Please state your preference",
    "text": "Please state your preference\nVergabe der Pr√§sentationsthemen\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link und geben Sie Ihre Themenpr√§ferenz an:\n\nhttps://simpleassign.com/poll/-NhW_EY5JXFZ25ZcYZbx"
  },
  {
    "objectID": "slides/slides-01.html#lets-spin-the-wheel",
    "href": "slides/slides-01.html#lets-spin-the-wheel",
    "title": "Kick-Off",
    "section": "Let‚Äôs spin the wheel?!",
    "text": "Let‚Äôs spin the wheel?!\nZuteilung der Pr√§sentationsthemen\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n    \n  \n  \n    4\n\n22.11.2023\n\nüìö Digital disconnection\n\n    5\n\n29.11.2023\n\nüì¶ Data collection methods\n\n    9\n\n10.01.2024\n\nüìö Media habits & routines\n\n    10\n\n17.01.2024\n\nüì¶ Data donation methods\n\n  \n  \n  \n\n\n\n\n\nIn case of emergency: Wheel of Names"
  },
  {
    "objectID": "slides/slides-01.html#before-we-meet-again",
    "href": "slides/slides-01.html#before-we-meet-again",
    "title": "Kick-Off",
    "section": "Before we meet again",
    "text": "Before we meet again\nHinweise und offene Fragen\nHinweise:\n\nLernen Sie die Kursseite & Zulip kennen! Und checken Sie die Infos () zur n√§chten Sitzung.\nBis zum 13.11.: Basiskurs R/RStudio durcharbeiten. Bitte senden Sie das Kurszertifikat an christoph.adrian@fau.de\n\nFragen:\n\nWhy no English? ü§∑\nHaben Sie noch Fragen?"
  },
  {
    "objectID": "slides/slides-01.html#literatur",
    "href": "slides/slides-01.html#literatur",
    "title": "Kick-Off",
    "section": "Literatur",
    "text": "Literatur\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-10.html#schedule",
    "href": "slides/slides-10.html#schedule",
    "title": "üî® Automatic text analysis in R",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis üé•\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-10.html#social-media-as-or",
    "href": "slides/slides-10.html#social-media-as-or",
    "title": "üî® Automatic text analysis in R",
    "section": "Social Media as üíä, üëπ or üç© ?",
    "text": "Social Media as üíä, üëπ or üç© ?\nDiscussion about digital disconnection on twitter\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nCollect all tweets (until 31.12.2022) via Twitter Academic Research Product Track v2 API & academictwitteR package (Barrie & Ho, 2021) that mention or discuss digital detox (and similar terms)\nDataset for session is a subsample (n = 46670) with only tweets that contain #digitaldetox."
  },
  {
    "objectID": "slides/slides-10.html#the-tidy-text-format-pipeline-basics",
    "href": "slides/slides-10.html#the-tidy-text-format-pipeline-basics",
    "title": "üî® Automatic text analysis in R",
    "section": "The tidy text format pipeline basics",
    "text": "The tidy text format pipeline basics\nFocus on single words and their relationship documents & sentiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#expansion-of-the-pipeline",
    "href": "slides/slides-10.html#expansion-of-the-pipeline",
    "title": "üî® Automatic text analysis in R",
    "section": "Expansion of the pipeline",
    "text": "Expansion of the pipeline\nFocus on modeling the realtionships between words & documents\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#quick-recap-on-document-term-matrix-dtm",
    "href": "slides/slides-10.html#quick-recap-on-document-term-matrix-dtm",
    "title": "üî® Automatic text analysis in R",
    "section": "Quick recap on Document-Term Matrix [DTM]",
    "text": "Quick recap on Document-Term Matrix [DTM]\nMost common structure for (classic) text mining\n\n\nA matrix where:\n\neach row represents one document (such as a tweet),\neach column represents one term, and\neach value (typically) contains the number of appearances of that term in that document.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation",
    "href": "slides/slides-10.html#step-by-step-dtm-creation",
    "title": "üî® Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n  select(tweet_id, user_name, text) %&gt;% \n  print(n = 15)\n\n\n# A tibble: 639,459 √ó 3\n   tweet_id   user_name      text        \n   &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;       \n 1 5777201122 Pete Blackshaw blackberry  \n 2 5777201122 Pete Blackshaw iphone      \n 3 5777201122 Pete Blackshaw read        \n 4 5777201122 Pete Blackshaw pew         \n 5 5777201122 Pete Blackshaw report      \n 6 5777201122 Pete Blackshaw teens       \n 7 5777201122 Pete Blackshaw distracted  \n 8 5777201122 Pete Blackshaw driving     \n 9 5777201122 Pete Blackshaw http        \n10 5777201122 Pete Blackshaw bit.ly      \n11 5777201122 Pete Blackshaw 4abr5p      \n12 5777201122 Pete Blackshaw digitaldetox\n13 4814687834 Andrew Gerrard dawn_wylie  \n14 4814687834 Andrew Gerrard prompted    \n15 4814687834 Andrew Gerrard question    \n# ‚Ñπ 639,444 more rows"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation-1",
    "href": "slides/slides-10.html#step-by-step-dtm-creation-1",
    "title": "üî® Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize ‚ñ∂Ô∏è Summarize\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Preview \ntweets_summarized %&gt;% \n  print(n = 15)\n\n\n# A tibble: 592,499 √ó 3\n   tweet_id            text              n\n   &lt;chr&gt;               &lt;chr&gt;         &lt;int&gt;\n 1 1000009901563838465 bite              1\n 2 1000009901563838465 detox             1\n 3 1000009901563838465 digital           1\n 4 1000009901563838465 digitaldetox      1\n 5 1000009901563838465 enjoy             2\n 6 1000009901563838465 fly               1\n 7 1000009901563838465 happitizer        1\n 8 1000009901563838465 happitizers       1\n 9 1000009901563838465 https             1\n10 1000009901563838465 inspiration       1\n11 1000009901563838465 mindgourmet       1\n12 1000009901563838465 mindgourmet‚Äôs     1\n13 1000009901563838465 sized             1\n14 1000009901563838465 t.co              1\n15 1000009901563838465 taste             1\n# ‚Ñπ 592,484 more rows"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation-2",
    "href": "slides/slides-10.html#step-by-step-dtm-creation-2",
    "title": "üî® Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize ‚ñ∂Ô∏è Summarize ‚ñ∂Ô∏è DTM\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dtm &lt;- tweets_summarized %&gt;% \n  cast_dtm(tweet_id, text, n)\n\n# Preview\ntweets_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 46670, terms: 87172)&gt;&gt;\nNon-/sparse entries: 592499/4067724741\nSparsity           : 100%\nMaximal term length: 49\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/slides-10.html#choose-or-combine-styles",
    "href": "slides/slides-10.html#choose-or-combine-styles",
    "title": "üî® Automatic text analysis in R",
    "section": "Choose or combine styles",
    "text": "Choose or combine styles\nSimple with tidytext, precise with quanteda\n\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dtm &lt;- tweets_summarized %&gt;% \n  cast_dtm(tweet_id, text, n)\n\n# Preview\ntweets_dtm\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags",
    "href": "slides/slides-10.html#an-example-network-of-hashtags",
    "title": "üî® Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n# Extract hashtags\ntweets_hashtags &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(\n    text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) \n\n# Extract most common hashtags\ntop50_hashtags_tidy &lt;- tweets_hashtags %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags-1",
    "href": "slides/slides-10.html#an-example-network-of-hashtags-1",
    "title": "üî® Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n# Extract DFM with only hashtags\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Extract most common hashtags \ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of hashtags\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags-2",
    "href": "slides/slides-10.html#an-example-network-of-hashtags-2",
    "title": "üî® Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n\nExpand for full code\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\nExpand for full code\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#a-new-input-in-the-pipeline",
    "href": "slides/slides-10.html#a-new-input-in-the-pipeline",
    "title": "üî® Automatic text analysis in R",
    "section": "A new input in the pipeline",
    "text": "A new input in the pipeline\nUnsupervised learning example: Topic modeling\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#building-a-shared-vocabulary-again",
    "href": "slides/slides-10.html#building-a-shared-vocabulary-again",
    "title": "üî® Automatic text analysis in R",
    "section": "Building a shared vocabulary ‚Ä¶ again",
    "text": "Building a shared vocabulary ‚Ä¶ again\nImportant terms and definitions\n\nTopic Modeling: Form of unsupervised machine learning method used to exploratively identify topics in a corpus. Often, these are so-called mixed-membership models.\nK: Number of topics to be calculated for a given a topic model.\nWord-Topic-Matrix: Matrix describing the conditional probability (beta) with which a feature is prevalent in a given topic.\nDocument-Topic-Matrix: Matrix describing the conditional probability (gamma) with which a topic is prevalent in a given document."
  },
  {
    "objectID": "slides/slides-10.html#beyond-lda",
    "href": "slides/slides-10.html#beyond-lda",
    "title": "üî® Automatic text analysis in R",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nDifferent topic modeling approaches\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) is a probabilistic generative model that assumes each document in a corpus is a mix of topics and each word in the document is attributable to one of the document‚Äôs topics.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) extends LDA by incorporating document-level covariates, allowing for the modeling of how external factors influence topic prevalence.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) represent words as continuous vectors in a high-dimensional space, capturing semantic relationships between words based on their context in the data.\nTopic Modeling with Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) leverages deep learning architectures to automatically learn latent topics from textual data.\n\n\n\nspecify the presumed number of topics K thatyou expect to find in a corpus (e.g., K = 5, i.e., 5 topics)\nthe model then tries to inductively identify 5 topics in the corpus based on the distribution of frequently co-occurring features.\nan algorithm is used for this purpose, which is why topic modeling is a type of ‚Äúmachine learning‚Äù."
  },
  {
    "objectID": "slides/slides-10.html#preparation-is-everything",
    "href": "slides/slides-10.html#preparation-is-everything",
    "title": "üî® Automatic text analysis in R",
    "section": "Preparation is everything",
    "text": "Preparation is everything\nSuggested pre-processing steps (based on Maier et al. (2018))\n\n\n\n‚ö†Ô∏è Deduplication;\n‚úÖ tokenization;\n‚úÖ transforming all characters to lowercase;\n‚úÖ removing punctuation and special characters;\n‚úÖ Removing stop-words;\n‚ö†Ô∏è term unification (lemmatizing or stemming);\nüèóÔ∏è relative pruning (attributed to Zipf‚Äôs law);\n\n\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(tweets_detox),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\n\nZipf‚Äôs law states that the frequency that a word appears is inversely proportional to its rank."
  },
  {
    "objectID": "slides/slides-10.html#how-to-find-k",
    "href": "slides/slides-10.html#how-to-find-k",
    "title": "üî® Automatic text analysis in R",
    "section": "How to find K",
    "text": "How to find K\nThe most important question of model selection\n\nThe choice of K (whether the model is instructed to identify 5, 15, or 100 topics), has a substantial impact on results:\n\nThe smaller K, the more fine-grained and usually the more exclusive topics;\nthe larger K, the more clearly topics identify individual events or issues.\n\nThe stm package (Roberts et al., 2019) has two build in solution to find the optimal K\n\nsearchK() function\nsetting K = 0 when estimating the model\n\nRecommendation for stm: (Manual) training and evaluation!\n\n\nHowever, with a larger K topics are oftentimes less exclusive, meaning that they somehow overlap."
  },
  {
    "objectID": "slides/slides-10.html#train-and-evaluate-topic-models",
    "href": "slides/slides-10.html#train-and-evaluate-topic-models",
    "title": "üî® Automatic text analysis in R",
    "section": "Train and evaluate topic models",
    "text": "Train and evaluate topic models\nBetter than searchK(): Manual exploration\n\n\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) \n\n# Estimate multiple models\nstm_exploration &lt;- tibble(\n  k = seq(from = 5, to = 85, by = 5)\n  ) %&gt;%\n  mutate(\n    mdl = furrr::future_map(\n      k, \n      ~stm::stm(\n        documents = quanteda_stm$documents,\n        vocab = quanteda_stm$vocab, \n        K = ., \n        seed = 42,\n        max.em.its = 1000,\n        init.type = \"Spectral\",\n        verbose = FALSE),\n    .options = furrr_options(seed = 42))\n  )\n\n\n\nstm_exploration$mdl\n\n[[1]]\nA topic model with 5 topics, 46574 documents and a 8268 word dictionary.\n\n[[2]]\nA topic model with 10 topics, 46574 documents and a 8268 word dictionary.\n\n[[3]]\nA topic model with 15 topics, 46574 documents and a 8268 word dictionary.\n\n[[4]]\nA topic model with 20 topics, 46574 documents and a 8268 word dictionary.\n\n[[5]]\nA topic model with 25 topics, 46574 documents and a 8268 word dictionary.\n\n[[6]]\nA topic model with 30 topics, 46574 documents and a 8268 word dictionary.\n\n[[7]]\nA topic model with 35 topics, 46574 documents and a 8268 word dictionary.\n\n[[8]]\nA topic model with 40 topics, 46574 documents and a 8268 word dictionary.\n\n[[9]]\nA topic model with 45 topics, 46574 documents and a 8268 word dictionary.\n\n[[10]]\nA topic model with 50 topics, 46574 documents and a 8268 word dictionary.\n\n[[11]]\nA topic model with 55 topics, 46574 documents and a 8268 word dictionary.\n\n[[12]]\nA topic model with 60 topics, 46574 documents and a 8268 word dictionary.\n\n[[13]]\nA topic model with 65 topics, 46574 documents and a 8268 word dictionary.\n\n[[14]]\nA topic model with 70 topics, 46574 documents and a 8268 word dictionary.\n\n[[15]]\nA topic model with 75 topics, 46574 documents and a 8268 word dictionary.\n\n[[16]]\nA topic model with 80 topics, 46574 documents and a 8268 word dictionary.\n\n[[17]]\nA topic model with 85 topics, 46574 documents and a 8268 word dictionary."
  },
  {
    "objectID": "slides/slides-10.html#semantic-coherence-as-the-key",
    "href": "slides/slides-10.html#semantic-coherence-as-the-key",
    "title": "üî® Automatic text analysis in R",
    "section": "Semantic coherence as the key",
    "text": "Semantic coherence as the key\nDifferent model statistics for evaluation\n\n\nExpand for full code\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n         y = NULL,\n         title = \"Model diagnostics by number of topics\"\n    ) +\n    theme_pubr() +\n    # add highlights \n    geom_vline(aes(xintercept =  5), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 10), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#C77CFF\", alpha = .5) \n\n\n\n\nSemantic Coherence: tells you how coherent topics are, i.e., how often features describing a topic co-occur and topics thus appear to be internally coherent.\nExclusivity: tells you how exclusive topics are, i.e., how much they differ from each other and topics thus appear to describe different things."
  },
  {
    "objectID": "slides/slides-10.html#finding-the-best-trade-off",
    "href": "slides/slides-10.html#finding-the-best-trade-off",
    "title": "üî® Automatic text analysis in R",
    "section": "Finding the best trade-off",
    "text": "Finding the best trade-off\nComparison of selected models based on exclusivty and semantic coherence\n\n\nExpand for full code\n# Models for comparison\nmodels_for_comparison = c(5, 10, 40)\n\n# Create figures\nstm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-10.html#a-first-overview",
    "href": "slides/slides-10.html#a-first-overview",
    "title": "üî® Automatic text analysis in R",
    "section": "A first overview",
    "text": "A first overview\nUnderstanding the ‚Äòfinal‚Äô model (k = 10)\n\ntpm %&gt;% plot(type = \"summary\")"
  },
  {
    "objectID": "slides/slides-10.html#a-more-detailed-overview",
    "href": "slides/slides-10.html#a-more-detailed-overview",
    "title": "üî® Automatic text analysis in R",
    "section": "A more detailed overview",
    "text": "A more detailed overview\nUnderstanding the ‚Äòfinal‚Äô model (k = 10)\n\n\nExpand for full code\n# Create data\ntop_gamma &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- tpm %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n  \n    \n    \n      Topic\n      Top Terms (based on beta)\n      Gamma\n    \n  \n  \n    10\nphone, via, much, #screentime, know, check, people, phones, #parenting, online\n0.122\n    9\nhelp, read, devices, use, world, tech, family, taking, kids, sleep\n0.114\n    3\nneed, great, week, going, listen, #unplugging, discuss, @icphenomenallyu, away, well\n0.111\n    2\n#unplug, just, weekend, #travel, unplug, enjoy, #nature, really, join, nature\n0.103\n    8\nnew, technology, smartphone, retreat, work, addiction, year, health, internet, without\n0.099\n    7\ncan, #mindfulness, feel, #switchoff, #digitalwellbeing, #wellness, things, #phonefree, #disconnecttoreconnect, #digitalminimalism\n0.098\n    4\namp, day, take, #mentalhealth, go, try, #wellbeing, now, give, every\n0.092\n    1\nsocial, media, get, life, back, like, good, #socialmedia, find, see\n0.091\n    5\nus, today, days, next, put, happy, may, facebook, share, hour\n0.089\n    6\none, break, tips, make, screen, love, #technology, free, looking, getting\n0.080"
  },
  {
    "objectID": "slides/slides-10.html#results-in-a-different-context",
    "href": "slides/slides-10.html#results-in-a-different-context",
    "title": "üî® Automatic text analysis in R",
    "section": "Results in a different context",
    "text": "Results in a different context\nMerge back with original data for further analysis and comparison\n\n\nExpand for full code\ntop_gamma %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"\", \n    y = \"Number of tweets\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-10.html#most-representative-tweets-for-topic-10",
    "href": "slides/slides-10.html#most-representative-tweets-for-topic-10",
    "title": "üî® Automatic text analysis in R",
    "section": "Most representative tweets for Topic 10",
    "text": "Most representative tweets for Topic 10\nCheck interpretability and relevance of topics\n\n\nExpand for full code\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n  \n    \n    \n      tweet_id\n      user_username\n      created_at\n      text\n      top_gamma\n    \n  \n  \n    1496707135794827266\nbeckygrantstr\n2022-02-24 04:42:43\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/EcV7yKn4WX\n0.8192142\n    1496343978672898048\nbeckygrantstr\n2022-02-23 04:39:39\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/o9e6WQIpm5\n0.8192142\n    1495981434665947137\nbeckygrantstr\n2022-02-22 04:39:02\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/rQgcPltrMt\n0.8192142\n    1499612427503247360\nbeckygrantstr\n2022-03-04 05:07:18\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/DQRQlN1iyq\n0.8192142\n    1499249315381977089\nbeckygrantstr\n2022-03-03 05:04:26\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/L7ly66J2fq\n0.8192142\n    1498886117394980865\nbeckygrantstr\n2022-03-02 05:01:13\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/uJPIMYY1Id\n0.8192142\n    1498522796611321864\nbeckygrantstr\n2022-03-01 04:57:30\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/z4Fv2bmeag\n0.8192142\n    1498159674008510465\nbeckygrantstr\n2022-02-28 04:54:35\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/SdZFAJy6kZ\n0.8192142\n    1497796525929472003\nbeckygrantstr\n2022-02-27 04:51:34\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/OKLMzwfQUA\n0.8192142\n    1497433419512524802\nbeckygrantstr\n2022-02-26 04:48:42\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/yTnqhTPV8h\n0.8192142"
  },
  {
    "objectID": "slides/slides-10.html#users-with-most-tweets-about-topic-10",
    "href": "slides/slides-10.html#users-with-most-tweets-about-topic-10",
    "title": "üî® Automatic text analysis in R",
    "section": "Users with most tweets about Topic 10",
    "text": "Users with most tweets about Topic 10\nCheck interpretability and relevance of topics\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n  \n    \n    \n      user_username\n      n\n      prop\n    \n  \n  \n    TimeToLogOff\n2384\n28.83\n    punkt\n390\n4.72\n    petitstvincent\n175\n2.12\n    tanyagoodin\n135\n1.63\n    ditox_unplug\n111\n1.34\n    OurHourOff\n99\n1.20\n    beckygrantstr\n63\n0.76\n    CreeEscape\n56\n0.68\n    ConsciDigital\n51\n0.62\n    phubboo\n50\n0.60"
  },
  {
    "objectID": "slides/slides-10.html#validate-validate-validate",
    "href": "slides/slides-10.html#validate-validate-validate",
    "title": "üî® Automatic text analysis in R",
    "section": "Validate, validate, validate!",
    "text": "Validate, validate, validate!\nThings to remember about topic models\n\ntopic models are a useful tool for automated content analysis, both when exploring a large amount of data and when it comes to systematically identifying relationships between topics and other variables\ncertain prerequisites such as minimum size and variety of the corpus (namely on the level of words and documents and their relation to each other) need to be met for a conclusive model\neverything above a certain degree of word frequencies is considered a ‚Äútopic,‚Äù even if it is not a topic in human interpretation\nReading the tea leaves (Chang et al., 2009) or (again): validate, validate, validate (e.g.¬†with oolong package (Chan & S√§ltzer, 2020))"
  },
  {
    "objectID": "slides/slides-10.html#and-now-you-model-away",
    "href": "slides/slides-10.html#and-now-you-model-away",
    "title": "üî® Automatic text analysis in R",
    "section": "üß™ And now ‚Ä¶ you: Model away!",
    "text": "üß™ And now ‚Ä¶ you: Model away!\n\n\n\n\n\n\nObjective of this exercise\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\nNext steps\n\nDownload files provided on StudOn or shared drives for the sessions\nUnzip the archive at a destination of your choice.\nDouble click on the Exercise-Automatic_text_analysis.Rproj to open the RStudio project. This ensures that all dependencies are working correctly.\nOpen the exercise.qmd file and follow the instructions.\nTip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs)."
  },
  {
    "objectID": "slides/slides-10.html#references",
    "href": "slides/slides-10.html#references",
    "title": "üî® Automatic text analysis in R",
    "section": "References",
    "text": "References\n\n\nBarrie, C., & Ho, J. (2021). academictwitteR: An r package to access the twitter academic research product track v2 API endpoint. Journal of Open Source Software, 6(62), 3272. https://doi.org/10.21105/joss.03272\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nChan, C., & S√§ltzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288‚Äì296.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988‚Äì1003. https://doi.org/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O‚ÄôReilly.\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-06.html#seminarplan",
    "href": "slides/slides-06.html#seminarplan",
    "title": "üìö Digital disconnection",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-06.html#kurzes-organisatorisches-update",
    "href": "slides/slides-06.html#kurzes-organisatorisches-update",
    "title": "üìö Digital disconnection",
    "section": "Kurzes organisatorisches Update",
    "text": "Kurzes organisatorisches Update\nInformationen zur n√§chsten Session\n\nBesondere Vorbereitung f√ºr die Session in der n√§chsten Woche:\n\nInstallation Zeeschuimer-Plugin (mind. 1 Person der Grupppe)\n4CAT-Logindaten via Zulip - Bitte testen!\nInformationen auf der Infopage zur n√§chsten Session"
  },
  {
    "objectID": "slides/slides-06.html#or",
    "href": "slides/slides-06.html#or",
    "title": "üìö Digital disconnection",
    "section": "üíä, üëπ or üç© ?",
    "text": "üíä, üëπ or üç© ?\nTheoretische Betrachtung von Digital Detox nach Vanden Abeele et al. (2022)\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      \n      \n        Social Media as a ‚Ä¶\n      \n    \n    \n      Drug\n      Demon\n      Donut\n    \n  \n  \n    What is at stake?\n\nAddiction/health\n\nDistraction\n\nWell-being\n\n    Root cause of problem\n\nIndividual susceptibility\n\nAddictive design\n\nInadequate fit\n\n    User agency\n\nAgency is limited due to innate susceptibilities\n\nAgency needs to be reclaimed from social media platforms\n\nUser has agency, but it is challenged by person-, technology- and context-specific elements\n\n    Focus of disconnection\n\nComplete abstinence, re-training of the ‚Äòfaulty brain‚Äô to break the dopamine link\n\nRemoving/weakening the distracting potential of tech, using persuasive design to support exerting social media self-control\n\nDisconnection interventions tailored to persons and/or contexts to ‚Äòoptimize the balance‚Äô between benefits and drawbacks of connectivity, mindful use\n\n    Digital disconnection examples\n\nDigital detox, cognitive behavioral therapy\n\nMuting phone, disabling notifications, putting phone in grey-scale, using apps that reward abstinence (e.g., Forest)\n\nLocative disconnection, disconnection apps that extensive tailoring to persons and contexts, mindfulness training"
  },
  {
    "objectID": "slides/slides-06.html#design-your-own-research-design",
    "href": "slides/slides-06.html#design-your-own-research-design",
    "title": "üìö Digital disconnection",
    "section": "üß™ Design your own research (design)",
    "text": "üß™ Design your own research (design)\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nIntegrieren Sie die Untersuchung von (der Absicht zu) ‚Äúdigital detox‚Äù in Ihr Forschungsdesign(konzept) der letzten Sitzung, sowohl via Abfrage als auch √ºber DBD.\n\n\n\nN√§chste Schritte\n\n\nFinden Sie sich in Ihren Gruppen zusammen\n√úberlegen Sie sich, wie Sie die Absicht oder bestehende Strategien zu ‚Äúdigital detox‚Äù in Ihr Untersuchungsdesign der letzter Woche integrieren k√∂nnen. Machen Sie mindestens einen Vorschlag f√ºr eine Integration via Befragung und einen via digitale Verhaltensdaten.\nHalten Sie Ihre Fragen & Desginideen auf der f√ºr Ihre Gruppe vorgesehenen Folienvorlage (n√§chste Slide) fest.\n\n\n\n\n\n‚àí+\n25:00"
  },
  {
    "objectID": "slides/slides-06.html#let-the-work-beginn",
    "href": "slides/slides-06.html#let-the-work-beginn",
    "title": "üìö Digital disconnection",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Pr√§sentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-06.html#literatur",
    "href": "slides/slides-06.html#literatur",
    "title": "üìö Digital disconnection",
    "section": "Literatur",
    "text": "Literatur\n\n\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007\n\n\n\n\n\nHome"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 13:15 - 14:15\nFG 2.031"
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 13:15 - 14:15\nFG 2.031"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Behavioral Data",
    "section": "",
    "text": "Note\n\n\n\nThis page contains an outline of the topics, contents, and assignments for the semester. Please note that the contents of the course will be updated as the semester progresses, with all changes documented here."
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Digital Behavioral Data",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License."
  },
  {
    "objectID": "slides/slides-07.html#seminarplan",
    "href": "slides/slides-07.html#seminarplan",
    "title": "üì¶ Data collection methods",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-07.html#what-is-it-who-made-it",
    "href": "slides/slides-07.html#what-is-it-who-made-it",
    "title": "üì¶ Data collection methods",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen 4CAT (Peeters & Hagen, 2022)\n\n\n\nTool zur Analyse und Verarbeitung von Daten aus sozialen Online-Plattformen\nZiel ist es, die Erfassung und Analyse von Daten aus diesen Plattformen √ºber eine Webschnittstelle zug√§nglich zu machen, ohne dass Programmier- oder Web-Scraping-Kenntnisse erforderlich sind."
  },
  {
    "objectID": "slides/slides-07.html#what-is-it-who-made-it-1",
    "href": "slides/slides-07.html#what-is-it-who-made-it-1",
    "title": "üì¶ Data collection methods",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen Zeeschuimer (Peeters, 2022)\n\n\n\nBrowsererweiterung, die w√§hrend des Besuchs einer Social-Media-Website Daten √ºber die Elemente sammelt, die in der Weboberfl√§che einer Plattform zu sehen sind\nDerzeit werden die folgenden Plattformen unterst√ºtzt:\n\n √ºber https://www.tiktok.com\n √ºber https://www.instagram.com\n\nErg√§nzung zu 4CAT (Peeters, 2022)\n\n\n\n\n\n\n\n\n\n\nDie Zielgruppe sind Forscher, die systematisch Inhalte auf Social-Media-Plattformen untersuchen wollen, die sich dem herk√∂mmlichen Scraping oder der API-basierten Datenerfassung widersetzen.\n\nSie k√∂nnen z. B. TikTok durchsuchen und sp√§ter eine Liste aller Beitr√§ge in der Reihenfolge exportieren, in der Sie sie gesehen haben. Die Daten k√∂nnen als JSON-Datei exportiert oder zur Analyse und Speicherung in eine 4CAT-Instanz exportiert werden. Zeeschuimer ist in erster Linie als Erg√§nzung zu 4CAT gedacht, aber Sie k√∂nnen seine Ausgabe auch in Ihre eigene Analysepipeline integrieren.\nDie Plattformunterst√ºtzung erfordert regelm√§√üige Wartung, um mit den √Ñnderungen auf den Plattformen Schritt zu halten. Wenn etwas nicht funktioniert, freuen wir uns √ºber Probleme und Pull Request\nDie Erweiterung st√∂rt Sie nicht beim normalen Surfen und l√§dt niemals automatisch Daten hoch, sondern nur, wenn Sie sie ausdr√ºcklich dazu auffordern."
  },
  {
    "objectID": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram",
    "href": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram",
    "title": "üì¶ Data collection methods",
    "section": "@tagesschau auf ",
    "text": "@tagesschau auf \nDatenerhebung mit Zeeschuimer"
  },
  {
    "objectID": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram-1",
    "href": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram-1",
    "title": "üì¶ Data collection methods",
    "section": "@tagesschau auf ",
    "text": "@tagesschau auf \nDaten & Analyse in üêàüêà 4CAT üêàüêà"
  },
  {
    "objectID": "slides/slides-07.html#posts-im-zeitverlauf",
    "href": "slides/slides-07.html#posts-im-zeitverlauf",
    "title": "üì¶ Data collection methods",
    "section": "Posts im Zeitverlauf",
    "text": "Posts im Zeitverlauf\nüêàüêà 4CAT üêàüêà: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#analyse-der-verwendeten-hashtags",
    "href": "slides/slides-07.html#analyse-der-verwendeten-hashtags",
    "title": "üì¶ Data collection methods",
    "section": "Analyse der verwendeten Hashtags",
    "text": "Analyse der verwendeten Hashtags\nüêàüêà 4CAT üêàüêà: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#wordcloud-der-top-hashtags",
    "href": "slides/slides-07.html#wordcloud-der-top-hashtags",
    "title": "üì¶ Data collection methods",
    "section": "Wordcloud der Top-Hashtags",
    "text": "Wordcloud der Top-Hashtags\nüêàüêà 4CAT üêàüêà: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#and-now-you-scraping-with-zeeschuimer-4cat",
    "href": "slides/slides-07.html#and-now-you-scraping-with-zeeschuimer-4cat",
    "title": "üì¶ Data collection methods",
    "section": "üß™ And now ‚Ä¶ you: Scraping with Zeeschuimer & 4CAT!",
    "text": "üß™ And now ‚Ä¶ you: Scraping with Zeeschuimer & 4CAT!\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nScrapen Sie mit dem Zeeschuimer-Plugin Posts zu einem Thema und auf einer Social-Media-Webseite Ihrer Wahl\n\n\n\nN√§chste Schritte\n\n\nNutzen Sie das Zeeschuimer-Plugin um Post zu einem Thema und von einer der verf√ºgbaren Social-Media-Plattform Ihrer Wahl zu sammeln.\nBitte beachten: Damit Sie die mit Zeeschuimer gesammelten Daten an den üêàüêà 4CAT üêàüêà Server schicken k√∂nnen, m√ºssen Sie\n\nsich vorher im selben Browser auf dem üêàüêà 4CAT üêàüêà Server einloggen\nIm Feld 4CAT server URL folgendes eintragen: http://10.204.20.178:80\n\nAnalysieren Sie die Daten mit Hilfe von üêàüêà 4CAT üêàüêà.\n\nErstellen Sie eine √úbersicht √ºber den Posts im Zeitverlauf\nFinden Sie die Top Hashtags.\nDokumentieren Sie die Ergebnisse f√ºr Ihre Gruppe in der daf√ºr vorgesehenen Folienvorlage (n√§chste Slide).\n\n\n\n\n\n\n‚àí+\n25:00"
  },
  {
    "objectID": "slides/slides-07.html#let-the-work-beginn",
    "href": "slides/slides-07.html#let-the-work-beginn",
    "title": "üì¶ Data collection methods",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Pr√§sentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-07.html#literatur",
    "href": "slides/slides-07.html#literatur",
    "title": "üì¶ Data collection methods",
    "section": "Literatur",
    "text": "Literatur\n\n\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571‚Äì589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-04.html#seminarplan",
    "href": "slides/slides-04.html#seminarplan",
    "title": "üìö Media routines & habits",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-04.html#kurzes-organisatorische-update",
    "href": "slides/slides-04.html#kurzes-organisatorische-update",
    "title": "üìö Media routines & habits",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Pr√ºfungsleistungen\n\n\nStudOn oder Zulip: Wo sollen Ihre Pr√§sentationen hochgeladen werden?\n\n\n\n\nKooperation mit Kurs (bzw. der √úbung) Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing\n\nHintergrund: Thematische √úberschneidungen, deswegen B√ºndelung der Kompetenzen\nüî® Text as data in R & üî® Topic Modeling in R werden auch f√ºr Studierende der √úbung des Kurses angeboten\nKonsequenz: Sitzungen finden im PC-Pool in der Langen Gasse statt\nFrage: English ok?"
  },
  {
    "objectID": "slides/slides-04.html#mehr-als-nur-stimulus-response",
    "href": "slides/slides-04.html#mehr-als-nur-stimulus-response",
    "title": "üìö Media routines & habits",
    "section": "Mehr als nur Stimulus & Response",
    "text": "Mehr als nur Stimulus & Response\nDie Rolle der Medien\n\n\n\n\n\n\nMedia effects\n\n\nthe deliberate and nondeliberate short and long term within person changes in cognitions (including beliefs), emotions, attitudes, and behavior that result from media use\n(Valkenburg et al., 2016)\n\n\n\n\nDifferential Susceptibility to Media Effects Model (Valkenburg & Peter, 2013)"
  },
  {
    "objectID": "slides/slides-04.html#aber-was-ist-eigentlich-mediennutzung",
    "href": "slides/slides-04.html#aber-was-ist-eigentlich-mediennutzung",
    "title": "üìö Media routines & habits",
    "section": "Aber was ist eigentlich Mediennutzung?",
    "text": "Aber was ist eigentlich Mediennutzung?\nKombination aus kanal- und kommunikationsorientierter Ansatz\n\nThe hierarchical CMC taxonomy (Meier & Reinecke, 2021)\nAusganngspunkt: - Mediennutzung hat eine Wirkung bzw. Effekte (z.B. auf Emotionen, Einstellung und Verhalten)"
  },
  {
    "objectID": "slides/slides-04.html#viele-messwiederholung-in-kurzen-abst√§nden",
    "href": "slides/slides-04.html#viele-messwiederholung-in-kurzen-abst√§nden",
    "title": "üìö Media routines & habits",
    "section": "(Viele) Messwiederholung in (kurzen) Abst√§nden",
    "text": "(Viele) Messwiederholung in (kurzen) Abst√§nden\nIntensive Longitudinal Designs (IDL) im Fokus\n\n\n\n‚Äúan intensive longitudinal design involves sequential measurements on five or more occasions during which a change process is expected to unfold within each subject (e.g., person or other sampling)‚Äù\n(Bolger & Laurenceau, 2013)"
  },
  {
    "objectID": "slides/slides-04.html#layers-and-layers",
    "href": "slides/slides-04.html#layers-and-layers",
    "title": "üìö Media routines & habits",
    "section": "Layers and layers",
    "text": "Layers and layers\nIDL im Fokus: Zusammenhang der Ebenen"
  },
  {
    "objectID": "slides/slides-04.html#verschiedene-varianten-des-situationssamplings",
    "href": "slides/slides-04.html#verschiedene-varianten-des-situationssamplings",
    "title": "üìö Media routines & habits",
    "section": "Verschiedene Varianten des Situationssamplings",
    "text": "Verschiedene Varianten des Situationssamplings\nSystematisierung nach Masur (2019)"
  },
  {
    "objectID": "slides/slides-04.html#personen--undoder-situationsebene",
    "href": "slides/slides-04.html#personen--undoder-situationsebene",
    "title": "üìö Media routines & habits",
    "section": "Personen- und/oder Situationsebene?",
    "text": "Personen- und/oder Situationsebene?\nVerschiedene Arten von Forschungsfragen mit Beispiel\n\n\nFragen auf Personenebene (between-subject): Daten √ºber Messzeitpunkte aggregiert\n\nPersonenmittelwert: Wie ist das durchschnittliche Wohlbefinden (Y) in der Personenstichprobe?\nVarianz der Personenmittelwerte: Welche Unterschiede im durchschnittlichen Wohlbefinden (Y) gibt es zwischen Personen?\nKorrelation auf Personenebene: H√§ngen Unterschiede im Wohlbefinden (Y) mit Unterschieden in der durchschnittlichen sozialen Interaktion (X) zusammen?\nKausalzusammenhang: Erkl√§rt eine experimentelle Manipulation der sozialen Interaktion (X) die Unterschiede im Wohlbefinden (Y)?\n\nFragen auf Situationsebene (within-subject): Daten mehrerer Messzeitpunkte einer Person\n\nVarianz der Situationswerte: Wie stark weicht das situative Wohlbefinden (Y) vom Durchschnitt einer Person ab?\nKorrelation auf Situationsebene: H√§ngen diese Abweichungen im situativen Wohlbefinden (Y) mit situativen Unterschieden in der sozialen Interaktion (X) einer Person zusammen?\nGranger Kausalzusammenhang: Erkl√§rt die soziale Interaktion einer Person in der Mitte des Tages (X) die Unterschiede im Wohlbefinden dieser Person am Ende des Tages (Y)?"
  },
  {
    "objectID": "slides/slides-04.html#just-one-more-scroll",
    "href": "slides/slides-04.html#just-one-more-scroll",
    "title": "üìö Media routines & habits",
    "section": "‚ÄúJust one more scroll‚Äù",
    "text": "‚ÄúJust one more scroll‚Äù\nMediengewohnheiten und Ihre Effekte\n\n\n\n\n\n\n\n\n\n\n\n\nProkrastination\n\n\n‚Äúthe voluntary delay of an intended and necessary and/or [personally] impoortant activity, despite expecting potential negative consequences that outweight the positive consequences of the delay‚Äù Klingsieck (2013)\n\n\n\n\nklare Unterscheidung zu ‚Äústrategischem Aufschieben‚Äù\nweit verbreitetes Ph√§nomen\nZusammenhang mit Wohlbefinden\n\n\n\n\nVerz√∂gerung ist - unn√∂tig oder irrational - trotz Bewusstsein √ºber m√∂gliche negative Konsequenzen - resultiert in negativen Konsequenzen (‚Äúschlechtes Gef√ºhl‚Äù)\nActivites == Aufgaben & Entscheidungen"
  },
  {
    "objectID": "slides/slides-04.html#design-your-own-research-design",
    "href": "slides/slides-04.html#design-your-own-research-design",
    "title": "üìö Media routines & habits",
    "section": "üß™ Design your own research (design)",
    "text": "üß™ Design your own research (design)\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nErstellung ein Forschungsdesign(konzept) zur Untersuchung des Einflusses von Mediennutzung(sroutinen) auf Wohlsein und/oder Procrastination im Rahmen dieses Kurses.\n\n\n\nN√§chste Schritte\n\n\nFinden Sie sich in Ihren Gruppen zusammen\nFormulieren Sie zwei Forschungsfragen (eine auf Personen- eine auf Situationsebene), die mit Hilfe eines ILD (siehe Systematik von Masur) untersucht werden kann. Nutzen Sie das CMC um die Mediennutzung m√∂glichst genau zu operationalisieren.\n√úberlegen Sie, wie bzw. welche digitalen Verhaltensdaten Ihre Untersuchung unterst√ºzen k√∂nnten\nHalten Sie Ihre Fragen & Desginideen auf der f√ºr Ihre Gruppe vorgesehenen Folienvorlage (n√§chste Slide) fest."
  },
  {
    "objectID": "slides/slides-04.html#let-the-work-beginn",
    "href": "slides/slides-04.html#let-the-work-beginn",
    "title": "üìö Media routines & habits",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Pr√§sentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-04.html#literatur",
    "href": "slides/slides-04.html#literatur",
    "title": "üìö Media routines & habits",
    "section": "Literatur",
    "text": "Literatur\n\n\nBolger, N., & Laurenceau, J.-P. (2013). Intensive longitudinal methods: An introduction to diary and experience sampling research. Guilford Press.\n\n\nKlingsieck, K. B. (2013). Procrastination: When Good Things Don‚Äôt Come to Those Who Wait. European Psychologist, 18(1), 24‚Äì34. https://doi.org/10.1027/1016-9040/a000138\n\n\nMasur, P. K. (2019). Capturing situational dynamics: Strength and pitfalls of the experience sampling method (P. M√ºller, S. Gei√ü, T. K. Naab, & C. Peter, Eds.; Vol. 15). Herbert von Halem Verlag. https://osf.io/vx5ha\n\n\nMeier, A., & Reinecke, L. (2021). Computer-Mediated Communication, Social Media, and Mental Health: A Conceptual and Empirical Meta-Review. Communication Research, 48(8), 1182‚Äì1209. https://doi.org/gjf96r\n\n\nValkenburg, P. M., & Peter, J. (2013). The Differential Susceptibility to Media Effects Model: Differential Susceptibility to Media Effects Model. Journal of Communication, 63(2), 221‚Äì243. https://doi.org/10.1111/jcom.12024\n\n\nValkenburg, P. M., Peter, J., & Walther, J. B. (2016). Media Effects: Theory and Research. Annual Review of Psychology, 67(1), 315‚Äì338. https://doi.org/10.1146/annurev-psych-122414-033608\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-11.html#schedule",
    "href": "slides/slides-11.html#schedule",
    "title": "Q&A",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis üé•\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-11.html#what-is-expected-update",
    "href": "slides/slides-11.html#what-is-expected-update",
    "title": "Q&A",
    "section": "What is expected (Update)",
    "text": "What is expected (Update)\nLeistungsanforderungen & Pr√ºfungsleistungen\n\n\n\nFeedback?"
  },
  {
    "objectID": "slides/slides-11.html#fa-person-chalkboard-project-topic-ideas",
    "href": "slides/slides-11.html#fa-person-chalkboard-project-topic-ideas",
    "title": "Q&A",
    "section": " Project topic idea(s)",
    "text": "Project topic idea(s)\nProjektidee vorstellen & weiterentwickeln\n\nUmfang: maximal 10 Minuten & 5 Slides pro Gruppe\nZiel: Idee f√ºr Gruppenprojekt pr√§sentieren, offene Fragen kl√§ren und Zeit f√ºr Diskussion & Feedback\nDeadline 31.01.\n\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-11.html#pitch-diskussion-repeat",
    "href": "slides/slides-11.html#pitch-diskussion-repeat",
    "title": "Q&A",
    "section": "Pitch ‚ûû Diskussion ‚ûû Repeat",
    "text": "Pitch ‚ûû Diskussion ‚ûû Repeat\nAblauf der Sitzung in der n√§chsten Woche\n\n\nProject topic idea(s) (ca 5-10 Min)\n\nkurzer √úberblick √ºber Thema, Forschungsfrage oder Motivation & ausgew√§hlte Daten(teil)stichprobe (2 Folien),\nkurze Beschreibung von Methode und (geplanter) Analyse (1 Folie)\nErgebnisse und/oder eine Herausforderung aufzeigen, die im Kurs diskutiert werden soll(en) (2 Folien).\n\n\nFragen & Diskussion (ca 5-10 Min)\n\nZeit f√ºr Fragen, entweder von der Gruppe an den Kurs oder umgekehrt.\n\n\n\n\nüîÅ f√ºr jede Gruppe\n\n\n\nStatus bzw. erster Ergebnisse der Project Proposal & Short Repots (max. 5 Folien)"
  },
  {
    "objectID": "slides/slides-11.html#fa-file-lines-project-proposal",
    "href": "slides/slides-11.html#fa-file-lines-project-proposal",
    "title": "Q&A",
    "section": " Project proposal",
    "text": "Project proposal\nErster Entwurf des short report\n\n\n\nUmfang: mindestens 500 W√∂rter\nZiel: Forschungsfrage (weiter-)entwickeln und verschriftlichen sowie fr√ºhzeitige Entwicklung einer spezifischen Analysestrategie\nDeadline 11.02.\nZusendung von Google Docs Vorlage"
  },
  {
    "objectID": "slides/slides-11.html#fa-comments-peer-review",
    "href": "slides/slides-11.html#fa-comments-peer-review",
    "title": "Q&A",
    "section": " Peer Review",
    "text": "Peer Review\nFeedback f√ºr Bericht geben & bekommmen\n\n\n\nUmfang: Schriftliches Feedback via Peer-Review-Formular\nZiel: Konstruktives Feedback schreiben\nDeadline: 18.02.\nVerwendung eines Google Forms"
  },
  {
    "objectID": "slides/slides-11.html#fa-file-signature-short-report",
    "href": "slides/slides-11.html#fa-file-signature-short-report",
    "title": "Q&A",
    "section": " Short Report",
    "text": "Short Report\nZusammenf√ºhrung der einzelnen Teileistungen\n\n\n\nUmfang: Grupppenbericht zu Kursthema mit 1200 bis 2400 W√∂rter (abh√§ngig von Gruppengr√∂√üe)\nZiel(e): Anwendung vorgestellter Methoden & Daten\nDeadline: 10.03.\nVerwendung eines Quarto Journal Templates"
  },
  {
    "objectID": "slides/slides-11.html#a-work-in-pogress",
    "href": "slides/slides-11.html#a-work-in-pogress",
    "title": "Q&A",
    "section": "A work in pogress",
    "text": "A work in pogress\nStatus des Datensatzes: Mehr Vorarbeit, weniger Issues\n\nAktueller Fokus: Datensatzbereinigung und Komprimierung\nAktuelles Problem: Duplikatsidentifikation\nDokumentation der √úberarbeitung wird zur Verf√ºgung gestellt"
  },
  {
    "objectID": "slides/slides-11.html#fragen-√ºber-fragen",
    "href": "slides/slides-11.html#fragen-√ºber-fragen",
    "title": "Q&A",
    "section": "Fragen √ºber Fragen",
    "text": "Fragen √ºber Fragen\nR-Sitzungen & Kurseevaluation\n\nWie waren die praktischen R-Sitzungen f√ºr Sie?\nWie sind Sie zurechtgekomment mit ‚Ä¶\n\nR/RStudio?\nden Inhalten (Textanalyse, Topic Modeling,etc)?\nEnglisch?\n\n\n. . .\n\nHaben Sie an der Kursevaluation teilgenommen?"
  },
  {
    "objectID": "slides/slides-11.html#literatur",
    "href": "slides/slides-11.html#literatur",
    "title": "Q&A",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-02.html#kurzes-update",
    "href": "slides/slides-02.html#kurzes-update",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Kurzes Update",
    "text": "Kurzes Update\nAllgemeine Infos zum Kurs\n\nüìñ Basisliteratur zu den Pr√§sentationen auf StudOn verf√ºgbar!\n‚è∞ Regelm√§√üige Kontrolle von Zulip\nüßÆ Denken Sie an die Deadline f√ºr das Zertifikat vom R-Basiskurs (13.11.2023)\nüó£Ô∏è 1. Pr√§sentationsgruppe: Denken Sie an die Zusendung des Entwurf der Pr√§sentationsfolien und das Feedbackgespr√§ch n√§chste Woche!\n\n\n\nZulip: Haben alle die Zuteilung der Themen/Gruppen gesehen?"
  },
  {
    "objectID": "slides/slides-02.html#die-w√ºrfel-sind-gefallen",
    "href": "slides/slides-02.html#die-w√ºrfel-sind-gefallen",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Die W√ºrfel sind gefallen",
    "text": "Die W√ºrfel sind gefallen\nKurzer √úberblick der Gruppenaufteilung\n\n\n\n\n\n\n\n  \n    \n    \n      Gruppe\n      Thema\n      Studierende\n    \n  \n  \n    A\nüìö Digital disconnection\nKofer, Rieger\n    B\nüì¶ Automatic text analysis (Topic Modeling & Netzwerkanalyse)\nK√∂bler, M√ºhlmeister, Neudecker\n    C\nüìö Media routines & habits\nBudak, Knapp, Kuck\n    D\nüì¶ Data collections methods (mit Schwerpunkt Data Donations)\nJakob, Neumeier"
  },
  {
    "objectID": "slides/slides-02.html#abstimmung-des-semesterplans",
    "href": "slides/slides-02.html#abstimmung-des-semesterplans",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Abstimmung des Semesterplans",
    "text": "Abstimmung des Semesterplans\n\n\n\n\n\n\n\n\n  \n    \n       Option A\n    \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüìÇ Project 1\n\nAnalysis of media content\n\n\n    4\n\n22.11.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    5\n\n29.11.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    6\n\n06.12.2023\n\nüî® Text as data in R\n\nChristoph Adrian\n\n    7\n\n13.12.2023\n\nüìä Presentation & Discussion\n\nAll groups\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project 2\n\nAnalysis of media usage\n\n\n    9\n\n10.01.2024\n\nüìö Media routines & habits\n\nGroup C\n\n    10\n\n17.01.2024\n\nüì¶ Data collection methods\n\nGroup D\n\n    11\n\n24.01.2024\n\nüî® Working data logs\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n    \n       Option B\n    \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-02.html#tldr-von-option-b",
    "href": "slides/slides-02.html#tldr-von-option-b",
    "title": "Einf√ºhrung & √úberblick",
    "section": "TL;DR von Option B",
    "text": "TL;DR von Option B\nZusammenfassung der √Ñnderungen\n\n\nPr√§sentationen in 2023 & Projektarbeit in 2024\nnur ein Pr√§sentationstermin mit ‚ÄúProject topic idea(s)‚Äù\nFokus auf Analyse von Medieninhalten (Project 1), keine Analyse der Logging-Daten (Project 2)\nAber (Optional): Project 2 light\n\nDurchf√ºhrung einer Mini-Studie im Kurs (ben√∂tigt Einverst√§ndnis)\nFokus des Projektberichts auf Datenaufbereitung & -exploration\n\n\n\n\n\n\nFragen? Anmerkungen?"
  },
  {
    "objectID": "slides/slides-02.html#please-vote",
    "href": "slides/slides-02.html#please-vote",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Please vote!",
    "text": "Please vote!\nWelche Option des Seminarplans bevorzugen Sie?\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/al3qwznms52z\nTemporary Access Code: 5623 9279\n\n\n\n\n\n\n\n    \n\n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "slides/slides-02.html#ergebnis",
    "href": "slides/slides-02.html#ergebnis",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-02.html#was-ist-das-eigentlich",
    "href": "slides/slides-02.html#was-ist-das-eigentlich",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Was ist das eigentlich?",
    "text": "Was ist das eigentlich?\nR√ºckblick auf einen Definitionversuch von Weller (2021)\n\n\n\n‚Ä¶ fasst eine Vielzahl von m√∂glichen Datenquellen zusammen, die verschiedene Arten von Aktivit√§ten aufzeichnen (h√§ufig sogar ‚Äúnur‚Äù als Nebenprodukt)\n‚Ä¶ k√∂nnen dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-02.html#und-im-kontext-des-seminars",
    "href": "slides/slides-02.html#und-im-kontext-des-seminars",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Und im Kontext des Seminars?",
    "text": "Und im Kontext des Seminars?\nArbeitsdefinition & Kernbereiche (GESIS) von DBD\n\n\n\n\nDBD umfasst digitale Beobachtungen menschlichen und algorithmischen Verhaltens,\nwie sie z.B. von Online-Plattformen (wie Google, Facebook oder dem World Wide Web) oder\nSensoren (wie Smartphones, RFID-Sensoren, Satelliten oder Street View-Kameras) erfasst werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchwerpunkt: Nutzung und Inhalte von soziale Medien\nComputational Social Science [CSS] Verfahren, z.B. zur Erhebung, Verarbeitung, Auswertung und Pr√§sentation\n\n\n\n\n\n\n\nUnterschiedliche Heraus- bzw. Anforderungen (je nach Bereich)"
  },
  {
    "objectID": "slides/slides-02.html#css-dbd",
    "href": "slides/slides-02.html#css-dbd",
    "title": "Einf√ºhrung & √úberblick",
    "section": "CSS üñáÔ∏è DBD",
    "text": "CSS üñáÔ∏è DBD\nKurzer Exkurs zur Bedeutung von Computational Social Science\n\n\n\nDefinition (Computational Social Science).\nWe define CSS as the development and application of computational methods to complex, typically large-scale, human (sometimes simulated) behavioral data.‚Äù (Lazer et al., 2020)\n\n\n\nhilft dabei ‚Ä¶\n\ngenuine digitale Ph√§nomene zu untersuchen\ndigitale Verhaltensdaten zu sammeln und vorzuverarbeiten\nneue Methoden zur Analyse von gro√üen Datens√§tzen anzuwenden\n\n\nCSS = neues Teilgebiet der Sozialwissenschaften oder neuer ‚ÄúWerkzeugkasten‚Äù zur Erg√§nzung der traditionellen sozialwissenschaftlichen Ans√§tze"
  },
  {
    "objectID": "slides/slides-02.html#von-verhalten-bis-interkation",
    "href": "slides/slides-02.html#von-verhalten-bis-interkation",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Von Verhalten bis Interkation",
    "text": "Von Verhalten bis Interkation\nBeispiel f√ºr untersuchbare Ph√§nomene samt Einschr√§nkungen\n\n\n\n\n\nQuelle: (Keusch & Kreuter, 2021)\n\n\n\n\nEinschr√§nkungen\n\nSelektive Nutzung von bestimmten digitalen Ger√§ten bzw. Funktionen\nKategorisierung ist Momentaufnahme und nicht √ºberschneidungsfrei\n\n\n\n\n\nEinige inh√§rent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)\nFehlen digitaler Spurendaten in all diesen Quadranten f√ºr bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Ger√§te."
  },
  {
    "objectID": "slides/slides-02.html#mehr-daten-durch-technologischen-fortschritt",
    "href": "slides/slides-02.html#mehr-daten-durch-technologischen-fortschritt",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Mehr Daten durch technologischen Fortschritt",
    "text": "Mehr Daten durch technologischen Fortschritt\nBeispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren\n\n\nGraphik aus Struminskaya et al. (2020)"
  },
  {
    "objectID": "slides/slides-02.html#verf√ºgbarkeit-als-pluspunkt",
    "href": "slides/slides-02.html#verf√ºgbarkeit-als-pluspunkt",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Verf√ºgbarkeit als Pluspunkt",
    "text": "Verf√ºgbarkeit als Pluspunkt\nDBD als wertvolle Quelle bei aktuellen, sensiblen & unvorhersehbaren Themen\n\nEinsatz besonders Vorteilhaft bei Themen bzw. Untersuchungen ‚Ä¶\n\n‚Ä¶ f√ºr die es schwierig ist, Studienteilnehmer*innen zu rekrutieren\n‚Ä¶ bei denen Beobachtungen vorteilhafter sind als Befragungen\n\n\n\nBeispiel: Streaming und/oder Mining von Inhalten aus bestehenden digitalen Kommunikationsstr√∂men\n\nZeitnaher als die Erstellung einer Umfrage\nZus√§tzlicher Nutzen als Archiv bei unvorhersehbaren Ereignissen\n\n\n\nüîî weitere Beispiele? - Well-being auf Basis von Instagram-Bildern & Texten"
  },
  {
    "objectID": "slides/slides-02.html#die-power-von-social-sensing",
    "href": "slides/slides-02.html#die-power-von-social-sensing",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Die Power von Social Sensing",
    "text": "Die Power von Social Sensing\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Fl√∂ck & Sen, 2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Zukunft: Linking"
  },
  {
    "objectID": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "href": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Mit Fokus auf die Platform",
    "text": "Mit Fokus auf die Platform\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Fl√∂ck & Sen, 2022)"
  },
  {
    "objectID": "slides/slides-02.html#online-plattformen-pr√§gen-die-gesellschaft",
    "href": "slides/slides-02.html#online-plattformen-pr√§gen-die-gesellschaft",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Online-Plattformen pr√§gen die Gesellschaft",
    "text": "Online-Plattformen pr√§gen die Gesellschaft\nGr√ºnde f√ºr den Fokus auf Onlineplattformen (Ulloa, 2021)\n\n\nvermitteln & formen menschliche Kommunikation (z.B. Tweet mit 280 Zeichen)\npolitische (Miss-)Nutzung\nGatekeeper f√ºr Informationen (z.B. ‚ÄúDr.Google‚Äù)\nt√§gliche algorithmische Empfehlungen und Werbung: Nachrichten, Produkte, Jobangebote, Bewerbungen, Versicherungen, Hotels, ‚Ä¶\n\n\n\nABER: Ber√ºcksichtigung der Art und Weise, wie Sie die Daten gesammelt werden!"
  },
  {
    "objectID": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "href": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Der Weg bestimmt das Ergebnis",
    "text": "Der Weg bestimmt das Ergebnis\nEinfluss der Erhebung auf die Daten(-form) (Davidson et al., 2023)"
  },
  {
    "objectID": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "href": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Wenn der Vorteil zum Nachteil wird",
    "text": "Wenn der Vorteil zum Nachteil wird\nAmbivalenz der Unaufdringlichkeit (Keusch & Kreuter, 2021)\n\nUnterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookies, Browserplugins & APIs) erhobenen Daten\nBewertung und Erwartung an Datensammlung ist abh√§ngig vom Kontext (z.B. Amazon vs.¬†Researchgate)\n\n\n\nParadoxes Dilemma\nEinerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne ohne Wissen um deren Weiterverarbeitung, andererseits h√§ufig Bedenken bez√ºglich Datenschutz & Privatsph√§re bei wissenschaftlichen Studien, die √ºber Verwendung der Daten aufkl√§ren.\n\n\nWarum? Pers√∂nlicher Nutzen?"
  },
  {
    "objectID": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "href": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Eine kleine Lobeshymne auf DBD",
    "text": "Eine kleine Lobeshymne auf DBD\nZwischenfazit\n\nDigitale Ger√§te oder Sensoren k√∂nnen sich an bestimmte Fakten besser ‚Äúerinnern‚Äù als das menschliche Ged√§chtnis.\nSensoren sind oft bereits in allt√§gliche Technologie eingebaut und produzieren digitale Verhaltensdaten als ein ‚ÄúNebenprodukt‚Äù.\nUnaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung f√ºr Teilnehmer*Innen\nKombination mit Umfragedaten m√∂glich (und bereichernd!)\n\n\n\nAber: Ber√ºcksichtigung der Rahmenbedingungen!\nZur erfolgreichen Nutzung m√ºssen Forschungsziele & verf√ºgbare Daten in Einklang gebracht, m√∂gliche Biases und methodische Probleme ber√ºcksichtigt sowie die Datenqualit√§t evaluiert werden.\n\n\nBietet die Plattform Zugang zu den ben√∂tigten Daten?\n\nWenn nicht, gibt es alternative Weg um an die Daten zu gelangen?\nWenn ja, ist dies legal/ethisch?"
  },
  {
    "objectID": "slides/slides-02.html#the-end-of-theory",
    "href": "slides/slides-02.html#the-end-of-theory",
    "title": "Einf√ºhrung & √úberblick",
    "section": "The End of Theory",
    "text": "The End of Theory\n\n\nZur Wichtigkeit von konzipierte Messungen & Designs\n\n\n\n‚ÄúWho knows why people do what they do? The point is they do it, and we can track and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves.‚Äù (Anderson, 2008)\n\n\n\n\nWas denken Sie?\n\n\n\n‚ÄúSize alone does not necessarily make the data better‚Äù (boyd & Ellison, 2007)\n\n\n‚ÄúThere are a lot of small data problems that occur in big data [which] don‚Äôt disappear because you‚Äôve got lots of the stuff. They get worse.‚Äù (Harford, 2014)"
  },
  {
    "objectID": "slides/slides-02.html#we-need-to-talk-about-biases",
    "href": "slides/slides-02.html#we-need-to-talk-about-biases",
    "title": "Einf√ºhrung & √úberblick",
    "section": "We need to talk about biases",
    "text": "We need to talk about biases\nSpezifische und allgemeine Herausforderungen f√ºr die Forschung mit DBD\nHintergrund: (Big) Data ist zunehmend Grundlage f√ºr politische Ma√ünahmen, die Gestaltung von Produkten und Dienstleistungen und f√ºr die automatisierte Entscheidungsfindung\n\nHerausforderungen in Bezug auf DBD-Forschung: fehlender Konsens √ºber ein Vokabular oder eine Taxonomie, h√§ufig nur impliziter Bezug in der Forschung\nGenerelle Herausforderung: bias ist ein weit gefasster & in unterschiedlichen Disziplinen genutzter Begriff"
  },
  {
    "objectID": "slides/slides-02.html#was-verstehen-sie-unter-bias",
    "href": "slides/slides-02.html#was-verstehen-sie-unter-bias",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Was verstehen Sie unter ‚Äúbias‚Äù?",
    "text": "Was verstehen Sie unter ‚Äúbias‚Äù?\nPlease participate!\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/al6nj5peoi88\nTemporary Access Code: 2359 9316\n\n\n\n\n\n\n\n    \n\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/slides-02.html#ergebnis-1",
    "href": "slides/slides-02.html#ergebnis-1",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-02.html#eher-konzept-als-begriff",
    "href": "slides/slides-02.html#eher-konzept-als-begriff",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Eher Konzept als Begriff",
    "text": "Eher Konzept als Begriff\nZur Ambigutit√§t des Begriffes bias und dessen Bedeutung im Seminar\n\n\nProblem: keine klare Grenzen zwischen den eher normativen Konnotationen (z.B. confirmation bias) und der statistischen Bedeutung des Begriffs (z.B. selection bias)\nDeswegen: Bewusstsein f√ºr Ambiguit√§t des Begriffes\n\nVerwendung in vielen Disziplinen wie der Sozialwissenschaft, der kognitiven Psychologie oder dem Recht\nUntersuchung von verschiedenen Ph√§nomenen, wie kognitive Voreingenommenheiten (Croskerry, 2002) sowie systemische, diskriminierende Ergebnisse (Friedman & Nissenbaum, 1996) oder Sch√§den (Barocas et al., 2017), aktuell z.B. bei der Verwendung von Machine Learning oder AI.\n\n\n\n\n\n\nVerwendung des Begriff haupts√§chlich in seiner statistischen Bedeutung, um auf Verzerrungen in sozialen Daten und deren Analysen hinzuweisen."
  },
  {
    "objectID": "slides/slides-02.html#know-your-bias",
    "href": "slides/slides-02.html#know-your-bias",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Know your bias!",
    "text": "Know your bias!\nFramework zur Minimierung von Fehlern und Problemen (Olteanu et al., 2019)\n\n\nBeschreibung:\n\nDie Analyse sozialer Daten beginnt mit bestimmten Zielen (Abschnitt 2.1), wie dem Verst√§ndnis oder der Beeinflussung von Ph√§nomenen, die f√ºr soziale Plattformen spezifisch sind (Typ I) und/oder von Ph√§nomenen, die √ºber soziale Plattformen hinausgehen (Typ II).\nDiese Ziele erfordern, dass die Forschung bestimmte Validit√§tskriterien erf√ºllt, die weiter oben beschrieben wurden (Abschnitt 2.2).\nDiese Kriterien k√∂nnen ihrerseits durch eine Reihe von allgemeinen Verzerrungen und Problemen beeintr√§chtigt werden (Abschnitt 3).\nDiese Herausforderungen k√∂nnen von den Merkmalen der einzelnen Datenplattformen (Abschnitt 4) abh√§ngen - die oft nicht unter der Kontrolle der Forschenden stehen - und von den Entscheidungen des Forschungsdesigns entlang einer Datenverarbeitungspipeline (Abschnitte 5 bis 8) - die oft unter der Kontrolle des Forschers stehen.\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken"
  },
  {
    "objectID": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "href": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "title": "Einf√ºhrung & √úberblick",
    "section": "The biggest problem of them all",
    "text": "The biggest problem of them all\nPotentielle Probleme mit der Qualit√§t der Daten\n\n\n\nDefinition (Data bias) (Olteanu et al., 2019)\nA systematic distortion in the sampled data that compromises its representativeness.\n\n\n\n\nSparsity: H√§ufig Heavy-Tail-Verteilung, was Analyse am ‚ÄúKopf‚Äù (in Bezug auf h√§ufige Elemente oder Ph√§nomene) erleichtert, am ‚ÄúSchwanz‚Äù (wie seltene Elemente oder Ph√§nomene) jedoch erschwert (Baeza-Yates, 2013)\nNoise: Unvollst√§ndige, besch√§digte, unzuverl√§ssige oder unglaubw√ºrdige Inhalte (boyd & Crawford, 2012; Naveed et al., 2011)\n\nAber: Unterscheidung von ‚ÄúNoise‚Äù und ‚ÄúSignal‚Äù ist oft unklar und h√§ngt von der Forschungsfrage ab (Salganik, 2018)\n\nOrganische vs gemessene Daten: Fragen zur Repr√§sentativit√§t (vs.¬†Stichprobenbeschreibung), Kausalit√§t (vs.¬†Korrelation) und Vorhersageg√ºte"
  },
  {
    "objectID": "slides/slides-02.html#bias-at-the-source",
    "href": "slides/slides-02.html#bias-at-the-source",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Bias at the source",
    "text": "Bias at the source\nPotentielle Probleme mit der Datenquelle oder -herkunft\n\nBiases, die auf das Design und die M√∂glichkeiten der Plattformen zur√ºckzuf√ºhren sind (functional biases).\nVerhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).\nFaktoren, die au√üerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen k√∂nnen (external biases)\nVorhandensein von nicht-individuellen Konten ein (non-individuals).\n\n\nfunctional biases:\n- Platform-specific design and features shape user behavior (z.B. Emojis) - Algorithms used for organizing and ranking content influence user behavior - Content presentation influences user behavior (z.B. UI)\nnormative biases:\n\nNorms are shaped by the attitudes and behaviors of online communities, which may be context-dependent (z.B. Partyfotos auf Instagram, aber nicht LinkedIn)\nThe awareness of being observed by others impacts user behavio (Anonymit√§t vs Klarnamen)\nSocial conformity and ‚Äúherding‚Äù happen in social platforms, and such behavioral traits shape user behavior (z.B. Ratings beinflussen eigenes Rating)\n\nexternal biase:\n\nCultural elements and social contexts are reflected in social datasets. (Zeichenlimit Japan vs.¬†Deutschland)\nMisinformation and disinformation.\nContents on different topics are treated differently.\nHigh-impact events, whether anticipated or not, are reflected on social media (z.B. Feiertage)\n\nnon-individual-accounts: Organizational accounts, Bots"
  },
  {
    "objectID": "slides/slides-02.html#gefangen-im-spannungsverh√§ltnis",
    "href": "slides/slides-02.html#gefangen-im-spannungsverh√§ltnis",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Gefangen im Spannungsverh√§ltnis",
    "text": "Gefangen im Spannungsverh√§ltnis\nForschungethik bei digitalen Daten\nHintergrund: Die Herausforderung besteht in der Kombination von zwei extremen Sichtweisen, der Betrachtung der Forschung mit sozialen Daten als ‚Äúklinische‚Äù Forschung oder als Computerforschung\n\nDie Sozialdatenforschung unterscheidet sich von klinischen Versuchen.\nEthische Entscheidungen in der Sozialdatenforschung m√ºssen gut √ºberlegt sein, da oft sind mehrere Werte betroffen, die miteinander in Konflikt stehen k√∂nnen\nDiskussion des Spannungsverh√§ltnisses am Beispiel von drei spezifischer ethischer Kriterien: Autonomie, Wohlt√§tigkeit und Gerechtigkeit\n\n\nHintergrund:\n\nDie Sozialdatenforschung √§hnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer F√§higkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden\ndie Sozialdatenforschung √§hnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.\n\nPunkt 2: Sch√§den, die die √ºblichen Arten der Sozialdatenforschung ( z. B. die Verletzung der Privatsph√§re oder der Anblick verst√∂render Bilder)verursachen k√∂nnen, oft nicht mit Sch√§den von klinischen Versuchen gleichzusetzen\nPunkt 3: Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten L√∂sungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015)."
  },
  {
    "objectID": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "href": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Achtung der individuellen Autonomie",
    "text": "Achtung der individuellen Autonomie\nDiskussion der Informierte Zustimmung als Indikator autonomer Entscheidung\n\n\n\n\n\n\nEinwilligung nach Aufkl√§rung setzt voraus, dass\n\n\n\n\ndie Forscher*Innen den potenziellen Teilnehmenden alle relevanten Informationen offenlegen;\ndie potenziellen Teilnehmenden in der Lage sind, diese Informationen zu bewerten;\ndie potenziellen Teilnehmenden freiwillig entscheiden k√∂nnen, ob sie teilnehmen wollen oder nicht;\ndie Teilnehmenden den Forschernden ihre ausdr√ºckliche Erlaubnis erteilen, h√§ufig in schriftlicher Form; und\ndie Teilnehmende die M√∂glichkeit haben, ihre Einwilligung jederzeit zur√ºckzuziehen.\n\n\n\n\n\nPotentielle Probleme mit Blick auf DBD\n\nDie Zustimmung von Millionen von Nutzern einzuholen ist nicht praktikabel.\nDie Nutzungsbedingungen sozialer Plattformen stellen m√∂glicherweise keine informierte Zustimmung zur Forschung dar.\nDas √∂ffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung."
  },
  {
    "objectID": "slides/slides-02.html#no-no-yes",
    "href": "slides/slides-02.html#no-no-yes",
    "title": "Einf√ºhrung & √úberblick",
    "section": "No ‚ÄúNo‚Äù ‚â† ‚ÄúYes‚Äù!",
    "text": "No ‚ÄúNo‚Äù ‚â† ‚ÄúYes‚Äù!\nEthische Erw√§gungen bei DBD-Forschung\n\nAus √∂ffentlicher Zug√§nglich- bzw. Verf√ºgbarkeit von Daten leitet sich nicht automatisch ethische Verwertbarkeit ab (boyd & Crawford, 2012; Zimmer, 2010)\n\nVerletzung der Privatsph√§re der Nutzer (Goroff, 2015)\nErm√∂glichung von rassischem, sozio√∂konomischem oder geschlechtsspezifischem Profiling (Barocas & Selbst, 2016)\n\nNegative Beispiele\n\nFacebook contagion experiment (2012-2014): Feeds von Nutzer*Innen so manipulierten, dass sie je nach den ge√§u√üerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014)\nEncore-Forschungsprojekt: Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett & Feamster, 2014)\n\n\n\nHintergrund:\n\nEthische Fragen bisher epistemische Bedenken (Verwendung von nicht schl√ºssigen oder fehlgeleiteten Beweisen), jetzt normativ Bedenken (Folgen der Forschung)\nForschung grunds√§tzlich in vielen L√§ndern gesetztlich geregelt\n\nNegativbeispiele:\n\nFacebook contagion experiment: Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a).\nEncore-Forschngsprojekt: Menschen in einigen L√§ndern durch diese Zugriffsversuche m√∂glicherweise gef√§hrdet wurden\n\nFolgende Abschnitte:\n\nzentrales Spannungsverh√§ltnis in der Forschungsethik digitaler Daten dargestellt.\nAnschlie√üend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohlt√§tigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4)."
  },
  {
    "objectID": "slides/slides-02.html#wohlt√§tigkeit-und-unsch√§dlichkeit-als-ziel",
    "href": "slides/slides-02.html#wohlt√§tigkeit-und-unsch√§dlichkeit-als-ziel",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Wohlt√§tigkeit und Unsch√§dlichkeit als Ziel",
    "text": "Wohlt√§tigkeit und Unsch√§dlichkeit als Ziel\nBewertung von Risken & Nutzen\nHintergrund: Nicht nur Fokus auf den Nutzen der Forschung, sondern auch auf die m√∂glichen Arten von Sch√§den, die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden k√∂nnen . (Sweeney, 2013)\n\nPotentielle Probleme\n\nDaten √ºber Einzelpersonen k√∂nnen ihnen schaden, wenn sie offengelegt werden.\nForschungsergebnisse k√∂nnen verwendet werden, um Schaden anzurichten.\n‚ÄúDual-Use‚Äù- und Sekund√§ranalysen sind in der Sozialdatenforschung immer h√§ufiger anzutreffen.\n\n\nDie Forschung zu sozialen Daten wird mit bestimmten Arten von Sch√§den in Verbindung gebracht, von denen die Verletzung der Privatsph√§re vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).\nBeispiel 1: Einige prominente Beispiele sind die Datenpanne bei Ashley Madison im Jahr 2015, bei der einer Website, die sich als Dating-Netzwerk f√ºr betr√ºgerische Ehepartner anpreist, Kontoinformationen (einschlie√ülich der vollst√§ndigen Namen der Nutzer) gestohlen und online gestellt wurden (Thomsen, 2015), sowie die j√ºngsten Datenpannen bei Facebook, bei denen Hunderte Millionen von Datens√§tzen mit Kommentaren, Likes, Reaktionen, Kontonamen, App-Passw√∂rtern und mehr √∂ffentlich gemacht wurden.\nzu 1: - Stalking, Diskriminierung, Erpressung oder Identit√§tsdiebstahl (Gross und Acquisti, 2005). - Zu lange Archivierung personenbezogener Daten oder die √∂ffentliche Freigabe schlecht anonymisierter Datens√§tze kann zu Verletzungen der Privatsph√§re f√ºhren, da diese Daten mit anderen Quellen kombiniert werden k√∂nnen, um Erkenntnisse √ºber Personen ohne deren Wissen zu gewinnen (Crawford und Finn, 2014; Goroff, 2015; Horvitz und Mulligan, 2015)\nzu 2: Abgesehen von der Tatsache, dass aus sozialen Daten gezogene R√ºckschl√ºsse in vielerlei Hinsicht falsch sein k√∂nnen, wie in dieser Studie hervorgehoben wird, k√∂nnen zu pr√§zise R√ºckschl√ºsse dazu f√ºhren, dass Menschen in immer kleinere Gruppen eingeteilt werden k√∂nnen (Barocas, 2014).\nzu 3: Daten, Instrumente und Schlussfolgerungen, die f√ºr einen bestimmten Zweck gewonnen wurden, f√ºr einen anderen Zweck verwendet werden (Hovy und Spruit, 2016; Benton et al., 2017)"
  },
  {
    "objectID": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "href": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Faire Verteilung von Risiken & Nutzen",
    "text": "Faire Verteilung von Risiken & Nutzen\nRecht & Gerechtigkeit\nHintergrund: H√§ufig wird unterstellt bzw. angenommen, dass es von Anfang an bekannt, wer durch die Forschung belastet und wer von den Ergebnissen profitieren wird.\n\nPotentielle Probleme\n\nDie digitale Kluft kann das Forschungsdesign beeinflussen (z.B. WEIRD Samples)\nAlgorithmen und Forschungsergebnisse k√∂nnen zu Diskriminierung f√ºhren.\nForschungsergebnisse sind m√∂glicherweise nicht allgemein zug√§nglich.\nNicht alle Interessengruppen werden √ºber die Verwendung von Forschungsergebnissen konsultiert.\n\n\nzu 1: Data divide: mangelnde Verf√ºgbarkeit von hochwertigen Daten √ºber Entwicklungsl√§nder und unterprivilegierte Gemeinschaften (Cinnamon und Schuurman, 2013). WEIRD = White, Educated, Industrialized, Rich, and Democratic\nzu 3: Idealerweise sollten die Menschen Zugang zu den Forschungsergebnissen und Artefakten haben, die aus der Untersuchung ihrer pers√∂nlichen Daten entstanden sind (Gross und Acquisti, 2005; Crawford und Finn, 2014).\nzu 4: In die √úberlegungen dar√ºber, wie, f√ºr wen und wann Forschungsergebnisse umgesetzt werden, sollten diejenigen einbezogen werden, die m√∂glicherweise betroffen sind oder deren Daten verwendet werden (Costanza-Chock, 2018; Design Justice, 2018; Green, 2018)"
  },
  {
    "objectID": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "href": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Zwei Trends, Drei Fragen, Vier Empfehlungen",
    "text": "Zwei Trends, Drei Fragen, Vier Empfehlungen\nZusammenfassung und Ausblick\nTrend 1: Skepsis gegen√ºber einfachen Antworten\n\n\nWie einstehen die Daten, was enthalten sie tats√§chlich und wie sind die Arbeitsdatens√§tze zusammengestellt?\nWird deutlich, was ausgewertet wird?\nWird die Verwendung von vorhandenen Datens√§tzen und Modellen des maschinellen Lernens hinterfragt?\n\n\nTrend 2: Wechsel von der Thematisierung zur Adressieung von Bedenken\n\n\nDetaillierte Dokumentation und kritische Pr√ºfung der Datensatz- und Modellerstellung\nDBD-Studien auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden\nTransparenzmechanismen zu schaffen, die es erm√∂glichen, Online-Plattformen zu √ºberpr√ºfen und Verzerrungen in Daten an der Quelle zu evaluieren\nForschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre √úbernahme zu f√∂rdern.\n\n\n\nSchlie√ülich gibt es angesichts der Komplexit√§t der inh√§rent kontextabh√§ngigen, anwendungs- und bereichsabh√§ngigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitsl√∂sungen - bei der Bewertung und Bek√§mpfung von Verzerrungen ist Nuancierung entscheidend."
  },
  {
    "objectID": "slides/slides-02.html#literatur",
    "href": "slides/slides-02.html#literatur",
    "title": "Einf√ºhrung & √úberblick",
    "section": "Literatur",
    "text": "Literatur\n\n\nAnderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. https://www.wired.com/2008/06/pb-theory/\n\n\nBaeza-Yates, R. A. (2013). Big data or right data.\n\n\nBarocas, S., Crawford, K., Shapiro, A., & Wallach, H. (2017). The problem with bias: From allocative to representational harms in machine learning. Special interest group for computing. Information and Society (SIGCIS), 2.\n\n\nBarocas, S., & Selbst, A. D. (2016). Big Data‚Äôs Disparate Impact. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2477899\n\n\nboyd, danah m., & Crawford, K. (2012). Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, Communication & Society, 15(5), 662‚Äì679. https://doi.org/10.1080/1369118X.2012.678878\n\n\nboyd, danah m., & Ellison, N. B. (2007). Social Network Sites: Definition, History, and Scholarship. Journal of Computer-Mediated Communication, 13(1), 210‚Äì230. https://doi.org/10.1111/j.1083-6101.2007.00393.x\n\n\nBurnett, S., & Feamster, N. (2014). Encore: Lightweight measurement of web censorship with cross-origin requests. https://doi.org/10.48550/ARXIV.1410.1211\n\n\nCroskerry, P. (2002). Achieving Quality in Clinical Decision Making: Cognitive Strategies and Detection of Bias. Academic Emergency Medicine, 9(11), 1184‚Äì1204. https://doi.org/10.1197/aemj.9.11.1184\n\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Linden, D. van der, Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. http://dx.doi.org/10.31234/osf.io/ps32z\n\n\nFl√∂ck, F., & Sen, I. (2022). Digital traces of human behaviour in online platforms  research design and error sources. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meet_the_experts_Digitaltraces_humanbehaviour.pdf\n\n\nFriedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems, 14(3), 330‚Äì347. https://doi.org/10.1145/230538.230561\n\n\nGoroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. Science, 347(6221), 479‚Äì480. https://doi.org/10.1126/science.aaa3483\n\n\nHarford, T. (2014). Big data: A big mistake? Significance, 11(5), 14‚Äì19. https://doi.org/10.1111/j.1740-9713.2014.00778.x\n\n\nKeusch, F., & Kreuter, F. (2021). Digital trace data. In Handbook of Computational Social Science, Volume 1 (1st ed., pp. 100‚Äì118). Routledge. https://doi.org/10.4324/9781003024583-8\n\n\nKramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788‚Äì8790. https://doi.org/10.1073/pnas.1320040111\n\n\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060‚Äì1062. https://doi.org/10.1126/science.aaz8170\n\n\nNaveed, N., Gottron, T., Kunegis, J., & Alhadi, A. C. (2011). the 20th ACM international conference. 183. https://doi.org/10.1145/2063576.2063607\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kƒ±cƒ±man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\nStruminskaya, B., Lugtig, P., Keusch, F., & H√∂hne, J. K. (2020). Augmenting Surveys With Data From Sensors and Apps: Opportunities and Challenges. Social Science Computer Review, 089443932097995. https://doi.org/10.1177/0894439320979951\n\n\nSweeney, L. (2013). Discrimination in Online Ad Delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue, 11(3), 10‚Äì29. https://doi.org/10.1145/2460276.2460278\n\n\nUlloa, R. (2021). Introduction to online data acquisition. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZimmer, M. (2010). ‚ÄúBut the data is already public‚Äù: on the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313‚Äì325. https://doi.org/10.1007/s10676-010-9227-5\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-08.html#seminarplan",
    "href": "slides/slides-08.html#seminarplan",
    "title": "üì¶ Automatic text analysis",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\nüéÉ Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinf√ºhrung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\nüî® Working with R\n\nChristoph Adrian\n\n    \nüó£Ô∏è\n\nPresentations\n\n\n    4\n\n22.11.2023\n\nüìö Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\nüìö Digital disconnection\n\n\n    6\n\n06.12.2023\n\nüìö Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\nüì¶ Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\nüì¶ Automatic text analysis\n\nGroup B\n\n    \n\nüéÑChristmas Break (No Lecture)\n\n\n    \nüìÇ Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\nüî® Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\nüî® Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\nüî® Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\nüìä Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\nüèÅ Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-08.html#kurzes-organisatorisches-update",
    "href": "slides/slides-08.html#kurzes-organisatorisches-update",
    "title": "üì¶ Automatic text analysis",
    "section": "Kurzes organisatorisches Update",
    "text": "Kurzes organisatorisches Update\nInformationen zu praktischen Sessions\n\n\n\n‚è∞ Reminder\n\n\nN√§chsten zwei Sitzungen (10.1. & 17.01) in Koorperatin mit Kurs Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing\n\n\n\n\nVortrag auf Englisch\nim CIP-Pool LG 0.215\nentweder zur Zeit des Seminars (11:30 - 13:00) oder im Zeitslot Ihrer eigenen √úbung"
  },
  {
    "objectID": "slides/slides-08.html#zeit-f√ºr-feedback-und-anmerkungen",
    "href": "slides/slides-08.html#zeit-f√ºr-feedback-und-anmerkungen",
    "title": "üì¶ Automatic text analysis",
    "section": "Zeit f√ºr Feedback und Anmerkungen",
    "text": "Zeit f√ºr Feedback und Anmerkungen\nEvaluation der Veranstaltung Digital behavioral data\n\n\nBitte nehmen Sie √ºber den QR Code oder folgenden Link an der Evaluation teil:\n\nLink: eva.fau.de\nLosung: WADPG"
  },
  {
    "objectID": "slides/slides-08.html#literatur",
    "href": "slides/slides-08.html#literatur",
    "title": "üì¶ Automatic text analysis",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "sessions/session-03.html",
    "href": "sessions/session-03.html",
    "title": "Session 3",
    "section": "",
    "text": "‚úçÔ∏è Dealine (13.11.2023) f√ºr Kurszertifikat des Basiskurs R/RStudio. Bitte via E-Mail an christoph.adrian@fau.de\nüîç Checken Sie die Ressourcen unter Computing (z.B. R-Textbooks & R-Cheatsheets)"
  },
  {
    "objectID": "sessions/session-03.html#prepare",
    "href": "sessions/session-03.html#prepare",
    "title": "Session 3",
    "section": "",
    "text": "‚úçÔ∏è Dealine (13.11.2023) f√ºr Kurszertifikat des Basiskurs R/RStudio. Bitte via E-Mail an christoph.adrian@fau.de\nüîç Checken Sie die Ressourcen unter Computing (z.B. R-Textbooks & R-Cheatsheets)"
  },
  {
    "objectID": "sessions/session-03.html#participate",
    "href": "sessions/session-03.html#participate",
    "title": "Session 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 03"
  },
  {
    "objectID": "sessions/session-03.html#practice",
    "href": "sessions/session-03.html#practice",
    "title": "Session 3",
    "section": "Practice",
    "text": "Practice\nüîß Quarto installieren und ausprobieren.\n\n\n\n\n\n\nUseful resource\n\n\n\nSehr hilfreiches  Quarto Video Tutorial von Andy Field\n\n\nüìã Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-03.html#suggested-readings",
    "href": "sessions/session-03.html#suggested-readings",
    "title": "Session 3",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBauer, P. C., & Landesvatter, C. (2023). Writing a reproducible paper with RStudio and quarto. https://osf.io/ur4xn\nJonge, E. de, & Loo, M. van der (2013). An introduction to data cleaning with R.\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group."
  },
  {
    "objectID": "sessions/session-03.html#useful-resources",
    "href": "sessions/session-03.html#useful-resources",
    "title": "Session 3",
    "section": "Useful resources",
    "text": "Useful resources\nüìñ N√ºtzliches und ausf√ºhrliches Tutorial zur Nutzung von Git/Github in RStudio\nüìñ Happy Git and GitHub for the useR\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-01.html",
    "href": "sessions/session-01.html",
    "title": "Session 1",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure that you answered the survey send to you by (StudOn) mail."
  },
  {
    "objectID": "sessions/session-01.html#prepare",
    "href": "sessions/session-01.html#prepare",
    "title": "Session 1",
    "section": "Prepare",
    "text": "Prepare\n‚úçÔ∏è Fill out the short survey before the first session (see  StudOn for Link)\nüì® Join  Zulip using the invitation (please check your email address used for StudOn).\nüìñ Read the syllabus"
  },
  {
    "objectID": "sessions/session-01.html#participate",
    "href": "sessions/session-01.html#participate",
    "title": "Session 1",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 01 - Kick Off\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-10.html",
    "href": "sessions/session-10.html",
    "title": "Session 10",
    "section": "",
    "text": "üñ•Ô∏è Session 10\nüß∑ Showcase"
  },
  {
    "objectID": "sessions/session-10.html#participate",
    "href": "sessions/session-10.html#participate",
    "title": "Session 10",
    "section": "",
    "text": "üñ•Ô∏è Session 10\nüß∑ Showcase"
  },
  {
    "objectID": "sessions/session-10.html#practice",
    "href": "sessions/session-10.html#practice",
    "title": "Session 10",
    "section": "Practice",
    "text": "Practice\nüìã Exercise 10"
  },
  {
    "objectID": "sessions/session-10.html#suggested-readings",
    "href": "sessions/session-10.html#suggested-readings",
    "title": "Session 10",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\nSilge, E. H., & Julia (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-07.html",
    "href": "sessions/session-07.html",
    "title": "Session 7",
    "section": "",
    "text": "üî® Please install the üè¥‚Äç‚ò†Ô∏è Zeeschuimer browser plugin (only Firefox-based browsers) by following the instructions from the Github page.\nüî® Please test your üêàüêà 4CAT üêàüêà login details as soon as possible after receiving them via Zulip."
  },
  {
    "objectID": "sessions/session-07.html#prepare",
    "href": "sessions/session-07.html#prepare",
    "title": "Session 7",
    "section": "",
    "text": "üî® Please install the üè¥‚Äç‚ò†Ô∏è Zeeschuimer browser plugin (only Firefox-based browsers) by following the instructions from the Github page.\nüî® Please test your üêàüêà 4CAT üêàüêà login details as soon as possible after receiving them via Zulip."
  },
  {
    "objectID": "sessions/session-07.html#participate",
    "href": "sessions/session-07.html#participate",
    "title": "Session 7",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Session 07\nüåê üêàüêà 4CAT üêàüêà Server"
  },
  {
    "objectID": "sessions/session-07.html#mandatory-literature",
    "href": "sessions/session-07.html#mandatory-literature",
    "title": "Session 7",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nBaumgartner, S. E., Sumter, S. R., Petkeviƒç, V., & Wiradhany, W. (2022). A Novel iOS Data Donation Approach: Automatic Processing, Compliance, and Reactivity in a Longitudinal Study. Social Science Computer Review, 089443932110710. https://doi.org/10.1177/08944393211071068\nOhme, J., Araujo, T., Boeschoten, L., Freelon, D., Ram, N., Reeves, B. B., & Robinson, T. N. (2023). Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking. Communication Methods and Measures, 1‚Äì18. https://doi.org/10.1080/19312458.2023.2181319\nSkatova, A., & Goulding, J. (2019). Psychology of personal data donation. PLOS ONE, 14(11), e0224240. https://doi.org/10.1371/journal.pone.0224240\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1‚Äì17. https://doi.org/10.1080/19312458.2022.2109608\nYee, A. Z. H., Yu, R., Lim, S. S., Lim, K. H., Dinh, T. T. A., Loh, L., Hadianto, A., & Quizon, M. (2022). ScreenLife capture: An open-source and user-friendly framework for collecting screenome data from android smartphones. https://osf.io/sphq4\n\n\nüêàüêà 4CAT üêàüêà-Toolkit\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571‚Äì589. https://doi.org/10.5117/ccr2022.2.007.hage\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\n\nOther additional readings\n\nBreuer, J., Kmetty, Z., Haim, M., & Stier, S. (2022). User-centric approaches for collecting Facebook data in the ‚Äòpost-API age‚Äô: experiences from two studies and recommendations for future research. Information, Communication & Society, 1‚Äì20. https://doi.org/10.1080/1369118X.2022.2097015\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Application programming interfaces and web data for social research (1st ed., pp. 33‚Äì45). Routledge. https://www.taylorfrancis.com/books/9781003025245/chapters/10.4324/9781003025245-4\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). A brief history of APIs (1st ed., pp. 17‚Äì32). Routledge. https://www.taylorfrancis.com/books/9781003025245/chapters/10.4324/9781003025245-3\nJ√ºnger, J. (2022). Verhaltens-, Forschungs- oder Datenschnittstellen?: Application Programming Interfaces (APIs) aus diachron und synchron vergleichender Perspektive. https://doi.org/10.48541/DCR.V10.6\nLomborg, S., & Bechmann, A. (2014). Using APIs for Data Collection on Social Media. The Information Society, 30(4), 256‚Äì265. https://doi.org/10.1080/01972243.2014.915276\nOhme, J., Araujo, T., Vreese, C. H. de, & Piotrowski, J. T. (2021). Mobile data donations: Assessing self-report accuracy and sample biases with the iOS Screen Time function. Mobile Media & Communication, 9(2), 293‚Äì313. https://doi.org/10.1177/2050157920959106\nPuschmann, C., & Ausserhofer, J. (2017). Social data APIs (M. T. Sch√§fer & K. Es, van, Eds.). Amsterdam University Press. http://en.aup.nl/books/9789462981362-the-datafied-society.html\nSloan, L., & Quan-Haase, A. (2016). The role of APIs in data sampling from social media (pp. 146‚Äì160). SAGE Publications Ltd. http://methods.sagepub.com/book/the-sage-handbook-of-social-media-research-methods/i1486.xml\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/session-08.html",
    "href": "sessions/session-08.html",
    "title": "Session 8",
    "section": "",
    "text": "All articles of this section have to be read & used by the presenting group\n\n\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#mandatory-literature",
    "href": "sessions/session-08.html#mandatory-literature",
    "title": "Session 8",
    "section": "",
    "text": "All articles of this section have to be read & used by the presenting group\n\n\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#other-additional-readings",
    "href": "sessions/session-08.html#other-additional-readings",
    "title": "Session 8",
    "section": "Other additional readings",
    "text": "Other additional readings\n\nTopic Modeling\n\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288‚Äì296.\n\n\n\nTwitter\n\nBanda, J. M., Tekumalla, R., Wang, G., Yu, J., Liu, T., Ding, Y., Artemova, E., Tutubalina, E., & Chowell, G. (2021). A Large-Scale COVID-19 Twitter Chatter Dataset for Open Scientific ResearchAn International Collaboration. Epidemiologia, 2(3), 315‚Äì324. https://doi.org/10.3390/epidemiologia2030024\nJungherr, A. (2016). Twitter use in election campaigns: A systematic literature review. Journal of Information Technology & Politics, 13(1), 72‚Äì91. https://doi.org/10.1080/19331681.2015.1132401\nKarami, A., Lundy, M., Webb, F., & Dwivedi, Y. K. (2020). Twitter and research: A systematic literature review through text mining. IEEE Access, 8, 67698‚Äì67717. https://doi.org/10.1109/ACCESS.2020.2983656\nMueller, S. D., & Saeltzer, M. (2022). Twitter made me do it! Twitter‚Äôs tonal platform incentive and its effect on online campaigning. Information, Communication & Society, 25(9), 1247‚Äì1272. https://doi.org/10.1080/1369118X.2020.1850841\nM√ºnch, F. V., Thies, B., Puschmann, C., & Bruns, A. (2021). Walking Through Twitter: Sampling a Language-Based Follow Network of Influential Twitter Accounts. Social Media + Society, 7(1), 205630512098447. https://doi.org/10.1177/2056305120984475\nVasko, V., & Trilling, D. (2019). A permanent campaign? Tweeting differences among members of Congress between campaign and routine periods. Journal of Information Technology & Politics, 16(4), 342‚Äì359. https://doi.org/10.1080/19331681.2019.1657046\nYuan, S., Chen, Y., Vojta, S., & Chen, Y. (2022). More aggressive, more retweets? Exploring the effects of aggressive climate change messages on Twitter. New Media & Society, 146144482211222. https://doi.org/10.1177/14614448221122202\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "exercise/exercise-09.html",
    "href": "exercise/exercise-09.html",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-09.html#background",
    "href": "exercise/exercise-09.html#background",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-09.html#preparation",
    "href": "exercise/exercise-09.html#preparation",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-09.html#import-and-process-the-data",
    "href": "exercise/exercise-09.html#import-and-process-the-data",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/exercise-09.html#exercises",
    "href": "exercise/exercise-09.html#exercises",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrush up basic knowledge of working with R, tidyverse and ggplot2\nGet to know the typical steps of tidy text analysis with tidytext, from tokenisation and summarisation to visualisation.\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the ‚ÄúRun all chunks above‚Äù-button  of the next chunk.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\nüìã Exercise 1: Transform to ‚Äòtidy text‚Äô\n\nDefine custom stopwords\n\nEdit the vector remove_custom_stopwords by defining additional words to be deleted based on the analysis presented in the session. (Use | as a logical OR operator in regular expressions. It allows you to specify alternatives, e.g.¬†add |https.)\n\nCreate new dataset tweets_tidy_cleaned\n\nBased on the dataset tweets_detox,\n\nuse the str_remove_all function to remove the specified patterns of the remove_custom_stopwords vector. Use the mutate() function to edit the text variable.\nTokenize the ‚Äòtext‚Äô column using unnest_tokens.\nFilter out stop words using filter and stopwords$words\n\nSave this transformation by creating a new dataset with the name tweets_tidy_cleaned.\n\nCheck if transformation was successful (e.g.¬†by using the print() function)\n\n\n#|¬†eval: false\n\n# Define custom stopwords\nremove_custom_stopwords &lt;- \"&amp;|&lt;|&gt; ADD NEW TERMS HERE\"\n\n# Create new dataset tweets_tidy_cleaned\n\n# Check\n\n\n\nüìã Exercise 2: Summarize tokens\n\nCreate summarized data\n\nBased on the dataset tweets_tidy_cleaned, summarize the frequency of the individual tokens by using the count()-function on the variable text. Use the argument sort = TRUE to sort the dataset based on descending frequency of the tokens.\nSave this transformation by creating a new dataset with the name tweets_summarized_cleaned.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nOptional: Check results with a wordcloud\n\nBased on the sorted dataset tweets_summarized_cleaned\n\nSelect only the 100 most frequent tokens, using the function top_n()\nCreate a ggplot()-base with label = text and size = n as aes() and\nUse ggwordcloud::geom_text_wordclout() to create the wordcloud.\nUse scale_size_are() to adopt the scaling of the wordcloud.\nUse theme_minimal() for clean visualisation.\n\n\n\n\n# Create summarized data\n\n\n# Preview Top 50 token\n\n\n# Check distribution parameters\n\n\n# Optional: Check results with a wordcloud\n\n\n\nüìã Exercise 3: Couting and correlating pairs of words\n\n3.1 Couting word pairs within tweets\n\nCouting word pairs among sections\n\nBased on the dataset tweets_tidy_cleaned, count word pairs using widyr::pairwise_count(), with the arguments item = text, feature = tweet_id and sort = TRUE.\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\n\n\n# Couting word pairs among sections\n\n# Check \n\n\n\n3.2 Pairwise correlation\n\nGetting pairwise correlation\n\nBased on the dataset tweets_tidy_cleaned,\n\ngroup the data with the function group_by() by the variable text\nuse filter(n() &gt;= X) to only use tokens that appear at least a certain amount of times (X).\nPlease feel free to select an X of your choice, however, I would strongly recommend an X &gt; 100, as otherwise the following function might not be able to compute.\ncreate word correlations using widyr::pairwise_cor(), with the arguments item = text,feature = tweet_id and sort = TRUE.\n\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck pairs with highest correlation by using the print() function.\n\n\n# Getting pairwise correlation \n\n\n# Check pairs with highest correlation\n\n\n\n3.3. Optinal: Visualization\n\nCustomize the parameters in the following code chunk:\n\nselected_words, for defining the words you want to get the correlates for\nnumber_of_correlates, to vary the number of correlates shown in the graph.\n\nSet #|eval : true to execute chunk\n\n\n# eval: false\n\n# Define parameters\nselected_words &lt;- c(\"digital\")\nnumber_of_correlates &lt;- 5\n\n# Visualize correlates\ntweets_pairs_corr_cleaned %&gt;% \n  filter(item1 %in% selected_words) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = number_of_correlates) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder(item2, correlation)) %&gt;% \n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nüìã Exercise 4: Sentiment\n\nApply afinn dictionary to get sentiment\n\nBased on the dataset tweets_tidy_cleaned,\n\nmatch the words from the afinn dictionary with the tokens in the tweets by using the inner_join() function. Within inner_join() , please use the get_sentiments() -function with the dictionary \"afinn\" for y , c(\"text\" = \"word\") for the by and \"many-to-many\" for the relationship argument.\nuse group_by() for grouping the tweets by the variable tweet_id\nand summarize() the sentiment for each tweet, by creating a new variable (within the summarize() function), called sentiment, that is the sum (use sum()) of the sentiment values assigned to the words of the dictionary (variable value ).\n\nSave this transformation by creating a new dataset with the name tweets_sentiment_cleaned.\n\nCheck if transformation was successful by using the print() function.\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nVisualize distribution\n\nBased on the newly created dataset tweets_sentiment_cleaned, create a ggplot with sentiment as aes() and by using geom_histogram().\n\n\n\n# Apply 'afinn' dictionary to get sentiment\n\n\n# Check transformation\n\n\n# Check distribution statistics \n\n\n# Visualize distribution"
  },
  {
    "objectID": "exercise/exercise-09_solution.html",
    "href": "exercise/exercise-09_solution.html",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#background",
    "href": "exercise/exercise-09_solution.html#background",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n‚ÄûDigital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one‚Äôs perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness‚Äú. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do ‚Äúwe‚Äù talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)\nInitial query is searching for ‚Äúdigital detox‚Äù, ‚Äú#digitaldetox‚Äù, ‚Äúdigital_detox‚Äù\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#preparation",
    "href": "exercise/exercise-09_solution.html#preparation",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#import-and-process-the-data",
    "href": "exercise/exercise-09_solution.html#import-and-process-the-data",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#exercises",
    "href": "exercise/exercise-09_solution.html#exercises",
    "title": "Exercise 09: üî® Text as data in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrush up basic knowledge of working with R, tidyverse and ggplot2\nGet to know the typical steps of tidy text analysis with tidytext, from tokenisation and summarisation to visualisation.\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the ‚ÄúRun all chunks above‚Äù-button  of the next chunk.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\nüìã Exercise 1: Transform to ‚Äòtidy text‚Äô\n\nDefine custom stopwords\n\nEdit the vector remove_custom_stopwords by defining additional words to be deleted based on the analysis presented in the session. (Use | as a logical OR operator in regular expressions. It allows you to specify alternatives, e.g.¬†add |https.)\n\nCreate new dataset tweets_tidy_cleaned\n\nBased on the dataset tweets_detox,\n\nuse the str_remove_all function to remove the specified patterns of the remove_custom_stopwords vector. Use the mutate() function to edit the text variable.\nTokenize the ‚Äòtext‚Äô column using unnest_tokens.\nFilter out stop words using filter and stopwords$words\n\nSave this transformation by creating a new dataset with the name tweets_tidy_cleaned.\n\nCheck if transformation was successful (e.g.¬†by using the print() function)\n\n\n# Define custom stopwords\nremove_custom_stopwords &lt;- \"&amp;|&lt;|&gt;|http*|t.c|digitaldetox\"\n\n# Create new dataset clean_tidy_tweets\ntweets_tidy_cleaned &lt;- tweets_detox %&gt;% \n  mutate(text = str_remove_all(text, remove_custom_stopwords)) %&gt;% \n  tidytext::unnest_tokens(\"text\", text) %&gt;% \n  filter(\n    !text %in% tidytext::stop_words$word)\n\n# Check\ntweets_tidy_cleaned %&gt;% print()\n\n# A tibble: 476,176 √ó 37\n   tweet_id   user_username text       created_at          in_reply_to_user_id\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;              \n 1 5777201122 pblackshaw    blackberry 2009-11-16 22:03:12 &lt;NA&gt;               \n 2 5777201122 pblackshaw    iphone     2009-11-16 22:03:12 &lt;NA&gt;               \n 3 5777201122 pblackshaw    read       2009-11-16 22:03:12 &lt;NA&gt;               \n 4 5777201122 pblackshaw    pew        2009-11-16 22:03:12 &lt;NA&gt;               \n 5 5777201122 pblackshaw    report     2009-11-16 22:03:12 &lt;NA&gt;               \n 6 5777201122 pblackshaw    teens      2009-11-16 22:03:12 &lt;NA&gt;               \n 7 5777201122 pblackshaw    distracted 2009-11-16 22:03:12 &lt;NA&gt;               \n 8 5777201122 pblackshaw    driving    2009-11-16 22:03:12 &lt;NA&gt;               \n 9 5777201122 pblackshaw    bit.ly     2009-11-16 22:03:12 &lt;NA&gt;               \n10 5777201122 pblackshaw    4abr5p     2009-11-16 22:03:12 &lt;NA&gt;               \n# ‚Ñπ 476,166 more rows\n# ‚Ñπ 32 more variables: author_id &lt;chr&gt;, lang &lt;chr&gt;, possibly_sensitive &lt;lgl&gt;,\n#   conversation_id &lt;chr&gt;, user_created_at &lt;chr&gt;, user_protected &lt;lgl&gt;,\n#   user_name &lt;chr&gt;, user_verified &lt;lgl&gt;, user_description &lt;chr&gt;,\n#   user_location &lt;chr&gt;, user_url &lt;chr&gt;, user_profile_image_url &lt;chr&gt;,\n#   user_pinned_tweet_id &lt;chr&gt;, retweet_count &lt;int&gt;, like_count &lt;int&gt;,\n#   quote_count &lt;int&gt;, user_tweet_count &lt;int&gt;, user_list_count &lt;int&gt;, ‚Ä¶\n\n\n\n\nüìã Exercise 2: Summarize tokens\n\nCreate summarized data\n\nBased on the dataset tweets_tidy_cleaned, summarize the frequency of the individual tokens by using the count()-function on the variable text. Use the argument sort = TRUE to sort the dataset based on descending frequency of the tokens.\nSave this transformation by creating a new dataset with the name tweets_summarized_cleaned.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nOptional: Check results with a wordcloud\n\nBased on the sorted dataset tweets_summarized_cleaned\n\nSelect only the 100 most frequent tokens, using the function top_n()\nCreate a ggplot()-base with label = text and size = n as aes() and\nUse ggwordcloud::geom_text_wordclout() to create the wordcloud.\nUse scale_size_are() to adopt the scaling of the wordcloud.\nUse theme_minimal() for clean visualisation.\n\n\n\n\n# Create summarized data\ntweets_summarized_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized_cleaned %&gt;% \n    print(n = 50)\n\n# A tibble: 87,756 √ó 2\n   text                 n\n   &lt;chr&gt;            &lt;int&gt;\n 1 digital           8521\n 2 detox             6623\n 3 time              6001\n 4 phone             4213\n 5 unplug            4021\n 6 day               3020\n 7 life              2548\n 8 social            2449\n 9 mindfulness       2408\n10 media             2264\n11 week              1848\n12 unplugging        1674\n13 weekend           1651\n14 health            1649\n15 hnology           1644\n16 mentalhealth      1611\n17 smartphone        1610\n18 screen            1557\n19 nature            1421\n20 travel            1411\n21 wellbeing         1397\n22 break             1386\n23 addiction         1355\n24 listen            1342\n25 kids              1339\n26 sleep             1315\n27 wellness          1301\n28 read              1297\n29 tips              1247\n30 retreat           1242\n31 family            1232\n32 discuss           1225\n33 socialmedia       1208\n34 devices           1184\n35 screentime        1183\n36 icphenomenallyu   1181\n37 offline           1072\n38 free              1071\n39 days              1068\n40 world             1068\n41 check             1062\n42 5                 1013\n43 disconnect        1003\n44 feel               980\n45 enjoy              950\n46 switchoff          949\n47 people             941\n48 digitalwellbeing   937\n49 book               932\n50 love               922\n# ‚Ñπ 87,706 more rows\n\n# Check distribution parameters \ntweets_summarized_cleaned %&gt;%\n  datawizard::describe_distribution()\n\nVariable | Mean |    SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-----------------------------------------------------------------------------------------\nn        | 5.43 | 62.69 |   0 | [1.00, 8521.00] |    67.71 |  6936.19 | 87756 |         0\n\n# Optional: Check results with a wordcloud\ntweets_summarized_cleaned %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n\n\n\n\n\n\nüìã Exercise 3: Couting and correlating pairs of words\n\n3.1 Couting word pairs within tweets\n\nCouting word pairs among sections\n\nBased on the dataset tweets_tidy_cleaned, count word pairs using widyr::pairwise_count(), with the arguments item = text, feature = tweet_id and sort = TRUE.\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\n\n\n# Couting word pairs among sections\ntweets_word_pairs_cleand &lt;- tweets_tidy_cleaned %&gt;% \n  widyr::pairwise_count(\n    item = text,\n    feature = tweet_id,\n    sort = TRUE)\n\n# Check \ntweets_word_pairs_cleand %&gt;% print(n = 50)\n\n# A tibble: 2,958,758 √ó 3\n   item1                 item2                     n\n   &lt;chr&gt;                 &lt;chr&gt;                 &lt;dbl&gt;\n 1 detox                 digital                5195\n 2 digital               detox                  5195\n 3 media                 social                 1999\n 4 social                media                  1999\n 5 discuss               listen                 1185\n 6 listen                discuss                1185\n 7 listen                unplugging             1183\n 8 unplugging            listen                 1183\n 9 discuss               unplugging             1181\n10 icphenomenallyu       unplugging             1181\n11 icphenomenallyu       listen                 1181\n12 unplugging            discuss                1181\n13 icphenomenallyu       discuss                1181\n14 unplugging            icphenomenallyu        1181\n15 listen                icphenomenallyu        1181\n16 discuss               icphenomenallyu        1181\n17 time                  digital                 958\n18 digital               time                    958\n19 screen                time                    883\n20 time                  screen                  883\n21 unplug                detox                   799\n22 detox                 unplug                  799\n23 dqqdpucxoe            unplugging              793\n24 dqqdpucxoe            listen                  793\n25 dqqdpucxoe            discuss                 793\n26 dqqdpucxoe            icphenomenallyu         793\n27 unplugging            dqqdpucxoe              793\n28 listen                dqqdpucxoe              793\n29 discuss               dqqdpucxoe              793\n30 icphenomenallyu       dqqdpucxoe              793\n31 unplug                digital                 791\n32 digital               unplug                  791\n33 time                  detox                   789\n34 detox                 time                    789\n35 phonefree             disconnecttoreconnect   682\n36 disconnecttoreconnect phonefree               682\n37 phonefree             switchoff               673\n38 switchoff             phonefree               673\n39 digitalminimalism     phonebreakup            636\n40 phonebreakup          digitalminimalism       636\n41 digitalminimalism     phonefree               633\n42 phonefree             digitalminimalism       633\n43 time                  unplug                  628\n44 unplug                time                    628\n45 disconnecttoreconnect switchoff               617\n46 switchoff             disconnecttoreconnect   617\n47 time                  phone                   616\n48 phone                 time                    616\n49 phonefree             phonebreakup            615\n50 phonebreakup          phonefree               615\n# ‚Ñπ 2,958,708 more rows\n\n\n\n\n3.2 Pairwise correlation\n\nGetting pairwise correlation\n\nBased on the dataset tweets_tidy_cleaned,\n\ngroup the data with the function group_by() by the variable text\nuse filter(n() &gt;= X) to only use tokens that appear at least a certain amount of times (X).\nPlease feel free to select an X of your choice, however, I would strongly recommend an X &gt; 100, as otherwise the following function might not be able to compute.\ncreate word correlations using widyr::pairwise_cor(), with the arguments item = text,feature = tweet_id and sort = TRUE.\n\nSave this transformation by creating a new dataset with the name tweets_pairs_corr_cleaned.\n\nCheck pairs with highest correlation by using the print() function.\n\n\n# Getting pairwise correlation \ntweets_pairs_corr_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  group_by(text) %&gt;% \n  filter(n() &gt;= 250) %&gt;% \n  pairwise_cor(text, tweet_id, sort = TRUE)\n\n# Check pairs with highest correlation\ntweets_pairs_corr_cleaned %&gt;% print(n = 50)\n\n# A tibble: 62,250 √ó 3\n   item1             item2             correlation\n   &lt;chr&gt;             &lt;chr&gt;                   &lt;dbl&gt;\n 1 jocelyn           brewer                  0.999\n 2 brewer            jocelyn                 0.999\n 3 jocelyn           discusses               0.983\n 4 discusses         jocelyn                 0.983\n 5 igjzl0z4b7        locally                 0.982\n 6 locally           igjzl0z4b7              0.982\n 7 discusses         brewer                  0.982\n 8 brewer            discusses               0.982\n 9 icphenomenallyu   discuss                 0.981\n10 discuss           icphenomenallyu         0.981\n11 taniamulry        wealth                  0.979\n12 wealth            taniamulry              0.979\n13 locally           cabins                  0.977\n14 cabins            locally                 0.977\n15 igjzl0z4b7        cabins                  0.970\n16 cabins            igjzl0z4b7              0.970\n17 jocelyn           nutrition               0.968\n18 nutrition         jocelyn                 0.968\n19 nutrition         brewer                  0.967\n20 brewer            nutrition               0.967\n21 discusses         nutrition               0.951\n22 nutrition         discusses               0.951\n23 icphenomenallyu   listen                  0.937\n24 listen            icphenomenallyu         0.937\n25 discuss           listen                  0.923\n26 listen            discuss                 0.923\n27 screenlifebalance mindfulliving           0.920\n28 mindfulliving     screenlifebalance       0.920\n29 limited           locally                 0.919\n30 locally           limited                 0.919\n31 screenlifebalance socialmediafast         0.915\n32 socialmediafast   screenlifebalance       0.915\n33 igjzl0z4b7        limited                 0.913\n34 limited           igjzl0z4b7              0.913\n35 limited           cabins                  0.908\n36 cabins            limited                 0.908\n37 taniamulry        info                    0.905\n38 info              taniamulry              0.905\n39 socialmediafast   digitaladdiction        0.904\n40 digitaladdiction  socialmediafast         0.904\n41 media             social                  0.901\n42 social            media                   0.901\n43 socialmediafast   mindfulliving           0.893\n44 mindfulliving     socialmediafast         0.893\n45 jocelyn           digitalnutrition        0.893\n46 digitalnutrition  jocelyn                 0.893\n47 brewer            digitalnutrition        0.892\n48 digitalnutrition  brewer                  0.892\n49 wealth            info                    0.891\n50 info              wealth                  0.891\n# ‚Ñπ 62,200 more rows\n\n\n\n\n3.3. Optinal: Visualization\n\nCustomize the parameters in the following code chunk:\n\nselected_words, for defining the words you want to get the correlates for\nnumber_of_correlates, to vary the number of correlates shown in the graph.\n\nSet #|eval : true to execute chunk\n\n\n# Define parameters\nselected_words &lt;- c(\"digital\")\nnumber_of_correlates &lt;- 5\n\n# Visualize correlates\ntweets_pairs_corr_cleaned %&gt;% \n  filter(item1 %in% selected_words) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = number_of_correlates) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder(item2, correlation)) %&gt;% \n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\nüìã Exercise 4: Sentiment\n\nApply afinn dictionary to get sentiment\n\nBased on the dataset tweets_tidy_cleaned,\n\nmatch the words from the afinn dictionary with the tokens in the tweets by using the inner_join() function. Within inner_join() , please use the get_sentiments() -function with the dictionary \"afinn\" for y , c(\"text\" = \"word\") for the by and \"many-to-many\" for the relationship argument.\nuse group_by() for grouping the tweets by the variable tweet_id\nand summarize() the sentiment for each tweet, by creating a new variable (within the summarize() function), called sentiment, that is the sum (use sum()) of the sentiment values assigned to the words of the dictionary (variable value ).\n\nSave this transformation by creating a new dataset with the name tweets_sentiment_cleaned.\n\nCheck if transformation was successful by using the print() function.\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nVisualize distribution\n\nBased on the newly created dataset tweets_sentiment_cleaned, create a ggplot with sentiment as aes() and by using geom_histogram().\n\n\n\n# Apply 'afinn' dictionary to get sentiment\ntweets_sentiment_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  inner_join(\n     y = get_sentiments(\"afinn\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  group_by(tweet_id) %&gt;% \n  summarize(sentiment = sum(value))\n\n# Check transformation\ntweets_sentiment_cleaned %&gt;% print()\n\n# A tibble: 23,956 √ó 2\n   tweet_id            sentiment\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 1000009901563838465         6\n 2 1000038819520008193         1\n 3 1000042717492187136        15\n 4 1000043574673715203        -1\n 5 1000075155891281925         2\n 6 1000094334660825088         3\n 7 1000104543395315713        -2\n 8 1000255467434729472        -1\n 9 1000305076286697472         4\n10 1000349838595248128         1\n# ‚Ñπ 23,946 more rows\n\n# Check distribution statistics \ntweets_sentiment_cleaned %&gt;%\n  datawizard::describe_distribution()\n\nVariable  | Mean |   SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-----------------------------------------------------------------------------------------\nsentiment | 1.24 | 2.77 |   4 | [-13.00, 21.00] |     0.40 |     1.66 | 23956 |         0\n\n# Visualize distribution\ntweets_sentiment_cleaned %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram() +\n  theme_pubr()"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html",
    "href": "exercise/local_exercise-03_solution.html",
    "title": "Exercise 03: üî® Working with R",
    "section": "",
    "text": "Download source file: \nOpen this exercise in other interactive and executable environments:"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#background",
    "href": "exercise/local_exercise-03_solution.html#background",
    "title": "Exercise 03: üî® Working with R",
    "section": "Background",
    "text": "Background\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#packages",
    "href": "exercise/local_exercise-03_solution.html#packages",
    "title": "Exercise 03: üî® Working with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegen√ºber der herk√∂mmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPr√§gnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#codechunks-aus-der-sitzung",
    "href": "exercise/local_exercise-03_solution.html#codechunks-aus-der-sitzung",
    "title": "Exercise 03: üî® Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste ‚ÄúRunde‚Äù der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,176 √ó 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and ‚Ä¶         1971 Hal Ash‚Ä¶             52 Bud Cort     man           \n 2 Venus                2006 Roger M‚Ä¶             50 Peter O'Too‚Ä¶ man           \n 3 The Quiet A‚Ä¶         2002 Phillip‚Ä¶             49 Michael Cai‚Ä¶ man           \n 4 The Big Leb‚Ä¶         1998 Joel Co‚Ä¶             45 David Huddl‚Ä¶ man           \n 5 Beginners            2010 Mike Mi‚Ä¶             43 Christopher‚Ä¶ man           \n 6 Poison Ivy           1992 Katt Sh‚Ä¶             42 Tom Skerritt man           \n 7 Whatever Wo‚Ä¶         2009 Woody A‚Ä¶             40 Larry David  man           \n 8 Entrapment           1999 Jon Ami‚Ä¶             39 Sean Connery man           \n 9 Husbands an‚Ä¶         1992 Woody A‚Ä¶             38 Woody Allen  man           \n10 Magnolia             1999 Paul Th‚Ä¶             38 Jason Robar‚Ä¶ man           \n# ‚Ñπ 1,166 more rows\n# ‚Ñπ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale √úberpr√ºfung der Daten\n\n\n\n\n\n\nSind die Daten ‚Äútechnisch korrekt‚Äù?\n\n\n\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n‚úÖ Wie viele eindeutige Werte hat jede Variable?\n‚úÖ Welcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\n‚úÖ Gibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n√úberblick √ºber die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,176\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"‚Ä¶\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992‚Ä¶\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel‚Ä¶\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, ‚Ä¶\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David‚Ä¶\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma‚Ä¶\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192‚Ä¶\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, ‚Ä¶\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", ‚Ä¶\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", ‚Ä¶\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0‚Ä¶\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, ‚Ä¶\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\n√úberpr√ºfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1176      0 2000.78 16.64 0.49 2004\n age_difference numeric age_difference 1176      0   10.46  8.52 0.25    8\n    actor_1_age numeric    actor_1_age 1176      0   39.99 10.92 0.32   39\n    actor_2_age numeric    actor_2_age 1176      0   31.25  8.44 0.25   30\n trimmed          range   iqr  skew\n 2003.68 88 (1935-2023) 15.00 -1.69\n    9.39      52 (0-52) 12.00  1.19\n   39.42     64 (17-81) 15.00  0.54\n   30.42     64 (17-81)  9.25  1.37\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,176 √ó 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ‚Ñπ 1,166 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 12 √ó 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal‚Ä¶\n 2 Oppenheimer                         20         2023 Cillian Mur‚Ä¶ Florence Pu‚Ä¶\n 3 The Northman                        20         2022 Alexander S‚Ä¶ Anya Taylor‚Ä¶\n 4 The Lost City                       16         2022 Channing Ta‚Ä¶ Sandra Bull‚Ä¶\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb‚Ä¶\n 6 Everything Everywhere ‚Ä¶              9         2022 Ke Huy Quan  Michelle Ye‚Ä¶\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co‚Ä¶\n 8 Oppenheimer                          7         2023 Cillian Mur‚Ä¶ Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc‚Ä¶ Zo√´ Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi‚Ä¶ Reese Withe‚Ä¶\n11 Your Place or Mine                   2         2023 Ashton Kutc‚Ä¶ Reese Withe‚Ä¶\n12 You People                           1         2023 Jonah Hill   Lauren Lond‚Ä¶\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\n√úberpr√ºfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1174) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.56 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1176\n\n\n\n\nSch√§tzung OLS\n\n# Sch√§tzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1174) |      p\n------------------------------------------------------------------------\n(Intercept)  |      231.15 | 29.20 | [173.86, 288.44] |    7.92 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.56 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8326.022 | 8326.043 | 8341.232 | 0.046 |     0.046 | 8.319 | 8.326\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1174) = 57.12, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 231.15 (95% CI [173.86, 288.44], t(1174) = 7.92, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1174) = -7.56, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#exercise-welche-rolle-spielt-das-geschlecht",
    "href": "exercise/local_exercise-03_solution.html#exercise-welche-rolle-spielt-das-geschlecht",
    "title": "Exercise 03: üî® Working with R",
    "section": "üìã Exercise: Welche Rolle spielt das Geschlecht?",
    "text": "üìã Exercise: Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun erg√§nzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die ‚ÄúG√ºltigkeit‚Äù der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und √úberarbeitungsschritte notwendig\n\n\n\n\n√úbepr√ºfung der _gender-Variablen\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\nage_gaps_correct %&gt;% \n  frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1176 valid N=1176 mean=1.01 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1160 | 98.64 |   98.64 |  98.64\nwoman |   16 |  1.36 |    1.36 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1176 valid N=1176 mean=1.99 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   17 |  1.45 |    1.45 |   1.45\nwoman | 1159 | 98.55 |   98.55 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1148\nwoman                            5    11\n\n\n\n\nSind die Daten ‚Äúkonsistent‚Äù?\n\n√úberpr√ºfung der Sortierung\n\nNutzen Sie die Funktion dplyr::summarise um auf Basis des Datensatzes age_gaps_correct folgende Variabeln zu erstellen:\np1_older\n\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 √ó 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.811   0.986             0.5\n\n\n\n\n\n√úberpr√ºfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=843 valid N=843 mean=1.40 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 610 | 72.36 |   72.36 |  72.36\n    2 | 160 | 18.98 |   18.98 |  91.34\n    3 |  54 |  6.41 |    6.41 |  97.75\n    4 |  14 |  1.66 |    1.66 |  99.41\n    5 |   3 |  0.36 |    0.36 |  99.76\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 √ó 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#die-zweite-datenexploration",
    "href": "exercise/local_exercise-03_solution.html#die-zweite-datenexploration",
    "title": "Exercise 03: üî® Working with R",
    "section": "Die zweite Datenexploration",
    "text": "Die zweite Datenexploration\n\nAlterskombinationen im √úberblick\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1176 valid N=1176 mean=3.77 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.94 |    0.94 |   0.94\n    2 |  12 |  1.02 |    1.02 |   1.96\n    3 | 209 | 17.77 |   17.77 |  19.73\n    4 | 944 | 80.27 |   80.27 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1176 valid N=1153 mean=0.82 sd=0.39\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 209 | 17.77 |   18.13 |  18.13\n    1 | 944 | 80.27 |   81.87 | 100.00\n &lt;NA&gt; |  23 |  1.96 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nWie sind die Altersunterschiede unterteilt, unter Ber√ºcksichtiung des Geschlechts?\n\nGraphische √úberpr√ºfung\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero f√ºr das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\n√úberpr√ºfung durch Modellierung\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters(standardize=\"basic\")\n\nParameter             | Std. Coef. |   SE |         95% CI | t(1150) |      p\n-----------------------------------------------------------------------------\n(Intercept)           |       0.00 | 0.00 | [ 0.00,  0.00] |    7.14 | &lt; .001\nrelease year          |      -0.19 | 0.03 | [-0.25, -0.14] |   -6.96 | &lt; .001\nolder male hetero [1] |       0.27 | 0.03 | [ 0.22,  0.33] |    9.87 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8050.434 | 8050.469 | 8070.635 | 0.125 |     0.124 | 7.914 | 7.925"
  },
  {
    "objectID": "exercise/exercise-03.html",
    "href": "exercise/exercise-03.html",
    "title": "Exercise 03: üî® Working with R",
    "section": "",
    "text": "Download source file\n Open this exercise in interactive and executable environment"
  },
  {
    "objectID": "exercise/exercise-03.html#background",
    "href": "exercise/exercise-03.html#background",
    "title": "Exercise 03: üî® Working with R",
    "section": "Background",
    "text": "Background\n\nThe best way to learn R is by trying. This document tries to display a version of the ‚Äúnormal‚Äù data processing procedure.\nUse tidytuesday data as an example to showcase the potential\n\n\n\n\n\n\n\nTodays‚Äôs data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "exercise/exercise-03.html#packages",
    "href": "exercise/exercise-03.html#packages",
    "title": "Exercise 03: üî® Working with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegen√ºber der herk√∂mmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPr√§gnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "exercise/exercise-03.html#codechunks-aus-der-sitzung",
    "href": "exercise/exercise-03.html#codechunks-aus-der-sitzung",
    "title": "Exercise 03: üî® Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste ‚ÄúRunde‚Äù der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,177 √ó 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and ‚Ä¶         1971 Hal Ash‚Ä¶             52 Bud Cort     man           \n 2 Venus                2006 Roger M‚Ä¶             50 Peter O'Too‚Ä¶ man           \n 3 The Quiet A‚Ä¶         2002 Phillip‚Ä¶             49 Michael Cai‚Ä¶ man           \n 4 The Big Leb‚Ä¶         1998 Joel Co‚Ä¶             45 David Huddl‚Ä¶ man           \n 5 Beginners            2010 Mike Mi‚Ä¶             43 Christopher‚Ä¶ man           \n 6 Poison Ivy           1992 Katt Sh‚Ä¶             42 Tom Skerritt man           \n 7 Whatever Wo‚Ä¶         2009 Woody A‚Ä¶             40 Larry David  man           \n 8 Entrapment           1999 Jon Ami‚Ä¶             39 Sean Connery man           \n 9 Husbands an‚Ä¶         1992 Woody A‚Ä¶             38 Woody Allen  man           \n10 Magnolia             1999 Paul Th‚Ä¶             38 Jason Robar‚Ä¶ man           \n# ‚Ñπ 1,167 more rows\n# ‚Ñπ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale √úberpr√ºfung der Daten\n\n\n\n\n\n\nSind die Daten ‚Äútechnisch korrekt‚Äù?\n\n\n\n\n‚úÖ Wie viele F√§lle sind enthalten? Wie viele Variablen?\n‚úÖ Wie lauten die Variablennamen? Sind sie sinnvoll?\n‚úÖ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n‚úÖ Wie viele eindeutige Werte hat jede Variable?\n‚úÖ Welcher Wert tritt am h√§ufigsten auf, und wie oft kommt er vor?\n‚úÖ Gibt es fehlende Werte? Wenn ja, wie h√§ufig ist dies der Fall?\n\n\n\n\n√úberblick √ºber die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,177\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"‚Ä¶\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992‚Ä¶\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel‚Ä¶\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, ‚Ä¶\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David‚Ä¶\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma‚Ä¶\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192‚Ä¶\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, ‚Ä¶\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", ‚Ä¶\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", ‚Ä¶\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0‚Ä¶\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, ‚Ä¶\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\n√úberpr√ºfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1177      0 2000.74 16.67 0.49 2004\n age_difference numeric age_difference 1177      0   10.48  8.53 0.25    8\n    actor_1_age numeric    actor_1_age 1177      0   39.97 10.90 0.32   39\n    actor_2_age numeric    actor_2_age 1177      0   31.27  8.50 0.25   30\n trimmed          range iqr  skew\n 2003.65 88 (1935-2023)  15 -1.68\n    9.41      52 (0-52)  12  1.19\n   39.41     64 (17-81)  15  0.53\n   30.42     64 (17-81)   9  1.39\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am h√∂chsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,177 √ó 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ‚Ñπ 1,167 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 12 √ó 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal‚Ä¶\n 2 Oppenheimer                         20         2023 Cillian Mur‚Ä¶ Florence Pu‚Ä¶\n 3 The Northman                        20         2022 Alexander S‚Ä¶ Anya Taylor‚Ä¶\n 4 The Lost City                       16         2022 Channing Ta‚Ä¶ Sandra Bull‚Ä¶\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb‚Ä¶\n 6 Everything Everywhere ‚Ä¶              9         2022 Ke Huy Quan  Michelle Ye‚Ä¶\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co‚Ä¶\n 8 Oppenheimer                          7         2023 Cillian Mur‚Ä¶ Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc‚Ä¶ Zo√´ Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi‚Ä¶ Reese Withe‚Ä¶\n11 Your Place or Mine                   2         2023 Ashton Kutc‚Ä¶ Reese Withe‚Ä¶\n12 You People                           1         2023 Jonah Hill   Lauren Lond‚Ä¶\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\n√úberpr√ºfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1175) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.68 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1177\n\n\n\n\nSch√§tzung OLS\n\n# Sch√§tzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1175) |      p\n------------------------------------------------------------------------\n(Intercept)  |      234.30 | 29.15 | [177.11, 291.50] |    8.04 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.68 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8334.623 | 8334.643 | 8349.835 | 0.048 |     0.047 | 8.324 | 8.331\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1175) = 58.96, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 234.30 (95% CI [177.11, 291.50], t(1175) = 8.04, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1175) = -7.68, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercise/exercise-03.html#exercise-welche-rolle-spielt-das-geschlecht",
    "href": "exercise/exercise-03.html#exercise-welche-rolle-spielt-das-geschlecht",
    "title": "Exercise 03: üî® Working with R",
    "section": "üìã Exercise: Welche Rolle spielt das Geschlecht?",
    "text": "üìã Exercise: Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun erg√§nzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die ‚ÄúG√ºltigkeit‚Äù der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und √úberarbeitungsschritte notwendig\n\n\n\n\n√úbepr√ºfung der _gender-Variablen\n\n\n\n\n\n\nExercise 1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\nage_gaps_correct %&gt;% \n  frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1177 valid N=1177 mean=1.01 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1162 | 98.73 |   98.73 |  98.73\nwoman |   15 |  1.27 |    1.27 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1177 valid N=1177 mean=1.99 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   16 |  1.36 |    1.36 |   1.36\nwoman | 1161 | 98.64 |   98.64 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1150\nwoman                            4    11\n\n\n\n\nSind die Daten ‚Äúkonsistent‚Äù?\n\n√úberpr√ºfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 √ó 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.811   0.987           0.499\n\n\n\n\n\n√úberpr√ºfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=844 valid N=844 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 611 | 72.39 |   72.39 |  72.39\n    2 | 160 | 18.96 |   18.96 |  91.35\n    3 |  54 |  6.40 |    6.40 |  97.75\n    4 |  14 |  1.66 |    1.66 |  99.41\n    5 |   3 |  0.36 |    0.36 |  99.76\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 √ó 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )"
  },
  {
    "objectID": "exercise/exercise-03.html#die-zweite-datenexploration",
    "href": "exercise/exercise-03.html#die-zweite-datenexploration",
    "title": "Exercise 03: üî® Working with R",
    "section": "Die zweite Datenexploration",
    "text": "Die zweite Datenexploration\n\nAlterskombinationen im √úberblick\n\n\n\n\n\n\nExercise 3\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1177 valid N=1177 mean=3.77 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.93 |    0.93 |   0.93\n    2 |  12 |  1.02 |    1.02 |   1.95\n    3 | 209 | 17.76 |   17.76 |  19.71\n    4 | 945 | 80.29 |   80.29 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1177 valid N=1154 mean=0.82 sd=0.39\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 209 | 17.76 |   18.11 |  18.11\n    1 | 945 | 80.29 |   81.89 | 100.00\n &lt;NA&gt; |  23 |  1.95 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nWie sind die Altersunterschiede unterteilt, unter Ber√ºcksichtiung des Geschlechts?\n\nGraphische √úberpr√ºfung\n\n\n\n\n\n\nExercise 4\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero f√ºr das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\n√úberpr√ºfung durch Modellierung\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter             | Coefficient |    SE |           95% CI | t(1151) |      p\n---------------------------------------------------------------------------------\n(Intercept)           |      204.26 | 28.14 | [149.04, 259.47] |    7.26 | &lt; .001\nrelease year          |       -0.10 |  0.01 | [ -0.13,  -0.07] |   -7.08 | &lt; .001\nolder male hetero [1] |        6.03 |  0.61 | [  4.83,   7.23] |    9.87 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8058.980 | 8059.015 | 8079.184 | 0.126 |     0.125 | 7.920 | 7.930\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year and older_male_hetero (formula: age_difference ~ release_year +\nolder_male_hetero). The model explains a statistically significant and weak\nproportion of variance (R2 = 0.13, F(2, 1151) = 83.25, p &lt; .001, adj. R2 =\n0.12). The model's intercept, corresponding to release_year = 0 and\nolder_male_hetero = 0, is at 204.26 (95% CI [149.04, 259.47], t(1151) = 7.26, p\n&lt; .001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.10, 95% CI [-0.13, -0.07], t(1151) = -7.08, p &lt; .001; Std. beta = -0.20, 95%\nCI [-0.25, -0.14])\n  - The effect of older male hetero [1] is statistically significant and positive\n(beta = 6.03, 95% CI [4.83, 7.23], t(1151) = 9.87, p &lt; .001; Std. beta = 0.71,\n95% CI [0.57, 0.85])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven‚Äôt covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "course-assignments.html",
    "href": "course-assignments.html",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g.¬†different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (üë•) or individual (üë§) work and are marked as such. The expected group size is 2-3 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nTotal\n100 Pts\n\n\n\n\nüë• Presentation\n20 Pts\n\n\nüë• Project topic idea(s)\n10 Pts\n\n\nüë• Project proposal (draft report)\n15 Pts\n\n\nüë§ Peer Review\n15 Pts\n\n\nüë• Written short report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g.¬†which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g.¬†the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).\nIf the final structure of the course contains several small projects, the listed assignments only have to be completed for one of the projects."
  },
  {
    "objectID": "course-assignments.html#sec-portfolio",
    "href": "course-assignments.html#sec-portfolio",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g.¬†different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (üë•) or individual (üë§) work and are marked as such. The expected group size is 2-3 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nTotal\n100 Pts\n\n\n\n\nüë• Presentation\n20 Pts\n\n\nüë• Project topic idea(s)\n10 Pts\n\n\nüë• Project proposal (draft report)\n15 Pts\n\n\nüë§ Peer Review\n15 Pts\n\n\nüë• Written short report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g.¬†which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g.¬†the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).\nIf the final structure of the course contains several small projects, the listed assignments only have to be completed for one of the projects."
  },
  {
    "objectID": "course-assignments.html#sec-presentation",
    "href": "course-assignments.html#sec-presentation",
    "title": "Assingments",
    "section": "üë• Presentation (20 Pts)",
    "text": "üë• Presentation (20 Pts)\nThe topics for the presentation will be assigned at the beginning of the course. The focus of the presentation is either on presenting the theoretical basis of the project (üìö) or the data source/collection (üì¶). The presentation should be 20 to 30 minutes long (including time for questions and discussion). You will be graded based on your individual contribution, so please clarify which slides are presented by whom (e.g., by adding the initials of the presenting individual in the footnote of the slide).\nLiterature for the preparation of the presentation will be provided. The texts relevant for the respective presentation can be found in the information on the preparation (üìñ) of the respective session. All texts listed in the section ‚ÄúMandatory literature‚Äù constitute the presentation literature for your respective presentation. Please include all texts on your topic in your presentation, but feel free to set your own priorities in the presentation. You may also cite other sources, provided they enrich the subject matter.\nThe aim of the presentation is to give the course participants an overview on your topic, e.g., central terms, definitions and features of the respective platform, method and/or tool. The presentation of the state of research (what is the goal of the studies and what do they show?) plays a subordinate role.\nAdditionally, presenters are required to meet with the instructor in the week before their presentation for a mandatory feedback. My office hours are directly after the session, on Wednesdays from 13:15 to 14:15. If you have scheduling conflicts, we can arrange another meeting time. Meetings can take place in person or via Zoom.\nIn advance of the feedback meeting, a first complete draft of the presentation must be submitted as a PowerPoint or PDF file via mail at the latest 24 hours before the meeting, one week before the presentation. During the feedback meeting, students will receive detailed feedback and tips on how to revise their presentation. The revised presentations are then given in presence in the respective sessions. Afterwards, a PDF of the slides is made available to the seminar.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nAt the latest 24 hours before the pre-meeting: Send the first complete draft of the presentation slides by e-mail to christoph.adrian@fau.de\nOne week before your presentation: Pre-meeting to discuss the presentation draft during the office hours. Please arrange an alternative date in good time if you are unable to attend the scheduled pre-meeting.\nUntil 09:00 of the day of the presentation: Send the final draft of the presentation by e-mail to christoph.adrian@fau.de"
  },
  {
    "objectID": "course-assignments.html#sec-topic-ideas",
    "href": "course-assignments.html#sec-topic-ideas",
    "title": "Assingments",
    "section": "üë• Project topic idea(s) (10 Pts)",
    "text": "üë• Project topic idea(s) (10 Pts)\nShort or ad-hoc presentation (‚Äúpitch‚Äù) of the status or result of your groups project work. The pitch should be about 5 to 10 minutes long, consist of max. 5 slides and be held by one group member only.\nThe aim of the pitch is to give the course and update about your projects progress as well as a basis for discussion and feedback. Therefore, you should\n\nfirst give a short overview of your topic, research question or motivation and selected data (sub-)sample (2 slides),\ndescribe your method and analysis (1 slide) and\nshow results and/or a challenge you face, that should be discussed by the course (2 slide).\n\nAfter the pitch there will be time for questions, either from the group to the course or vice versa.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nPrepare a short presentation before and present you pitch in every ‚ÄúPresentation & Discussion (üìä)‚Äù session, using the Google Slides templates provided."
  },
  {
    "objectID": "course-assignments.html#sec-proposal",
    "href": "course-assignments.html#sec-proposal",
    "title": "Assingments",
    "section": "üë• Project proposal (15 pts)",
    "text": "üë• Project proposal (15 pts)\nThe project proposal is basically the frist written draft of your final short report, consisting of the write-up of your project topic idea and/or associated feedback. The idea is for you to think about the specific analysis strategy early on, as well as present initial results or evaluation. The project proposal is the basis for the peer review.\nThe project proposal should be at least 500 words and include the following points:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you‚Äôre investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore and/or your hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data (sub-)sample you selected. This includes\n\ndescription of the ‚Äúobservations‚Äù in the data set,\ndescription of how the data was ‚Äúoriginally‚Äù collected.\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the central method as well as necessary pre-processing steps of the data.\nDescription, visualization and/or summary statistics of the central variables.\n(Intended) analysis strategy with which you want to answer the research question/hypothesis (with initial results, if available)\n\nThe grading of your proposal will be as followd:\n\nProject proposal grading breakdown\n\n\nTotal\n15 pts\n\n\n\n\nIntroduction\n4 pts\n\n\nData description\n3 pts\n\n\nAnalysis plan\n7 pts\n\n\nOrganization + formatting\n1 pts\n\n\n\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nSubmit the draft project proposal using the provided Google Docs template no later than two weeks after the associated ‚ÄúPresentation & Discussion‚Äù session."
  },
  {
    "objectID": "course-assignments.html#sec-peer-review",
    "href": "course-assignments.html#sec-peer-review",
    "title": "Assingments",
    "section": "üë§ Peer Review (15 pts)",
    "text": "üë§ Peer Review (15 pts)\nCritically reviewing others‚Äô work is a crucial part of the scientific process. Therefore, each group will be assigned one other group‚Äôs project proposal to review before the final submission. This way, you will be able to get additional feedback before handing in your final report.\nDuring the peer review process, you will be provided read-only access to your partner groups‚Äô report via Google Docs. Provide your review in the form of another Google Doc using the template provided.\n\nProcess and questions\n\nOpen the Google Doc of the team you‚Äôre reviewing and read their project draft.\nThen, go to Peer Review Google Doc template, fill out the relevant fields and answer the following questions:\n\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the group might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?\n\nThe peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team‚Äôs report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nProvide feedback using the provided Google Docs template until two weeks after you received the document for peer-reviewing."
  },
  {
    "objectID": "course-assignments.html#sec-written-report",
    "href": "course-assignments.html#sec-written-report",
    "title": "Assingments",
    "section": "üë• Written short report (40 pts)",
    "text": "üë• Written short report (40 pts)\nThe goal of the written short report is for each group to use at least one of the method or data presented to explore a topic of your own choosing. Choose both data and topic based on your group‚Äôs interests, experience or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis. You are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the word limit.\nThe written report should be 750 to 1000 words per person. However, when written as a group report, the number or words scale with a factor of 0.8 per person (e.g., a group of two should write 1200 to 1600 words, a group of three 1800 to 2400 words). All analyses as well as the written report must be done in RStudio and all components of the project must be reproducible. The mandatory components of the reports are: Introduction, Data/Methodology, Results, Discussion & Conclusion. You are free to add additional sections as necessary. You will be graded based on your individual contribution, so please clarify which part of the report was written by whom (e.g., by adding the initials of the author in the header of the section).\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nFinal submission of the revised written report is due 01.03.2024, 23:591."
  },
  {
    "objectID": "course-assignments.html#footnotes",
    "href": "course-assignments.html#footnotes",
    "title": "Assingments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that this is a temporary deadline. The final deadline will be adjusted in the course in consultation with the students if necessary.‚Ü©Ô∏é"
  }
]