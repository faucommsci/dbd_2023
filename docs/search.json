[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nTime\nLocation (Room)\n\n\n\n\n25.10.2023\nWednesdays, 11:30 - 13:00\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nDepending on the students’ feedback in the survey before the course starts, the course can be held in (incl. presentations, pitches, discussions, etc.) English or German.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer."
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "First session\nTime\nLocation (Room)\n\n\n\n\n25.10.2023\nWednesdays, 11:30 - 13:00\nFindelgasse, 2.024\n\n\n\n\n\n\n\n\n\nAssignment and course language\n\n\n\n\nDepending on the students’ feedback in the survey before the course starts, the course can be held in (incl. presentations, pitches, discussions, etc.) English or German.\nRegardless of the feedback on the course language, the written assignments can be in German or English, depending on what the student/groups prefer."
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nIn this seminar, students are introduced to working with digital behavioral data (DBD). DBD refer to digital traces of human behavior that are knowingly or unknowingly left in online environments (e.g. social media, messengers, entertainment media, or digital collaboration tools). These rich data is increasingly available to social scientific research in the public interest, but can also be used to derive strategic insights for business decisions.\nStudents learn how to work with DBD alongside the entire research process, from data collection, preprocessing and analysis, to reporting and provision (e.g. via open science tools). Students first get a comprehensive overview of the ways in which DBD can be collected (e.g., API scraping, usage logging, mock-up virtual environments, or data donations), as well as the requirements for data protection, research ethics, and data quality. Afterwards, students practice and apply their newly acquired knowledge in small projects on use cases from media and communication research. In doing so, they learn important computer-based methods with which large digital behavioral data sets (e.g. texts, images, usage behavior logs) can be processed and analyzed. By completing this module, participants will get an up-to-date overview and practical insights into how the potential of observational data (digital traces) can be used to better understand the behavior of media users in digital environments."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will\n\noverview and understand central opportunities of DBD and accompanying challenges for data collection and preprocessing\nevaluate the strengths and weaknesses of different ways of collecting DBD\nget to know and understand central requirements for data protection, research ethics, and data quality\nget to know and overview key computational social science methods to analyze DBD\npractice and apply knowledge on DBD, statistics, and data analysis in small projects of their own"
  },
  {
    "objectID": "course-syllabus.html#recommended-prerequisites",
    "href": "course-syllabus.html#recommended-prerequisites",
    "title": "Syllabus",
    "section": "Recommended prerequisites",
    "text": "Recommended prerequisites\n\nInterest in social scientific perspectives on media, communication, and digital technologies.\nBasic knowledge of working with statistical software such as Stata, R, Python, or SPSS is required.\nStudents are recommended, but not required, to also visit the lecture Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing."
  },
  {
    "objectID": "course-syllabus.html#organization-of-the-course",
    "href": "course-syllabus.html#organization-of-the-course",
    "title": "Syllabus",
    "section": "Organization of the course",
    "text": "Organization of the course\nRegistration for the course takes place via  StudOn. There you will receive the first information and instructions. Please make sure that\n\nyou complete the short survey before the seminar begins.\nyou check if you have received the invitation and joined  Zulip.\n\nAll slides, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website. I will regularly send out course announcements by e-mail or Zulip, make sure to check one or the other of these regularly."
  },
  {
    "objectID": "course-syllabus.html#preliminary-schedule",
    "href": "course-syllabus.html#preliminary-schedule",
    "title": "Syllabus",
    "section": "(Preliminary) Schedule",
    "text": "(Preliminary) Schedule\n\n\n\n\n\n\nImportant information\n\n\n\n\nPlease note that this is a provisional schedule. Part of the Kick-Off session is the presentation, discussion and voting on different project ideas.\nAll sessions marked with a 🔨 are hands-on sessions actively working with R.\nAll sessions marked with a 📚 or 📦 are presentation sessions where a group of students will give a detailed presentation.\n\n\n\n\n\n\nSession\nDatum\nTopic\n\n\n\n\n\n\nIntroduction\n\n\n1\n25.10.2023\nKick-Off\n\n\n-\n01.11.2023\n🎃 Holiday (No Lecture)\n\n\n2\n08.11.2023\nDBD: Overview\n\n\n3\n15.11.2023\n🔨 Working with R\n\n\n\n📂 Project 1\nAnalysis of media content\n\n\n4\n22.11.2023\n📚 Digital disconnection\n\n\n5\n29.11.2023\n📦 Data collection methods\n\n\n6\n06.12.2023\n🔨 Text as data\n\n\n7\n13.12.2023\n📊 Presentation & Discussion\n\n\n8\n20.12.2023\nBuffer Session\n\n\n-\n-\n🎄Christmas Break (No Lecture)\n\n\n\n📂 Project 2\nAnalysis of media usage\n\n\n9\n11.01.2024\n📚 Media habits & routines\n\n\n10\n18.01.2024\n📦 Data donations methods\n\n\n11\n25.01.2024\n🔨 Working data logs\n\n\n12\n02.02.2024\n📊 Presentation & Discussion\n\n\n13\n08.02.2024\n🏁 Recap, Evaluation & Discussion\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor an even more detailed overview of the course schedule as well as the linked content of the individual sessions (e.g. slides or literature for the respective presentation), please see Schedule.\n\n\nThe course consists of three main parts:\n\nPart I: Introduction\nThe first three sessions form the (theoretical) basis for the course.\n\nThe kick-off session is mainly for getting to know each other and organizing the course. \nThe second session is to give you an extended introduction DBD, including challenges and important frameworks. \nThe third session is about practical work with R and RStudio.  \n\n\n\nPart II: Analysis of media content (Project 1)\n\nThe focus of the second section is the observation and analysis of (written) social media content (e.g. post and/or hashtags) with the help of different methods (e.g. topic modeling, sentiment analysis etc.).\n\n\n\nPart III: Analysis of media usage (Project 2)\n\nThe third part is about the effects of media use. Here you will learn how to process and analyze logging data and how to link it to survey data with the help of a survey conducted on the participants of the course during the first months of the semester."
  },
  {
    "objectID": "course-syllabus.html#sessions",
    "href": "course-syllabus.html#sessions",
    "title": "Syllabus",
    "section": "Sessions",
    "text": "Sessions\nThe goal of the sessions is to be as interactive as possible. In general, the sessions consist of two parts. In the first part (± 30 - 45 minutes) at the beginning of the session, there are usually presentations (including discussion), which are more or less detailed depending on the stage of the project. The second part (± 45 - 60 minutes) consists of a group activity (with concluding discussion), which should either be about deepening the presentation content or about independent work on one’s own or the group project.\nMy role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. You are expected to bring a laptop to each class so that you can take part in the in-session exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone.\n\nWhere to ask questions\n\nIf you have a question during the lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nAny general questions about session content, assignments or about the project should be posted on Zulip, so that everyone can benefit from the answers. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question, I encourage you to respond!\nE-mails should be reserved for personal matters."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nIn order to obtain credits and a grade, participants are required to\n\nattend regularly (at least 80% of the sessions) and participate actively. A maximum of two sessions can be missed without excuse. Absence in further sessions can only be excused in case of illness (i. e. with a medical certificate).\ncomplete various assignments as part of a portfolio. The type and scope of the assignments depends on the number of participants and the project(s). Detailed information can be found in the section Assignments."
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\n\n\n\n\n\n\nTL;DR\n\n\n\nDo not cheat!\n\n\nFor general information on formatting, style, citation, appendices, wording of the affidavit, etc., see our Guide to Academic Writing.\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\nPolicy on use of generative artificial intelligence (AI):\nYou should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course1: (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content."
  },
  {
    "objectID": "course-syllabus.html#recommended-textbooks",
    "href": "course-syllabus.html#recommended-textbooks",
    "title": "Syllabus",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of Computational Social Science, Volume 1: Theory, Case Studies and Ethics (1st ed.). Routledge. https://doi.org/10.4324/9781003024583\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Handbook of computational social science, volume 2. Routledge. https://doi.org/10.4324/9781003025245\nHaim, M. (2023). Computational Communication Science: Eine Einführung. Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-40171-9\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press."
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D↩︎"
  },
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\n🔗 R for Data Science, 2nd Edition\n🔗 Data Visualization: A Practical Introduction\n🔗 Tidy modeling with R\n🔗 Text Mining with R"
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann"
  },
  {
    "objectID": "exercise/exercise-10.html",
    "href": "exercise/exercise-10.html",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-10.html#background",
    "href": "exercise/exercise-10.html#background",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-10.html#preparation",
    "href": "exercise/exercise-10.html#preparation",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-10.html#import-and-process-the-data",
    "href": "exercise/exercise-10.html#import-and-process-the-data",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\nGet twitter data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nDTM/DFM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  mutate(across(text, ~str_replace_all(., \"#digitaldetox\", \"\"))) %&gt;% \n  select(-c(detox_dy, retweet_dy)) %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\n\n\nGet topic model (data)\n\n# TPM data\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))\n\n# Base data with topics \ntweets_detox_topics &lt;- qs::qread(here(\"local_data/tweets-digital-detox-topics.qs\"))"
  },
  {
    "objectID": "exercise/exercise-10.html#exercises",
    "href": "exercise/exercise-10.html#exercises",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "📋 Exercises",
    "text": "📋 Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the “Run all chunks above”-button  of the next chunk.\nYou can choose to solve some exercises using either the tidytext or quanteda functions. As the work steps differ, please select the tab with your preferred package.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\n📋 Exercise 1: Hashtag co-occurence\n\n\n\n\n\n\nObjective(s)\n\n\n\nRecreate the network plot from the session without the token #digitaldetox\n\n\n\nwith tidytextwith quanteda\n\n\n\nCreate new dataset tweets_dfm_cleaned\n\nBased on the dataset tweets_detox,\n\nUse mutate() and create the variables\n\ntext that removes the hashtag \"#digitaldetox\" using str_remove_all() and\nhashtag that extracts hashtags from the variable text with the help of str_extract_all() (and the pattern \"#\\\\S+\").\n\nTokenize the data by using only unnest() on the variable hashtag.\nSummarize occurrences using count(tweet_id, hashtags).\nConvert to DFM using cast_dfm(tweet_id, hashtags, n).\n\nSave this transformation by creating a new dataset with the name tweets_dfm_cleaned.\n\nCreate new dataset top_hashtags_tidy\n\nBased on the dataset tweets_detox,\n\nRepeat steps 1. & 2. from before.\nSummarize occurrences using count(hashtags, sort = TRUE).\nExtract top 50 hashtags by using slice_head(n = 50).\nConvert to string vector by using pull() .\n\nSave this transformation by creating a new dataset with the name top_hashtags_tidy.\n\nVisualize co-occurence\n\nBased on the dataset tweets_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network()\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n\n\nCreate new dataset quanteda_dfm_cleaned:\n\nBased on the dataset quanteda_dfm,\n\nUse quanteda::dfm_select(pattern = \"#*\") to create a DFM containing only hashtags.\nUse quanteda::dfm_remove(pattern = \"#digitaldetox\") to removes the hashtag \"#digitaldetox\".\n\nSave this transformation by creating a new dataset with the name quanteda_dfm_cleaned.\n\nCreate new dataset top_hashtags_quanteda\n\nBased on the dataset quanteda_dfm_cleaned,\n\nUse topfeatures(50) o extract the top 50 most common hashtags.\nUse names() to only store the names (not the values).\n\nSave this transformation by creating a new dataset with the name top_hashtags_quanteda.\n\nVisualize co-occurence\n\nBased on the dataset quanteda_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network() .\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n\n\n\n# Create new dataset tweets_dfm_cleaned | quanteda_dfm_cleaned\n\n# Create new dataset top_hashtags_quanteda |top_hashtags_quanteda\n\n# Visualize co-occurence\n\n\n\n📋 Exercise 2: Understanding Topic 9\n\n\n\n\n\n\nObjective(s)\n\n\n\nTake a closer look at another topic from the topic model used in the session by examining the most representative tweets and the users who post the most tweets on that topic.\n\n\n\n2.1: Explore top tweets\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nArrange the selected tweets in descending order based on top_gamma values using arrange(-top_gamma).\nExtract the top 10 tweets using slice_head(n = 10).\nSelect only relevant columns (tweet_id, user_username, created_at, text, top_gamma) using select().\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\n\n2.1 Explore top users\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nCount the number of tweets per user using count(user_username, sort = TRUE).\nCalculate the proportion of tweets for each user by creating a new column prop using mutate(prop = round(n/sum(n)*100, 2)).\nExtract the top 15 users with the highest engagement using slice_head(n = 15).\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\n\n\n📋 Exercise 3: Expolore different topic model\n\n\n\n\n\n\nObjective(s)\n\n\n\nIn the session, three models were considered for closer examination. Choose one of the other topics and recreate all the steps for the initial exploration of the topic model (as in the session).\n\n\n\n3.1 Initial exploration\n\nExplore a different topic model (estimation)\n\nBased on the dataset stm_results,\n\nUse the filter(k == X) to select a different topic model (estimation) of your choice (by defining X).\nTip: Use stm_results$k to see available options for X\nUse pull(mdl) %&gt;% .[[1]] to select the topic model (estimation) with the specified number of topics.\n\nSave this transformation by creating a new dataset with the name tpm_new.\n\nVisual exploration of tpm_new\n\nVisualize the summary of the selected topic model using plot(type = \"summary\") .\n\n\n\n# Explore a different topic model (estimation)\ntpm_new &lt;- stm_results |&gt;\n   filter(k == ) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\n\n\n\n3.2 Document/Word-topic relations\n\nCalculate mean gamma values\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"gamma\") on tpm_new to get the gamma values.\nCalculate the mean gamma values for each topic using group_by() and summarise().\nUse arrange() to sort topics (descending) by gamma\n\nSave this transformation by creating a new dataset with the name top_gamma_new.\n\nIdentify top terms of each topic\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"beta\") on tpm_new to get the beta values.\nArrange terms within each topic in descending order based on beta values using group_by(), arrange(-beta), and top_n(10, wt = beta).\nSelect relevant columns (topic, term) using select().\nSummarize the top 10 terms for each topic using summarise(terms_beta = toString(term), .groups = \"drop\").\n\nSave this transformation by creating a new dataset with the name top_beta_new.\n\nCombine top topics and top terms\n\nJoin the dataframes top_gamma_new and top_beta_new based on the “topic” column using left_join().\nWithin mutate(), - Adjusted topic names by topic = paste0(\"Topic \", topic) and - Reorder the dataset with topic = reorder(topic, gamma)\nSave this transformation by creating a new dataset with the name top_topics_terms_new.\n\nPreview the results\n\nDisplay a table preview of the top topics and terms with rounded gamma values using gt()\n\n\n\n# Calculate mean gamma values\n\n# Identify top terms of each topic\n\n# Combine top topics and top terms\n\n# Preview the esults"
  },
  {
    "objectID": "exercise/showcase-10.html",
    "href": "exercise/showcase-10.html",
    "title": "Showcase 10: 🔨 Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides"
  },
  {
    "objectID": "exercise/showcase-10.html#background",
    "href": "exercise/showcase-10.html#background",
    "title": "Showcase 10: 🔨 Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/showcase-10.html#preparation",
    "href": "exercise/showcase-10.html#preparation",
    "title": "Showcase 10: 🔨 Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/showcase-10.html#import-and-process-the-data",
    "href": "exercise/showcase-10.html#import-and-process-the-data",
    "title": "Showcase 10: 🔨 Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n# Topic models \nstm_k0 &lt;- qs::qread(here(\"local_data/stm_k0.qs\"))\nstm_exploration &lt;- qs::qread(here(\"local_data/stm_exploration.qs\"))\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))"
  },
  {
    "objectID": "exercise/showcase-10.html#text-as-data-in-r-part-ii",
    "href": "exercise/showcase-10.html#text-as-data-in-r-part-ii",
    "title": "Showcase 10: 🔨 Automatic analysis of text in R",
    "section": "Text as data in R (Part II)",
    "text": "Text as data in R (Part II)\n\nStep-by-step DTM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\nDocument-feature matrix of: 46,670 documents, 87,172 features (99.99% sparse) and 0 docvars.\n                     features\ndocs                  bite detox digital digitaldetox enjoy fly happitizer\n  1000009901563838465    1     1       1            1     2   1          1\n  1000038819520008193    0     0       0            1     0   0          0\n  1000042717492187136    0     0       0            1     0   0          0\n  1000043574673715203    0     0       0            1     0   0          0\n  1000053895035531264    0     1       1            1     0   0          0\n  1000075155891281925    0     0       0            1     0   0          0\n                     features\ndocs                  happitizers https inspiration\n  1000009901563838465           1     1           1\n  1000038819520008193           0     1           0\n  1000042717492187136           0     1           0\n  1000043574673715203           0     2           0\n  1000053895035531264           0     2           0\n  1000075155891281925           0     1           0\n[ reached max_ndoc ... 46,664 more documents, reached max_nfeat ... 87,162 more features ]\n\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\nDocument-feature matrix of: 46,670 documents, 49,123 features (99.98% sparse) and 35 docvars.\n            features\ndocs         put blackberry iphone read pew report teens distracted driving\n  5777201122   1          1      1    1   1      1     1          1       1\n  4814687834   0          0      0    0   0      0     0          0       0\n  4813781509   0          0      0    0   0      0     0          0       0\n  3351604894   0          0      0    0   0      0     0          0       0\n  3350930292   0          0      0    0   0      0     0          0       0\n  3349372574   0          0      0    0   0      0     0          0       0\n            features\ndocs         #digitaldetox\n  5777201122             1\n  4814687834             1\n  4813781509             1\n  3351604894             1\n  3350930292             1\n  3349372574             1\n[ reached max_ndoc ... 46,664 more documents, reached max_nfeat ... 49,113 more features ]\n\n\n\n\n\n\n\nNetwork of hashtags\n\ntidytextquanteda\n\n\n\n# Extract hashtags\ntweets_hashtags &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) \n\n# Extract most common hashtags\ntop50_hashtags_tidy &lt;- tweets_hashtags %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n# Extract DFM with only hashtags\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Extract most common hashtags \ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of hashtags\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n\n\n\n\nTopic modeling\n\nPreparation\n\nPruning\n\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim(\n    min_docfreq = 0.0001, \n    max_docfreq = .099, \n    docfreq_type = \"prop\"\n  )\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n    convert(to = \"stm\")\n\n\n\n\n\n\n\nConverting the trimmmed DFM to an stm object will result in an errormessage\n\nWarning message:\nIn dfm2stm(x, docvars, omit_empty = TRUE) : Dropped 46,670 empty document(s)\n\nThis is due to the fact, that some tweets are “empty” or do not match with any feature after the pruning. To successfully match the stm results with the original data, the “empty” tweets need to be dropped. To identify the empty cases, run\n\n# Check if tweet contains feature\nempty_docs &lt;- Matrix::rowSums(as(quanteda_dfm_trim, \"Matrix\")) == 0 \n\n# Create vector for empty tweet identification \nempty_docs_ids &lt;- quanteda_dfm_trim@docvars$docname[empty_docs]\n\n\n# Optional: Print indices of empty documents\nif (any(empty_docs)) {\n  cat(\"Indices of empty documents:\", which(empty_docs), \"\\n\")\n  \n  # Print corresponding docnames\n  cat(\"Docnames of empty documents:\", quanteda_dfm_trim@docvars$docname[empty_docs], \"\\n\")\n}\n\n\n\n\n\n\n\nSelect model\n\nbased on k = 0\n\n# Estimate model\nstm_k0 &lt;- stm(\n    quanteda_stm$documents, \n    quanteda_stm$vocab, \n    K = 0, \n    max.em.its = 50,\n    init.type = \"Spectral\", \n    seed = 42 \n  )\n\n\n# Preview\nstm_k0\n\nA topic model with 72 topics, 46574 documents and a 8268 word dictionary.\n\n\n\n\nbased on different model diagnostics\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) # use multiple sessions \n\n# Estimate multiple models\nstm_exploration &lt;- tibble(k = seq(from = 5, to = 85, by = 5)) %&gt;% \n    mutate(mdl = furrr::future_map(k, ~stm::stm(\n      documents =  quanteda_stm$documents,\n      vocab = quanteda_stm$vocab, \n      K = ., \n      seed = 42,\n      max.em.its = 1000,\n      init.type = \"Spectral\",\n      verbose = FALSE),\n      .options = furrr::furrr_options(seed = 42))\n  )\n\n\nstm_exploration$mdl\n\n[[1]]\nA topic model with 5 topics, 46574 documents and a 8268 word dictionary.\n\n[[2]]\nA topic model with 10 topics, 46574 documents and a 8268 word dictionary.\n\n[[3]]\nA topic model with 15 topics, 46574 documents and a 8268 word dictionary.\n\n[[4]]\nA topic model with 20 topics, 46574 documents and a 8268 word dictionary.\n\n[[5]]\nA topic model with 25 topics, 46574 documents and a 8268 word dictionary.\n\n[[6]]\nA topic model with 30 topics, 46574 documents and a 8268 word dictionary.\n\n[[7]]\nA topic model with 35 topics, 46574 documents and a 8268 word dictionary.\n\n[[8]]\nA topic model with 40 topics, 46574 documents and a 8268 word dictionary.\n\n[[9]]\nA topic model with 45 topics, 46574 documents and a 8268 word dictionary.\n\n[[10]]\nA topic model with 50 topics, 46574 documents and a 8268 word dictionary.\n\n[[11]]\nA topic model with 55 topics, 46574 documents and a 8268 word dictionary.\n\n[[12]]\nA topic model with 60 topics, 46574 documents and a 8268 word dictionary.\n\n[[13]]\nA topic model with 65 topics, 46574 documents and a 8268 word dictionary.\n\n[[14]]\nA topic model with 70 topics, 46574 documents and a 8268 word dictionary.\n\n[[15]]\nA topic model with 75 topics, 46574 documents and a 8268 word dictionary.\n\n[[16]]\nA topic model with 80 topics, 46574 documents and a 8268 word dictionary.\n\n[[17]]\nA topic model with 85 topics, 46574 documents and a 8268 word dictionary.\n\n\n\n\n\nExploration\n\n# Create heldout\nheldout &lt;- make.heldout(\n  quanteda_stm$documents,\n  quanteda_stm$vocab,\n  seed = 42)\n\n# Create model diagnostics\nstm_results &lt;- stm_exploration %&gt;%\n  mutate(exclusivity = map(mdl, exclusivity),\n         semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n         eval_heldout = map(mdl, eval.heldout, heldout$missing),\n         residual = map(mdl, checkResiduals, quanteda_stm$documents),\n         bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n         lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n         lbound = bound + lfact,\n         iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))\n\n\n# Preview \nstm_results\n\n# A tibble: 17 × 10\n       k mdl    exclusivity semantic_coherence eval_heldout residual       bound\n   &lt;dbl&gt; &lt;list&gt; &lt;list&gt;      &lt;list&gt;             &lt;list&gt;       &lt;list&gt;         &lt;dbl&gt;\n 1     5 &lt;STM&gt;  &lt;dbl [5]&gt;   &lt;dbl [5]&gt;          &lt;named list&gt; &lt;named list&gt; -3.25e6\n 2    10 &lt;STM&gt;  &lt;dbl [10]&gt;  &lt;dbl [10]&gt;         &lt;named list&gt; &lt;named list&gt; -3.21e6\n 3    15 &lt;STM&gt;  &lt;dbl [15]&gt;  &lt;dbl [15]&gt;         &lt;named list&gt; &lt;named list&gt; -3.17e6\n 4    20 &lt;STM&gt;  &lt;dbl [20]&gt;  &lt;dbl [20]&gt;         &lt;named list&gt; &lt;named list&gt; -3.12e6\n 5    25 &lt;STM&gt;  &lt;dbl [25]&gt;  &lt;dbl [25]&gt;         &lt;named list&gt; &lt;named list&gt; -3.09e6\n 6    30 &lt;STM&gt;  &lt;dbl [30]&gt;  &lt;dbl [30]&gt;         &lt;named list&gt; &lt;named list&gt; -3.09e6\n 7    35 &lt;STM&gt;  &lt;dbl [35]&gt;  &lt;dbl [35]&gt;         &lt;named list&gt; &lt;named list&gt; -3.08e6\n 8    40 &lt;STM&gt;  &lt;dbl [40]&gt;  &lt;dbl [40]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n 9    45 &lt;STM&gt;  &lt;dbl [45]&gt;  &lt;dbl [45]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n10    50 &lt;STM&gt;  &lt;dbl [50]&gt;  &lt;dbl [50]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n11    55 &lt;STM&gt;  &lt;dbl [55]&gt;  &lt;dbl [55]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n12    60 &lt;STM&gt;  &lt;dbl [60]&gt;  &lt;dbl [60]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n13    65 &lt;STM&gt;  &lt;dbl [65]&gt;  &lt;dbl [65]&gt;         &lt;named list&gt; &lt;named list&gt; -3.05e6\n14    70 &lt;STM&gt;  &lt;dbl [70]&gt;  &lt;dbl [70]&gt;         &lt;named list&gt; &lt;named list&gt; -3.07e6\n15    75 &lt;STM&gt;  &lt;dbl [75]&gt;  &lt;dbl [75]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n16    80 &lt;STM&gt;  &lt;dbl [80]&gt;  &lt;dbl [80]&gt;         &lt;named list&gt; &lt;named list&gt; -3.03e6\n17    85 &lt;STM&gt;  &lt;dbl [85]&gt;  &lt;dbl [85]&gt;         &lt;named list&gt; &lt;named list&gt; -3.06e6\n# ℹ 3 more variables: lfact &lt;dbl&gt;, lbound &lt;dbl&gt;, iterations &lt;dbl&gt;\n\n# Visualize\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n         y = NULL,\n         title = \"Model diagnostics by number of topics\"\n    ) +\n    theme_pubr() +\n    # add highlights \n    geom_vline(aes(xintercept =  5), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 10), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#C77CFF\", alpha = .5) \n\n\n\n\n\n# Models for comparison\nmodels_for_comparison = c(5, 10, 40)\n\n# Create figures\nfig_excl &lt;- stm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n      theme_pubr() \n\n# Create plotly\nfig_excl %&gt;% plotly::ggplotly()\n\n\n\n\n\n\n\nUnderstanding\n\nSelect model for further analysis\n\nn_topics &lt;- 10\n\ntpm &lt;- stm_results |&gt;\n   filter(k == n_topics) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n\n\nOverview\n\ntpm %&gt;% plot(type = \"summary\")\n\n\n\ntpm %&gt;% plot(type = \"hist\")\n\n\n\n\n\n\nDocument/Word-topic relations\n\n# Create data\ntop_gamma &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- tpm %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      topic\n      terms_beta\n      gamma\n    \n  \n  \n    Topic 10\nphone, via, much, #screentime, know, check, people, phones, #parenting, online\n0.122\n    Topic 9\nhelp, read, devices, use, world, tech, family, taking, kids, sleep\n0.114\n    Topic 3\nneed, great, week, going, listen, #unplugging, discuss, @icphenomenallyu, away, well\n0.111\n    Topic 2\n#unplug, just, weekend, #travel, unplug, enjoy, #nature, really, join, nature\n0.103\n    Topic 8\nnew, technology, smartphone, retreat, work, addiction, year, health, internet, without\n0.099\n    Topic 7\ncan, #mindfulness, feel, #switchoff, #digitalwellbeing, #wellness, things, #phonefree, #disconnecttoreconnect, #digitalminimalism\n0.098\n    Topic 4\namp, day, take, #mentalhealth, go, try, #wellbeing, now, give, every\n0.092\n    Topic 1\nsocial, media, get, life, back, like, good, #socialmedia, find, see\n0.091\n    Topic 5\nus, today, days, next, put, happy, may, facebook, share, hour\n0.089\n    Topic 6\none, break, tips, make, screen, love, #technology, free, looking, getting\n0.080\n  \n  \n  \n\n\n\n\n\n\nDocument-topic-relations\n\n# Prepare for merging\ntopic_gammas &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = document, \n    names_from = \"topic\", \n    names_prefix = \"gamma_topic_\",\n    values_from = \"gamma\")\n      \ngammas &lt;- tpm %&gt;%\n  tidytext::tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n  rename(\n    top_topic = topic,\n    top_gamma = gamma) %&gt;% \n  ungroup() %&gt;% \n  left_join(., topic_gammas, by = join_by(document))\n\n# Merge with original data\ntweets_detox_topics &lt;- tweets_detox %&gt;% \n  filter(!(tweet_id %in% empty_docs_ids)) %&gt;% \n  bind_cols(gammas) %&gt;% \n  select(-document)\n\n# Preview\ntweets_detox_topics \n\n# A tibble: 46,574 × 50\n   tweet_id   user_username text         created_at          in_reply_to_user_id\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;              \n 1 5777201122 pblackshaw    Put down yo… 2009-11-16 22:03:12 &lt;NA&gt;               \n 2 4814687834 andrewgerrard @Dawn_Wylie… 2009-10-12 18:48:30 18003723           \n 3 4813781509 pblackshaw    Is Social M… 2009-10-12 17:55:57 &lt;NA&gt;               \n 4 3351604894 KurtyD        Google gear… 2009-08-16 23:20:53 &lt;NA&gt;               \n 5 3350930292 KurtyD        Wifi on the… 2009-08-16 22:31:22 &lt;NA&gt;               \n 6 3349372574 KurtyD        Adios amigo… 2009-08-16 20:37:03 &lt;NA&gt;               \n 7 2722418665 evelynanne    Ok, I'm hea… 2009-07-19 14:31:16 &lt;NA&gt;               \n 8 1959962952 halriley      I'm thinkin… 2009-05-29 14:11:36 &lt;NA&gt;               \n 9 1638612063 deconst       Interesting… 2009-04-28 13:03:14 &lt;NA&gt;               \n10 1626172818 deconst       Back from #… 2009-04-27 03:55:25 &lt;NA&gt;               \n# ℹ 46,564 more rows\n# ℹ 45 more variables: author_id &lt;chr&gt;, lang &lt;chr&gt;, possibly_sensitive &lt;lgl&gt;,\n#   conversation_id &lt;chr&gt;, user_created_at &lt;chr&gt;, user_protected &lt;lgl&gt;,\n#   user_name &lt;chr&gt;, user_verified &lt;lgl&gt;, user_description &lt;chr&gt;,\n#   user_location &lt;chr&gt;, user_url &lt;chr&gt;, user_profile_image_url &lt;chr&gt;,\n#   user_pinned_tweet_id &lt;chr&gt;, retweet_count &lt;int&gt;, like_count &lt;int&gt;,\n#   quote_count &lt;int&gt;, user_tweet_count &lt;int&gt;, user_list_count &lt;int&gt;, …\n\n\n\n\nTopic distribution\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\n\nTopic distribution over time\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(year, fill = top_topic)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(year, fill = top_topic)) +\n  geom_bar(position = \"fill\") +\n  theme_pubr() \n\n\n\n\n\n\nFocus on specific topics\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n    \n      tweet_id\n      user_username\n      created_at\n      text\n      top_gamma\n    \n  \n  \n    1496707135794827266\nbeckygrantstr\n2022-02-24 04:42:43\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/EcV7yKn4WX\n0.8192142\n    1496343978672898048\nbeckygrantstr\n2022-02-23 04:39:39\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/o9e6WQIpm5\n0.8192142\n    1495981434665947137\nbeckygrantstr\n2022-02-22 04:39:02\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/rQgcPltrMt\n0.8192142\n    1499612427503247360\nbeckygrantstr\n2022-03-04 05:07:18\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/DQRQlN1iyq\n0.8192142\n    1499249315381977089\nbeckygrantstr\n2022-03-03 05:04:26\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/L7ly66J2fq\n0.8192142\n    1498886117394980865\nbeckygrantstr\n2022-03-02 05:01:13\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/uJPIMYY1Id\n0.8192142\n    1498522796611321864\nbeckygrantstr\n2022-03-01 04:57:30\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/z4Fv2bmeag\n0.8192142\n    1498159674008510465\nbeckygrantstr\n2022-02-28 04:54:35\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/SdZFAJy6kZ\n0.8192142\n    1497796525929472003\nbeckygrantstr\n2022-02-27 04:51:34\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/OKLMzwfQUA\n0.8192142\n    1497433419512524802\nbeckygrantstr\n2022-02-26 04:48:42\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/yTnqhTPV8h\n0.8192142\n  \n  \n  \n\n\n\n\n\n\nTop Users\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 15)\n\n# A tibble: 15 × 3\n   user_username       n  prop\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt;\n 1 TimeToLogOff     2384 28.8 \n 2 punkt             390  4.72\n 3 petitstvincent    175  2.12\n 4 tanyagoodin       135  1.63\n 5 ditox_unplug      111  1.34\n 6 OurHourOff         99  1.2 \n 7 beckygrantstr      63  0.76\n 8 CreeEscape         56  0.68\n 9 ConsciDigital      51  0.62\n10 phubboo            50  0.6 \n11 digitaldetoxing    47  0.57\n12 smudgedlippy       40  0.48\n13 detox_india        32  0.39\n14 winiepuh           27  0.33\n15 iamscentered       23  0.28"
  },
  {
    "objectID": "exercise/showcase-09.html",
    "href": "exercise/showcase-09.html",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "",
    "text": "Open session slides"
  },
  {
    "objectID": "exercise/showcase-09.html#background",
    "href": "exercise/showcase-09.html#background",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/showcase-09.html#preparation",
    "href": "exercise/showcase-09.html#preparation",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nsource(here::here(\"slides/schedule.R\"))\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/showcase-09.html#import-and-process-the-data",
    "href": "exercise/showcase-09.html#import-and-process-the-data",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/showcase-09.html#digital-disconnection-on-fa-brands-twitter-not-𝕏",
    "href": "exercise/showcase-09.html#digital-disconnection-on-fa-brands-twitter-not-𝕏",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Digital disconnection on  (not 𝕏)",
    "text": "Digital disconnection on  (not 𝕏)\n\nThe structure of the data set & available variables\n\ntweets_correct %&gt;% glimpse()\n\nRows: 361,408\nColumns: 37\n$ tweet_id               &lt;chr&gt; \"7223508762\", \"7222271007\", \"7219735500\", \"7216…\n$ user_username          &lt;chr&gt; \"Princessbride24\", \"winnerandy\", \"the_enthusias…\n$ text                   &lt;chr&gt; \"@Ali_Sweeney detox same week as the digital cl…\n$ created_at             &lt;dttm&gt; 2009-12-31 05:25:55, 2009-12-31 04:44:20, 2009…\n$ in_reply_to_user_id    &lt;chr&gt; \"23018333\", NA, NA, NA, NA, NA, NA, \"19475829\",…\n$ author_id              &lt;chr&gt; \"16157429\", \"14969949\", \"16217478\", \"18001568\",…\n$ lang                   &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\",…\n$ possibly_sensitive     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ conversation_id        &lt;chr&gt; \"7222577237\", \"7222271007\", \"7219735500\", \"7216…\n$ user_created_at        &lt;chr&gt; \"2008-09-06T15:13:58.000Z\", \"2008-06-01T07:34:3…\n$ user_protected         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_name              &lt;chr&gt; \"Sam\", \"Andrew\", \"The Enthusiast\", \"🌻BahSun🌻\"…\n$ user_verified          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_description       &lt;chr&gt; \"As you wish.\", \"Life is a playground, so enjoy…\n$ user_location          &lt;chr&gt; \"Houston\", \"California\", \"Melbourne, Australia\"…\n$ user_url               &lt;chr&gt; \"https://t.co/hchLJvesW1\", NA, \"http://t.co/bLK…\n$ user_profile_image_url &lt;chr&gt; \"https://pbs.twimg.com/profile_images/587051289…\n$ user_pinned_tweet_id   &lt;chr&gt; NA, NA, NA, NA, \"1285575168010735621\", NA, NA, …\n$ retweet_count          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ like_count             &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quote_count            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ user_tweet_count       &lt;int&gt; 3905, 18808, 4494, 35337, 284181, 18907, 39562,…\n$ user_list_count        &lt;int&gt; 7, 12, 101, 49, 540, 82, 91, 12, 117, 46, 192, …\n$ user_followers_count   &lt;int&gt; 103, 481, 2118, 693, 17677, 2527, 8784, 100, 26…\n$ user_following_count   &lt;int&gt; 370, 1488, 391, 317, 16470, 320, 11538, 120, 25…\n$ sourcetweet_type       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_id         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_text       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_lang       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_author_id  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ year                   &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,…\n$ month                  &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,…\n$ day                    &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 29, 28, 28, 27, 27,…\n$ hour                   &lt;int&gt; 5, 4, 3, 1, 22, 2, 0, 20, 21, 1, 19, 8, 8, 3, 9…\n$ minute                 &lt;int&gt; 25, 44, 22, 42, 49, 31, 39, 23, 45, 47, 31, 28,…\n$ retweet_dy             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,…\n$ detox_dy               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n\n\n\n\nBonus: Distribution statistics\n\ntweets_correct %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n361408\n\n\nNumber of columns\n37\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n19\n\n\nlogical\n5\n\n\nnumeric\n12\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntweet_id\n0\n1.00\n9\n19\n0\n361408\n0\n\n\nuser_username\n0\n1.00\n2\n15\n0\n185679\n0\n\n\ntext\n0\n1.00\n13\n938\n0\n343311\n0\n\n\nin_reply_to_user_id\n321844\n0.11\n2\n19\n0\n30993\n0\n\n\nauthor_id\n0\n1.00\n3\n19\n0\n185679\n0\n\n\nlang\n0\n1.00\n2\n3\n0\n60\n0\n\n\nconversation_id\n0\n1.00\n9\n19\n0\n354482\n0\n\n\nuser_created_at\n0\n1.00\n24\n24\n0\n185594\n0\n\n\nuser_name\n0\n1.00\n0\n50\n50\n178328\n0\n\n\nuser_description\n0\n1.00\n0\n353\n26741\n166383\n0\n\n\nuser_location\n66727\n0.82\n1\n147\n0\n51237\n65\n\n\nuser_url\n103146\n0.71\n12\n58\n0\n115594\n0\n\n\nuser_profile_image_url\n0\n1.00\n0\n226\n44\n182266\n0\n\n\nuser_pinned_tweet_id\n232501\n0.36\n4\n19\n0\n56397\n0\n\n\nsourcetweet_type\n349949\n0.03\n6\n6\n0\n1\n0\n\n\nsourcetweet_id\n349949\n0.03\n11\n19\n0\n10480\n0\n\n\nsourcetweet_text\n349949\n0.03\n1\n755\n0\n10283\n0\n\n\nsourcetweet_lang\n349949\n0.03\n2\n3\n0\n42\n0\n\n\nsourcetweet_author_id\n349949\n0.03\n2\n19\n0\n7419\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npossibly_sensitive\n0\n1\n0.01\nFAL: 359068, TRU: 2340\n\n\nuser_protected\n0\n1\n0.00\nFAL: 361408\n\n\nuser_verified\n0\n1\n0.06\nFAL: 340005, TRU: 21403\n\n\nretweet_dy\n0\n1\n0.02\nFAL: 354735, TRU: 6673\n\n\ndetox_dy\n0\n1\n0.16\nFAL: 303396, TRU: 58012\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nretweet_count\n0\n1\n0.77\n21.53\n0\n0\n0\n0.00\n9035\n▇▁▁▁▁\n\n\nlike_count\n0\n1\n3.17\n219.89\n0\n0\n0\n1.00\n98181\n▇▁▁▁▁\n\n\nquote_count\n0\n1\n0.09\n11.07\n0\n0\n0\n0.00\n5213\n▇▁▁▁▁\n\n\nuser_tweet_count\n0\n1\n60860.29\n173889.51\n1\n2933\n12371\n41843.00\n9641956\n▇▁▁▁▁\n\n\nuser_list_count\n0\n1\n375.59\n2643.28\n0\n7\n46\n171.00\n216734\n▇▁▁▁▁\n\n\nuser_followers_count\n0\n1\n37077.25\n532883.29\n0\n274\n1157\n4724.25\n61075617\n▇▁▁▁▁\n\n\nuser_following_count\n0\n1\n2952.88\n18952.23\n0\n221\n808\n2067.00\n1407439\n▇▁▁▁▁\n\n\nyear\n0\n1\n2016.96\n2.73\n2007\n2015\n2017\n2019.00\n2022\n▁▂▇▇▅\n\n\nmonth\n0\n1\n6.36\n3.49\n1\n3\n7\n9.00\n12\n▇▅▅▆▇\n\n\nday\n0\n1\n15.50\n8.95\n1\n7\n15\n23.00\n31\n▇▆▆▆▆\n\n\nhour\n0\n1\n12.62\n6.08\n0\n8\n13\n17.00\n23\n▃▅▆▇▅\n\n\nminute\n0\n1\n27.09\n17.93\n0\n11\n27\n43.00\n59\n▇▆▆▆▆\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ncreated_at\n0\n1\n2007-11-19 09:14:10\n2022-12-31 23:18:38\n2017-04-26 08:15:03\n352271\n\n\n\n\n\n\n\nDistribution of tweets with reference to digital detox over time\n\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = retweet_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Is the tweet a retweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#1DA1F2\", \"#004389\")) +\n    theme_pubr() +    \n    # add annotations\n    annotate(\n      \"text\", \n      x = 14, y = 55000,\n      label = \"Increase of character limit from 140 to 280\") + \n    geom_curve(\n      data = data.frame(\n        x = 14.2965001234837,y = 53507.2283841571,\n        xend = 11.5275706534335, yend = 45412.4966032138),\n      mapping = aes(x = x, y = y, xend = xend, yend = yend),\n      angle = 127L,\n      curvature = 0.28,\n      arrow = arrow(30L, unit(0.1, \"inches\"), \"last\", \"closed\"),\n      inherit.aes = FALSE)\n\n\n\n\n\n\nTweets with reference to ditigal detox by (participating) users\n\ntweets_correct %&gt;% \n    group_by(author_id) %&gt;% \n    summarize(n = n()) %&gt;% \n    mutate(\n        n_grp = case_when(\n                     n &lt;=    1 ~ 1,\n            n &gt;   1 & n &lt;=  10 ~ 2,\n            n &gt;  10 & n &lt;=  25 ~ 3,\n            n &gt;  25 & n &lt;=  50 ~ 4,\n            n &gt;  50 & n &lt;=  75 ~ 5,\n            n &gt;  75 & n &lt;= 100 ~ 6,\n            n &gt; 100 ~ 7\n        ), \n        n_grp_fct = factor(\n            n_grp,\n            levels = c(1:7),\n            labels = c(\n            \" 1\",\n            \" 2 - 10\", \"11 - 25\",\n            \"26 - 50\", \"50 - 75\",\n            \"75 -100\", \"&gt; 100\")\n        )\n    ) %&gt;% \n    sjmisc::frq(n_grp_fct) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Tweets by users with a least on tweet\"\n    #    ) %&gt;% \n    cols_label(\n      val = \"Number of tweets by user\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Number of tweets by user\n      n\n      %raw\n      %cum\n    \n  \n  \n     1\n143364\n77.21\n77.21\n     2 - 10\n39981\n21.53\n98.74\n    11 - 25\n1647\n0.89\n99.63\n    26 - 50\n435\n0.23\n99.86\n    50 - 75\n88\n0.05\n99.91\n    75 -100\n55\n0.03\n99.94\n    &gt; 100\n109\n0.06\n100.00\n    NA\n0\n0.00\nNA\n  \n  \n  \n\n\n\n\n\n\nTweets with reference to ditigal detox by language\n\ntweets_correct %&gt;% \n    sjmisc::frq(\n        lang, \n        sort.frq = c(\"desc\"), \n        min.frq = 2000\n    ) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Language of the collected tweets\"\n    #   ) %&gt;% \n    cols_label(\n      val = \"Twitter language code\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Twitter language code\n      n\n      %raw\n      %cum\n    \n  \n  \n    en\n274351\n75.91\n75.91\n    fr\n20407\n5.65\n81.56\n    es\n16248\n4.50\n86.05\n    de\n14091\n3.90\n89.95\n    pt\n7127\n1.97\n91.92\n    it\n5999\n1.66\n93.58\n    da\n3490\n0.97\n94.55\n    in\n3095\n0.86\n95.41\n    ja\n2428\n0.67\n96.08\n    n &lt; 2000\n14172\n3.92\n100.00\n    NA\n0\n0.00\nNA"
  },
  {
    "objectID": "exercise/showcase-09.html#text-as-data-in-r-part-i",
    "href": "exercise/showcase-09.html#text-as-data-in-r-part-i",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Text as data in R (Part I)",
    "text": "Text as data in R (Part I)\n\nBuild a subsample: Tweets containing #digitaldetox\n\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = detox_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"#digitaldetox used in tweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#1DA1F2\")) +\n    theme_pubr() \n\n\n\n\n\n\n\n\n\n\n\nBonus: distribution statistics\n\ntweets_detox %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n46670\n\n\nNumber of columns\n37\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n19\n\n\nlogical\n5\n\n\nnumeric\n12\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntweet_id\n0\n1.00\n10\n19\n0\n46670\n0\n\n\nuser_username\n0\n1.00\n3\n15\n0\n15383\n0\n\n\ntext\n0\n1.00\n18\n873\n0\n45039\n0\n\n\nin_reply_to_user_id\n43310\n0.07\n3\n19\n0\n2617\n0\n\n\nauthor_id\n0\n1.00\n3\n19\n0\n15383\n0\n\n\nlang\n0\n1.00\n2\n2\n0\n1\n0\n\n\nconversation_id\n0\n1.00\n10\n19\n0\n46235\n0\n\n\nuser_created_at\n0\n1.00\n24\n24\n0\n15383\n0\n\n\nuser_name\n0\n1.00\n1\n50\n0\n15252\n0\n\n\nuser_description\n0\n1.00\n0\n227\n1089\n14612\n0\n\n\nuser_location\n5596\n0.88\n1\n113\n0\n5834\n2\n\n\nuser_url\n5863\n0.87\n18\n44\n0\n11413\n0\n\n\nuser_profile_image_url\n0\n1.00\n59\n151\n0\n15317\n0\n\n\nuser_pinned_tweet_id\n21644\n0.54\n9\n19\n0\n4756\n0\n\n\nsourcetweet_type\n44990\n0.04\n6\n6\n0\n1\n0\n\n\nsourcetweet_id\n44990\n0.04\n18\n19\n0\n1620\n0\n\n\nsourcetweet_text\n44990\n0.04\n23\n402\n0\n1620\n0\n\n\nsourcetweet_lang\n44990\n0.04\n2\n3\n0\n14\n0\n\n\nsourcetweet_author_id\n44990\n0.04\n4\n19\n0\n1227\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npossibly_sensitive\n0\n1\n0.00\nFAL: 46559, TRU: 111\n\n\nuser_protected\n0\n1\n0.00\nFAL: 46670\n\n\nuser_verified\n0\n1\n0.03\nFAL: 45200, TRU: 1470\n\n\nretweet_dy\n0\n1\n0.00\nFAL: 46670\n\n\ndetox_dy\n0\n1\n1.00\nTRU: 46670\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nretweet_count\n0\n1\n0.70\n30.82\n0\n0.00\n0.0\n0\n6618\n▇▁▁▁▁\n\n\nlike_count\n0\n1\n2.88\n220.54\n0\n0.00\n0.0\n1\n47308\n▇▁▁▁▁\n\n\nquote_count\n0\n1\n0.06\n1.62\n0\n0.00\n0.0\n0\n340\n▇▁▁▁▁\n\n\nuser_tweet_count\n0\n1\n27905.41\n76117.39\n1\n1809.00\n10199.0\n16045\n1268964\n▇▁▁▁▁\n\n\nuser_list_count\n0\n1\n208.86\n557.75\n0\n15.00\n109.0\n163\n25072\n▇▁▁▁▁\n\n\nuser_followers_count\n0\n1\n7627.12\n74560.68\n0\n519.25\n2410.0\n5032\n7977533\n▇▁▁▁▁\n\n\nuser_following_count\n0\n1\n2223.94\n6345.92\n0\n398.00\n1749.5\n1989\n475935\n▇▁▁▁▁\n\n\nyear\n0\n1\n2017.40\n2.24\n2009\n2016.00\n2017.0\n2019\n2022\n▁▂▅▇▃\n\n\nmonth\n0\n1\n6.43\n3.48\n1\n3.00\n7.0\n9\n12\n▇▅▅▆▇\n\n\nday\n0\n1\n15.58\n8.77\n1\n8.00\n16.0\n23\n31\n▇▆▇▆▆\n\n\nhour\n0\n1\n12.50\n5.52\n0\n9.00\n13.0\n16\n23\n▂▅▆▇▃\n\n\nminute\n0\n1\n24.62\n17.97\n0\n7.00\n25.0\n39\n59\n▇▅▆▃▃\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ncreated_at\n0\n1\n2009-01-04 12:10:04\n2022-12-31 21:23:34\n2017-09-08 10:45:05\n46448\n\n\n\n\n\n\n\nTransform data to tidy text\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n    # Remove HTML entities\n    mutate(text = str_remove_all(text, remove_reg)) %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n    select(tweet_id, user_username, text) %&gt;% \n    print(n = 5)\n\n# A tibble: 639,459 × 3\n  tweet_id   user_username text      \n  &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;     \n1 5777201122 pblackshaw    blackberry\n2 5777201122 pblackshaw    iphone    \n3 5777201122 pblackshaw    read      \n4 5777201122 pblackshaw    pew       \n5 5777201122 pblackshaw    report    \n# ℹ 639,454 more rows\n\n\n\n\nSummarize all tokens over all tweets\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized %&gt;% \n    print(n = 15)\n\n# A tibble: 87,172 × 2\n   text             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 t.co         57530\n 2 https        52890\n 3 digitaldetox 46642\n 4 digital       8521\n 5 detox         6623\n 6 time          6000\n 7 http          4674\n 8 phone         4213\n 9 unplug        4021\n10 day           2939\n11 life          2548\n12 social        2449\n13 mindfulness   2408\n14 media         2264\n15 technology    2065\n# ℹ 87,157 more rows\n\n\n\n\nVisualization of Top 100 token\n\ntweets_summarized %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 30) +\n    theme_minimal()"
  },
  {
    "objectID": "exercise/showcase-09.html#modeling-realtionships-between-words",
    "href": "exercise/showcase-09.html#modeling-realtionships-between-words",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Modeling realtionships between words",
    "text": "Modeling realtionships between words\n\nCount word pairs within tweets\n\n# Create word paris\ntweets_word_pairs &lt;- tweets_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        tweet_id,\n        sort = TRUE)\n\n# Preview\ntweets_word_pairs %&gt;% \n    print(n = 15)\n\n# A tibble: 3,466,216 × 3\n   item1        item2            n\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;\n 1 t.co         digitaldetox 40640\n 2 digitaldetox t.co         40640\n 3 t.co         https        36605\n 4 https        t.co         36605\n 5 https        digitaldetox 36486\n 6 digitaldetox https        36486\n 7 digital      digitaldetox  7791\n 8 digitaldetox digital       7791\n 9 t.co         digital       7365\n10 digital      t.co          7365\n11 https        digital       6856\n12 digital      https         6856\n13 detox        digitaldetox  6004\n14 digitaldetox detox         6004\n15 t.co         detox         5672\n# ℹ 3,466,201 more rows\n\n\n\n\nSummarize and correlate tokens within tweets\n\n# Create word correlation\ntweets_pairs_corr &lt;- tweets_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        tweet_id, \n        sort = TRUE)\n\n# Preview\ntweets_pairs_corr %&gt;% \n    print(n = 15)\n\n# A tibble: 41,820 × 3\n   item1           item2           correlation\n   &lt;chr&gt;           &lt;chr&gt;                 &lt;dbl&gt;\n 1 jocelyn         brewer                0.999\n 2 brewer          jocelyn               0.999\n 3 jocelyn         discusses             0.983\n 4 discusses       jocelyn               0.983\n 5 discusses       brewer                0.982\n 6 brewer          discusses             0.982\n 7 icphenomenallyu discuss               0.981\n 8 discuss         icphenomenallyu       0.981\n 9 taniamulry      wealth                0.979\n10 wealth          taniamulry            0.979\n11 jocelyn         nutrition             0.968\n12 nutrition       jocelyn               0.968\n13 nutrition       brewer                0.967\n14 brewer          nutrition             0.967\n15 discusses       nutrition             0.951\n# ℹ 41,805 more rows\n\n\n\n\nDisplay correlates for specific token\n\ntweets_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\"detox\", \"digital\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#04316A\")) +\n  theme_pubr()"
  },
  {
    "objectID": "exercise/showcase-09.html#dictionary-based-approach-sentiment-analysis",
    "href": "exercise/showcase-09.html#dictionary-based-approach-sentiment-analysis",
    "title": "Showcase 09: 🔨 Text as data in R",
    "section": "Dictionary based approach: Sentiment analysis",
    "text": "Dictionary based approach: Sentiment analysis\n\nMost commmon positive and negative words\n\ntweets_sentiment_count &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\ntweets_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007D29\")) +\n  theme_pubr()\n\n\n\n\n\nLink word sentiment to tidy data\n\ntweets_sentiment &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(tweet_id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\ntweets_sentiment \n\n# A tibble: 24,372 × 4\n   tweet_id            positive negative sentiment\n   &lt;chr&gt;                  &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 1000009901563838465        3        0         3\n 2 1000038819520008193        1        0         1\n 3 1000042717492187136        4        0         4\n 4 1000043574673715203        1        1         0\n 5 1000075155891281925        4        0         4\n 6 1000086637987139590        1        0         1\n 7 1000094334660825088        1        0         1\n 8 1000133715194920960        1        0         1\n 9 1000255467434729472        0        1        -1\n10 1000271209353895938        1        1         0\n# ℹ 24,362 more rows\n\n\n\n\nOverall distribution sentiment by tweets\n\ntweets_sentiment %&gt;% \n  ggplot(aes(as.factor(sentiment))) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"Sentiment (sum) of tweet\", \n    y = \"Number of tweets\"\n  ) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nDevelopment of tweet sentiment over the years\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Proportion of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "exercise/exercise-10_solution.html",
    "href": "exercise/exercise-10_solution.html",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#background",
    "href": "exercise/exercise-10_solution.html#background",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#preparation",
    "href": "exercise/exercise-10_solution.html#preparation",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, ggwordcloud, # visualization\n  gt, gtExtras, # fancy tables\n  tidytext, textdata, widyr, # tidy text processing\n  quanteda, # quanteda text processing\n  quanteda.textplots, \n  topicmodels, stm, \n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#import-and-process-the-data",
    "href": "exercise/exercise-10_solution.html#import-and-process-the-data",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\nGet twitter data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nDTM/DFM creation\n\ntidytextquanteda\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dfm &lt;- tweets_summarized %&gt;% \n  cast_dfm(tweet_id, text, n)\n\n# Preview\ntweets_dfm\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  mutate(across(text, ~str_replace_all(., \"#digitaldetox\", \"\"))) %&gt;% \n  select(-c(detox_dy, retweet_dy)) %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\n\n\nGet topic model (data)\n\n# TPM data\nstm_results &lt;- qs::qread(here(\"local_data/stm_results.qs\"))\n\n# Base data with topics \ntweets_detox_topics &lt;- qs::qread(here(\"local_data/tweets-digital-detox-topics.qs\"))"
  },
  {
    "objectID": "exercise/exercise-10_solution.html#exercises",
    "href": "exercise/exercise-10_solution.html#exercises",
    "title": "Exercise 10: 🔨 Automatic analysis of text in R",
    "section": "📋 Exercises",
    "text": "📋 Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the “Run all chunks above”-button  of the next chunk.\nYou can choose to solve some exercises using either the tidytext or quanteda functions. As the work steps differ, please select the tab with your preferred package.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\n📋 Exercise 1: Hashtag co-occurence\n\n\n\n\n\n\nObjective(s)\n\n\n\nRecreate the network plot from the session without the token #digitaldetox\n\n\n\nwith tidytextwith quanteda\n\n\n\nCreate new dataset tweets_dfm_cleaned\n\nBased on the dataset tweets_detox,\n\nUse mutate() and create the variables\n\ntext that removes the hashtag \"#digitaldetox\" using str_remove_all() and\nhashtag that extracts hashtags from the variable text with the help of str_extract_all() (and the pattern \"#\\\\S+\").\n\nTokenize the data by using only unnest() on the variable hashtag.\nSummarize occurrences using count(tweet_id, hashtags).\nConvert to DFM using cast_dfm(tweet_id, hashtags, n).\n\nSave this transformation by creating a new dataset with the name tweets_dfm_cleaned.\n\nCreate new dataset top_hashtags_tidy\n\nBased on the dataset tweets_detox,\n\nRepeat steps 1. & 2. from before.\nSummarize occurrences using count(hashtags, sort = TRUE).\nExtract top 50 hashtags by using slice_head(n = 50).\nConvert to string vector by using pull() .\n\nSave this transformation by creating a new dataset with the name top_hashtags_tidy.\n\nVisualize co-occurence\n\nBased on the dataset tweets_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network()\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n# Create new dataset `tweets_dfm_cleaned`\ntweets_dfm_cleaned &lt;- tweets_detox %&gt;% \n  # Edit data\n  mutate(\n    text = str_remove_all(text, \"#digitaldetox\"), \n    hashtags = str_extract_all(text, \"#\\\\S+\")\n    ) %&gt;%\n  # Tokenize\n  unnest(hashtags) %&gt;% \n  # Summarize\n  count(tweet_id, hashtags) %&gt;% \n  # Transform to DFM\n  cast_dfm(tweet_id, hashtags, n)\n\n# Create new dataset `top_hashtags_tidy`\ntop_hashtags_tidy &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize co-occurence\ntweets_dfm_cleaned %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\nCreate new dataset quanteda_dfm_cleaned:\n\nBased on the dataset quanteda_dfm,\n\nUse quanteda::dfm_select(pattern = \"#*\") to create a DFM containing only hashtags.\nUse quanteda::dfm_remove(pattern = \"#digitaldetox\") to removes the hashtag \"#digitaldetox\".\n\nSave this transformation by creating a new dataset with the name quanteda_dfm_cleaned.\n\nCreate new dataset top_hashtags_quanteda\n\nBased on the dataset quanteda_dfm_cleaned,\n\nUse topfeatures(50) o extract the top 50 most common hashtags.\nUse names() to only store the names (not the values).\n\nSave this transformation by creating a new dataset with the name top_hashtags_quanteda.\n\nVisualize co-occurence\n\nBased on the dataset quanteda_dfm_cleaned,\n\nTransform data to feature co-occurrence matrix [FCM] using quanteda::fcm()\nSelect relevant hashtags using quanteda::fcm_select(pattern = top_hashtags_tidy, case_insensitive = FALSE).\nVisualize using quanteda.textplots::textplot_network() .\n\n\nInterpret and compare results\n\nAnalyze patterns or connections among top hashtags.\nDiscuss insights from the visualization, especially in comparison to the visualization of the slides/showcase.\n\n\n\n# Create new dataset `quanteda_dfm_cleaned`\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Create new dataset `top_hashtags_quanteda`\ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Visualize co-occurence\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n# Create new dataset tweets_dfm_cleaned | quanteda_dfm_cleaned\n\n# Create new dataset top_hashtags_quanteda |top_hashtags_quanteda\n\n# Visualize co-occurence\n\n\n\n📋 Exercise 2: Understanding Topic 9\n\n\n\n\n\n\nObjective(s)\n\n\n\nTake a closer look at another topic from the topic model used in the session by examining the most representative tweets and the users who post the most tweets on that topic.\n\n\n\n2.1: Explore top tweets\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nArrange the selected tweets in descending order based on top_gamma values using arrange(-top_gamma).\nExtract the top 10 tweets using slice_head(n = 10).\nSelect only relevant columns (tweet_id, user_username, created_at, text, top_gamma) using select().\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 9) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt()\n\n\n\n2.1 Explore top users\n\nBased on the dataset tweets_detox_topics\n\nUse filter() to only analyze tweets that belong to topic 9. Use the variable top_topic for filtering.\nCount the number of tweets per user using count(user_username, sort = TRUE).\nCalculate the proportion of tweets for each user by creating a new column prop using mutate(prop = round(n/sum(n)*100, 2)).\nExtract the top 15 users with the highest engagement using slice_head(n = 15).\nCreate a tabular presentation of the selected tweets using gt() from the gt package.\n\nInterpret and compare results (with a partner)\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 9) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 15)\n\n\n\n\n📋 Exercise 3: Expolore different topic model\n\n\n\n\n\n\nObjective(s)\n\n\n\nIn the session, three models were considered for closer examination. Choose one of the other topics and recreate all the steps for the initial exploration of the topic model (as in the session).\n\n\n\n3.1 Initial exploration\n\nExplore a different topic model (estimation)\n\nBased on the dataset stm_results,\n\nUse the filter(k == X) to select a different topic model (estimation) of your choice (by defining X).\nTip: Use stm_results$k to see available options for X\nUse pull(mdl) %&gt;% .[[1]] to select the topic model (estimation) with the specified number of topics.\n\nSave this transformation by creating a new dataset with the name tpm_new.\n\nVisual exploration of tpm_new\n\nVisualize the summary of the selected topic model using plot(type = \"summary\") .\n\n\n\n# Explore a different topic model (estimation)\ntpm_new &lt;- stm_results |&gt;\n   filter(k == ) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\n\n\n# Explore a different topic model (estimation)\nn_topics &lt;- 40\n\ntpm_new &lt;- stm_results |&gt;\n   filter(k == n_topics) |&gt; \n   pull(mdl) %&gt;% .[[1]]\n\n# Visual exploration of tpm_new\nplot(tpm_new, type = \"summary\")\n\n\n\n3.2 Document/Word-topic relations\n\nCalculate mean gamma values\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"gamma\") on tpm_new to get the gamma values.\nCalculate the mean gamma values for each topic using group_by() and summarise().\nUse arrange() to sort topics (descending) by gamma\n\nSave this transformation by creating a new dataset with the name top_gamma_new.\n\nIdentify top terms of each topic\n\nbased on the dataset tpm_new\n\nUse tidy(matrix = \"beta\") on tpm_new to get the beta values.\nArrange terms within each topic in descending order based on beta values using group_by(), arrange(-beta), and top_n(10, wt = beta).\nSelect relevant columns (topic, term) using select().\nSummarize the top 10 terms for each topic using summarise(terms_beta = toString(term), .groups = \"drop\").\n\nSave this transformation by creating a new dataset with the name top_beta_new.\n\nCombine top topics and top terms\n\nJoin the dataframes top_gamma_new and top_beta_new based on the “topic” column using left_join().\nWithin mutate(), - Adjusted topic names by topic = paste0(\"Topic \", topic) and - Reorder the dataset with topic = reorder(topic, gamma)\nSave this transformation by creating a new dataset with the name top_topics_terms_new.\n\nPreview the results\n\nDisplay a table preview of the top topics and terms with rounded gamma values using gt()\n\n\n\n# Calculate mean gamma values\n\n# Identify top terms of each topic\n\n# Combine top topics and top terms\n\n# Preview the esults\n\n\n# Calculate mean gamma values\ntop_gamma_new &lt;- tpm_new %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\n# Identify top terms of each topic\ntop_beta_new &lt;- tpm_new %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\n# Combine top topics and top terms\ntop_topics_terms_new &lt;- top_beta_new %&gt;% \n  dplyr::left_join(top_gamma_new, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = paste0(\"Topic \", topic),\n          topic = reorder(topic, gamma)\n      )\n\n# Preview the esults\ntop_topics_terms_new %&gt;% \n  gt()"
  },
  {
    "objectID": "sessions/session-09.html",
    "href": "sessions/session-09.html",
    "title": "Session 9",
    "section": "",
    "text": "🖥️ Session 09\n🧷 Showcase"
  },
  {
    "objectID": "sessions/session-09.html#participate",
    "href": "sessions/session-09.html#participate",
    "title": "Session 9",
    "section": "",
    "text": "🖥️ Session 09\n🧷 Showcase"
  },
  {
    "objectID": "sessions/session-09.html#practice",
    "href": "sessions/session-09.html#practice",
    "title": "Session 9",
    "section": "Practice",
    "text": "Practice\n📋 Exercise 09"
  },
  {
    "objectID": "sessions/session-09.html#suggested-readings",
    "href": "sessions/session-09.html#suggested-readings",
    "title": "Session 9",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nSilge, E. H., & Julia (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-06.html",
    "href": "sessions/session-06.html",
    "title": "Session 6",
    "section": "",
    "text": "🖥️ Session 06"
  },
  {
    "objectID": "sessions/session-06.html#participate",
    "href": "sessions/session-06.html#participate",
    "title": "Session 6",
    "section": "",
    "text": "🖥️ Session 06"
  },
  {
    "objectID": "sessions/session-06.html#mandatory-literature",
    "href": "sessions/session-06.html#mandatory-literature",
    "title": "Session 6",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nNassen, L.-M., Vandebosch, H., Poels, K., & Karsay, K. (2023). Opt-out, abstain, unplug. A systematic review of the voluntary digital disconnection literature. Telematics and Informatics, 81, 101980. https://doi.org/10.1016/j.tele.2023.101980\nRadtke, T., Apel, T., Schenkel, K., Keller, J., & Von Lindern, E. (2022). Digital detox: An effective solution in the smartphone era? A systematic literature review. Mobile Media & Communication, 10(2), 190–215. https://doi.org/10.1177/20501579211028647\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007"
  },
  {
    "objectID": "sessions/session-06.html#other-additional-readings",
    "href": "sessions/session-06.html#other-additional-readings",
    "title": "Session 6",
    "section": "Other additional readings",
    "text": "Other additional readings\n\nMoe, H., & Madsen, O. J. (2021). Understanding digital disconnection beyond media studies. Convergence: The International Journal of Research into New Media Technologies, 27(6), 1584–1598. https://doi.org/10.1177/13548565211048969\nSchmuck, D. (2020). Does Digital Detox Work? Exploring the Role of Digital Detox Applications for Problematic Smartphone Use and Well-Being of Young Adults Using Multigroup Analysis. Cyberpsychology, Behavior, and Social Networking, 23(8), 526–532. https://doi.org/10.1089/cyber.2019.0578\nSyvertsen, T., & Enli, G. (2019). Digital detox: Media resistance and the promise of authenticity. Convergence: The International Journal of Research into New Media Technologies, 26(5-6), 1269–1283. https://doi.org/10.1177/1354856519847325\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-04.html",
    "href": "sessions/session-04.html",
    "title": "Session 4",
    "section": "",
    "text": "🔍 Kontrolle der Ergebnisse des Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-04.html#prepare",
    "href": "sessions/session-04.html#prepare",
    "title": "Session 4",
    "section": "",
    "text": "🔍 Kontrolle der Ergebnisse des Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-04.html#participate",
    "href": "sessions/session-04.html#participate",
    "title": "Session 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 04"
  },
  {
    "objectID": "sessions/session-04.html#mandatory-literature",
    "href": "sessions/session-04.html#mandatory-literature",
    "title": "Session 4",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nGardner, B. (2022). Habit and behavioural complexity: habitual instigation and execution as predictors of simple and complex behaviours. Current Research in Behavioral Sciences, 3, 100081. https://doi.org/10.1016/j.crbeha.2022.100081\nMeier, A., & Reinecke, L. (2021). Computer-Mediated Communication, Social Media, and Mental Health: A Conceptual and Empirical Meta-Review. Communication Research, 48(8), 1182–1209. https://doi.org/gjf96r\nNaab, T. K., & Schnauber, A. (2014). Habitual Initiation of Media Use and a Response-Frequency Measure for Its Examination. Media Psychology, 19(1), 126–155. https://doi.org/10.1080/15213269.2014.951055\nSchnauber-Stockmann, A., & Mangold, F. (2020). Day-to-day routines of media platform use in the digital age: A structuration perspective. Communication Monographs, 87(4), 464–483. https://doi.org/10.1080/03637751.2020.1758336\nSchnauber-Stockmann, A., Scharkow, M., & Breuer, J. (2022). Routines and the Predictability of Day-to-Day Web Use. Media Psychology, 1–23. https://doi.org/10.1080/15213269.2022.2121286"
  },
  {
    "objectID": "sessions/session-04.html#other-readings",
    "href": "sessions/session-04.html#other-readings",
    "title": "Session 4",
    "section": "Other readings",
    "text": "Other readings\n\nAnderson, I. A., & Wood, W. (2020). Habits and the electronic herd: The psychology behind social media’s successes and failures. Consumer Psychology Review, 4(1), 83–99. https://doi.org/10.1002/arcp.1063\nAnderson, I. A., & Wood, W. (2023). Social motivations’ limited influence on habitual behavior: Tests from social media engagement. Motivation Science, 9(2), 107–119. https://doi.org/10.1037/mot0000292\nBayer, J. B., Anderson, I. A., & Tokunaga, R. S. (2022). Building and breaking social media habits. Current Opinion in Psychology, 45, 101303. https://doi.org/10.1016/j.copsyc.2022.101303\nBayer, J. B., & LaRose, R. (2018). Technology habits: Progress, problems, and prospects (pp. 111–130). Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0_7\nCarden, L., & Wood, W. (2018). Habit formation and change. Current Opinion in Behavioral Sciences, 20, 117–122. https://doi.org/10.1016/j.cobeha.2017.12.009\nKatevas, K., Arapakis, I., & Pielot, M. (2018). Typical phone use habits. Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services. https://doi.org/10.1145/3229434.3229441\nKruglanski, A. W., & Szumowska, E. (2020). Habitual Behavior Is Goal-Driven. Perspectives on Psychological Science, 15(5), 1256–1271. https://doi.org/10.1177/1745691620917676\nMazar, A., & Wood, W. (2018). Defining habit in psychology (pp. 13–29). Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0_2\nMeier, A. (2021). Studying problems, not problematic usage: Do mobile checking habits increase procrastination and decrease well-being? Mobile Media & Communication, 10(2), 272–293. https://doi.org/10.1177/20501579211029326\nMeier, A., Beyens, I., Siebers, T., Pouwels, J. L., & Valkenburg, P. M. (2023). Habitual social media and smartphone use are linked to task delay for some, but not all, adolescents. Journal of Computer-Mediated Communication, 28(3). https://doi.org/10.1093/jcmc/zmad008\nNaab, T. K., Karnowski, V., & Schlütz, D. (2018). Reporting Mobile Social Media Use: How Survey and Experience Sampling Measures Differ. Communication Methods and Measures, 13(2), 126–147. https://doi.org/10.1080/19312458.2018.1555799\nOulasvirta, A., Rattenbury, T., Ma, L., & Raita, E. (2011). Habits make smartphone use more pervasive. Personal and Ubiquitous Computing, 16(1), 105–114. https://doi.org/10.1007/s00779-011-0412-2\nSchnauber, A. (2017). Medienselektion im alltag. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-15441-7\nSchnauber-Stockmann, A., Meier, A., & Reinecke, L. (2018). Procrastination out of Habit? The Role of Impulsive Versus Reflective Media Selection in Procrastinatory Media Use. Media Psychology, 21(4), 640–668. https://doi.org/10.1080/15213269.2018.1476156\nSchnauber-Stockmann, A., & Naab, T. K. (2018). The process of forming a mobile media habit: results of a longitudinal study in a real-world setting. Media Psychology, 22(5), 714–742. https://doi.org/10.1080/15213269.2018.1513850\nVerplanken, B. (Ed.). (2018). The psychology of habit. Springer International Publishing. https://doi.org/10.1007/978-3-319-97529-0\nWood, W., & Rünger, D. (2016). Psychology of Habit. Annual Review of Psychology, 67(1), 289–314. https://doi.org/10.1146/annurev-psych-122414-033417\nWood, W., Mazar, A., & Neal, D. T. (2021). Habits and Goals in Human Behavior: Separate but Interacting Systems. Perspectives on Psychological Science, 17(2), 590–605. https://doi.org/10.1177/1745691621994226\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-02.html",
    "href": "sessions/session-02.html",
    "title": "Session 2",
    "section": "",
    "text": "✍️ Start working on Basiskurs R/RStudio. Dealine for the course certificate (via mail to christoph.adrian@fau.de) is 13.11.2023\n\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013"
  },
  {
    "objectID": "sessions/session-02.html#prepare",
    "href": "sessions/session-02.html#prepare",
    "title": "Session 2",
    "section": "",
    "text": "✍️ Start working on Basiskurs R/RStudio. Dealine for the course certificate (via mail to christoph.adrian@fau.de) is 13.11.2023\n\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://www.taylorfrancis.com/books/9781003024583/chapters/10.4324/9781003024583-8\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013"
  },
  {
    "objectID": "sessions/session-02.html#participate",
    "href": "sessions/session-02.html#participate",
    "title": "Session 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 02\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/slides-09.html#schedule",
    "href": "slides/slides-09.html#schedule",
    "title": "🔨 Text as data in R",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis 🎥\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-09.html#looking-at-the-discourse",
    "href": "slides/slides-09.html#looking-at-the-discourse",
    "title": "🔨 Text as data in R",
    "section": "Looking at the discourse",
    "text": "Looking at the discourse\nThe theoretical background: digital disconnection\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk (on Twitter) about digital detox/disconnection:\nIs social media a 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\nSource: dup-magazin.de\n\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“"
  },
  {
    "objectID": "slides/slides-09.html#social-media-as-or",
    "href": "slides/slides-09.html#social-media-as-or",
    "title": "🔨 Text as data in R",
    "section": "Social Media as 💊, 👹 or 🍩 ?",
    "text": "Social Media as 💊, 👹 or 🍩 ?\nRelationship between social media use, digital well-being and digital disconnection\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      \n      \n        Social Media as a …\n      \n    \n    \n      Drug\n      Demon\n      Donut\n    \n  \n  \n    What is at stake?\n\nAddiction/health\n\nDistraction\n\nWell-being\n\n    Root cause of problem\n\nIndividual susceptibility\n\nAddictive design\n\nInadequate fit\n\n    User agency\n\nAgency is limited due to innate susceptibilities\n\nAgency needs to be reclaimed from social media platforms\n\nUser has agency, but it is challenged by person-, technology- and context-specific elements\n\n    Focus of disconnection\n\nComplete abstinence, re-training of the ‘faulty brain’ to break the dopamine link\n\nRemoving/weakening the distracting potential of tech, using persuasive design to support exerting social media self-control\n\nDisconnection interventions tailored to persons and/or contexts to ‘optimize the balance’ between benefits and drawbacks of connectivity, mindful use\n\n    Digital disconnection examples\n\nDigital detox, cognitive behavioral therapy\n\nMuting phone, disabling notifications, putting phone in grey-scale, using apps that reward abstinence (e.g., Forest)\n\nLocative disconnection, disconnection apps that extensive tailoring to persons and contexts, mindfulness training\n\n  \n  \n  \n\n\n\n\n(Vanden Abeele et al., 2022)"
  },
  {
    "objectID": "slides/slides-09.html#fa-brands-twitter-digital-detox-digitaldetox",
    "href": "slides/slides-09.html#fa-brands-twitter-digital-detox-digitaldetox",
    "title": "🔨 Text as data in R",
    "section": " digital detox, #digitaldetox, …",
    "text": "digital detox, #digitaldetox, …\nInformation about the data collection process\n\nGoal: Collect all tweets (until 31.12.2022) that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nActual collection was done in the beginning of 2023 (before the takeover by Musk and the associated change in API access)\nAccess via Twitter Academic Research Product Track v2 API with the help of the academictwitteR package (Barrie & Ho, 2021)\n\n\nacademictwitteR::build_query(\n  query = c(\"digital detox\", \"#digitaldetox\", \"digital_detox\"),\n  is_retweet = FALSE\n  )"
  },
  {
    "objectID": "slides/slides-09.html#a-quick-glimpse",
    "href": "slides/slides-09.html#a-quick-glimpse",
    "title": "🔨 Text as data in R",
    "section": "A quick glimpse",
    "text": "A quick glimpse\nThe structure of the data set & available variables\n\ntweets_correct %&gt;% glimpse()\n\nRows: 361,408\nColumns: 37\n$ tweet_id               &lt;chr&gt; \"7223508762\", \"7222271007\", \"7219735500\", \"7216…\n$ user_username          &lt;chr&gt; \"Princessbride24\", \"winnerandy\", \"the_enthusias…\n$ text                   &lt;chr&gt; \"@Ali_Sweeney detox same week as the digital cl…\n$ created_at             &lt;dttm&gt; 2009-12-31 05:25:55, 2009-12-31 04:44:20, 2009…\n$ in_reply_to_user_id    &lt;chr&gt; \"23018333\", NA, NA, NA, NA, NA, NA, \"19475829\",…\n$ author_id              &lt;chr&gt; \"16157429\", \"14969949\", \"16217478\", \"18001568\",…\n$ lang                   &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\",…\n$ possibly_sensitive     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ conversation_id        &lt;chr&gt; \"7222577237\", \"7222271007\", \"7219735500\", \"7216…\n$ user_created_at        &lt;chr&gt; \"2008-09-06T15:13:58.000Z\", \"2008-06-01T07:34:3…\n$ user_protected         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_name              &lt;chr&gt; \"Sam\", \"Andrew\", \"The Enthusiast\", \"🌻BahSun🌻\"…\n$ user_verified          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_description       &lt;chr&gt; \"As you wish.\", \"Life is a playground, so enjoy…\n$ user_location          &lt;chr&gt; \"Houston\", \"California\", \"Melbourne, Australia\"…\n$ user_url               &lt;chr&gt; \"https://t.co/hchLJvesW1\", NA, \"http://t.co/bLK…\n$ user_profile_image_url &lt;chr&gt; \"https://pbs.twimg.com/profile_images/587051289…\n$ user_pinned_tweet_id   &lt;chr&gt; NA, NA, NA, NA, \"1285575168010735621\", NA, NA, …\n$ retweet_count          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ like_count             &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quote_count            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ user_tweet_count       &lt;int&gt; 3905, 18808, 4494, 35337, 284181, 18907, 39562,…\n$ user_list_count        &lt;int&gt; 7, 12, 101, 49, 540, 82, 91, 12, 117, 46, 192, …\n$ user_followers_count   &lt;int&gt; 103, 481, 2118, 693, 17677, 2527, 8784, 100, 26…\n$ user_following_count   &lt;int&gt; 370, 1488, 391, 317, 16470, 320, 11538, 120, 25…\n$ sourcetweet_type       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_id         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_text       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_lang       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sourcetweet_author_id  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ year                   &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,…\n$ month                  &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,…\n$ day                    &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 29, 28, 28, 27, 27,…\n$ hour                   &lt;int&gt; 5, 4, 3, 1, 22, 2, 0, 20, 21, 1, 19, 8, 8, 3, 9…\n$ minute                 &lt;int&gt; 25, 44, 22, 42, 49, 31, 39, 23, 45, 47, 31, 28,…\n$ retweet_dy             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,…\n$ detox_dy               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…"
  },
  {
    "objectID": "slides/slides-09.html#biggest-attention-in-2016-steady-decline-thereafter",
    "href": "slides/slides-09.html#biggest-attention-in-2016-steady-decline-thereafter",
    "title": "🔨 Text as data in R",
    "section": "Biggest attention in 2016, steady decline thereafter",
    "text": "Biggest attention in 2016, steady decline thereafter\nDistribution of tweets with reference to digital detox over time\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = retweet_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Is the tweet a retweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#1DA1F2\", \"#004389\")) +\n    theme_pubr() +    \n    # add annotations\n    annotate(\n      \"text\", \n      x = 14, y = 55000,\n      label = \"Increase of character limit from 140 to 280\") + \n    geom_curve(\n      data = data.frame(\n        x = 14.2965001234837,y = 53507.2283841571,\n        xend = 11.5275706534335, yend = 45412.4966032138),\n      mapping = aes(x = x, y = y, xend = xend, yend = yend),\n      angle = 127L,\n      curvature = 0.28,\n      arrow = arrow(30L, unit(0.1, \"inches\"), \"last\", \"closed\"),\n      inherit.aes = FALSE)"
  },
  {
    "objectID": "slides/slides-09.html#sporadic-tweeting-on-the-topic-mainly-in-english",
    "href": "slides/slides-09.html#sporadic-tweeting-on-the-topic-mainly-in-english",
    "title": "🔨 Text as data in R",
    "section": "Sporadic tweeting on the topic, mainly in English",
    "text": "Sporadic tweeting on the topic, mainly in English\nTweets with reference to ditigal detox by (participating) users and language\n\n\nExpand for full code\n# Number of tweets by user \ntweets_correct %&gt;% \n    group_by(author_id) %&gt;% \n    summarize(n = n()) %&gt;% \n    mutate(\n        n_grp = case_when(\n                     n &lt;=    1 ~ 1,\n            n &gt;   1 & n &lt;=  10 ~ 2,\n            n &gt;  10 & n &lt;=  25 ~ 3,\n            n &gt;  25 & n &lt;=  50 ~ 4,\n            n &gt;  50 & n &lt;=  75 ~ 5,\n            n &gt;  75 & n &lt;= 100 ~ 6,\n            n &gt; 100 ~ 7\n        ), \n        n_grp_fct = factor(\n            n_grp,\n            levels = c(1:7),\n            labels = c(\n            \" 1\",\n            \" 2 - 10\", \"11 - 25\",\n            \"26 - 50\", \"50 - 75\",\n            \"75 -100\", \"&gt; 100\")\n        )\n    ) %&gt;% \n    sjmisc::frq(n_grp_fct) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Tweets by users with a least on tweet\"\n    #    ) %&gt;% \n    cols_label(\n      val = \"Number of tweets by user\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n# Language of Tweets\ntweets_correct %&gt;% \n    sjmisc::frq(\n        lang, \n        sort.frq = c(\"desc\"), \n        min.frq = 2000\n    ) %&gt;% \n    as.data.frame() %&gt;% \n    select(val, frq:cum.prc) %&gt;% \n    # create table\n    gt() %&gt;% \n    # tab_header(\n    #     title = \"Language of the collected tweets\"\n    #   ) %&gt;% \n    cols_label(\n      val = \"Twitter language code\",\n      frq = \"n\",\n      raw.prc = html(\"%&lt;sub&gt;raw&lt;/sub&gt;\"),\n      cum.prc = html(\"%&lt;sub&gt;cum&lt;/sub&gt;\")\n  ) %&gt;% \n  gt_theme_538()\n\n\n\n\n\n\n\n\n  \n    \n    \n      Number of tweets by user\n      n\n      %raw\n      %cum\n    \n  \n  \n     1\n143364\n77.21\n77.21\n     2 - 10\n39981\n21.53\n98.74\n    11 - 25\n1647\n0.89\n99.63\n    26 - 50\n435\n0.23\n99.86\n    50 - 75\n88\n0.05\n99.91\n    75 -100\n55\n0.03\n99.94\n    &gt; 100\n109\n0.06\n100.00\n    NA\n0\n0.00\nNA\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      Twitter language code\n      n\n      %raw\n      %cum\n    \n  \n  \n    en\n274351\n75.91\n75.91\n    fr\n20407\n5.65\n81.56\n    es\n16248\n4.50\n86.05\n    de\n14091\n3.90\n89.95\n    pt\n7127\n1.97\n91.92\n    it\n5999\n1.66\n93.58\n    da\n3490\n0.97\n94.55\n    in\n3095\n0.86\n95.41\n    ja\n2428\n0.67\n96.08\n    n &lt; 2000\n14172\n3.92\n100.00\n    NA\n0\n0.00\nNA"
  },
  {
    "objectID": "slides/slides-09.html#deciding-on-the-right-method",
    "href": "slides/slides-09.html#deciding-on-the-right-method",
    "title": "🔨 Text as data in R",
    "section": "Deciding on the right method",
    "text": "Deciding on the right method\nDifferent approaches of computational analysis of text\n\nNo method that is the one, but specific applications for specific methods\nApart from analyzing basic word and text metrics, there are three variants of automatic text analysis:\n\n🔎 Dictionary Approaches (e.g. Sentiment Analysis)\nUnsupervised Text Analysis (e.g. Topic Modeling)\nSupervised Text Analysis (e.g. ML Classifier)"
  },
  {
    "objectID": "slides/slides-09.html#building-a-shared-vocabulary",
    "href": "slides/slides-09.html#building-a-shared-vocabulary",
    "title": "🔨 Text as data in R",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nImportant terms & definitions\n\n\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. “Hello”, “123”, and “-” are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. “The weather looks good” is an example of a sentence, and the tokens of the sentence are [“The”, “weather”, “looks”, “good].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word’s id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/slides-09.html#explore-tweets-with-digitaldetox",
    "href": "slides/slides-09.html#explore-tweets-with-digitaldetox",
    "title": "🔨 Text as data in R",
    "section": "Explore tweets with #digitaldetox",
    "text": "Explore tweets with #digitaldetox\nWorking through a typical text analysis using tidy data principles\n\n(Silge & Robinson, 2017)\nBut: Tidy data has a specific structure:\n\nEach variable is a column\nEach observation is a row\nEach type of observational unit is a table.\n\nThus the tidy text format is defined as a table with one-token-per-row (Silge & Robinson, 2017)."
  },
  {
    "objectID": "slides/slides-09.html#focusing-on-digitaldetox",
    "href": "slides/slides-09.html#focusing-on-digitaldetox",
    "title": "🔨 Text as data in R",
    "section": "Focusing on #digitaldetox",
    "text": "Focusing on #digitaldetox\nBuild a subsample\n\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )\n\n\n\nExpand for full code\ntweets_correct %&gt;% \n  ggplot(aes(x = as.factor(year), fill = detox_dy)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"#digitaldetox used in tweet?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#1DA1F2\")) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#tokenization-of-the-tweets",
    "href": "slides/slides-09.html#tokenization-of-the-tweets",
    "title": "🔨 Text as data in R",
    "section": "Tokenization of the tweets",
    "text": "Tokenization of the tweets\nTransform data to tidy text\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n    # Remove HTML entities\n    mutate(text = str_remove_all(text, remove_reg)) %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n    select(tweet_id, user_username, text) %&gt;% \n    print(n = 5)\n\n# A tibble: 639,459 × 3\n  tweet_id   user_username text      \n  &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;     \n1 5777201122 pblackshaw    blackberry\n2 5777201122 pblackshaw    iphone    \n3 5777201122 pblackshaw    read      \n4 5777201122 pblackshaw    pew       \n5 5777201122 pblackshaw    report    \n# ℹ 639,454 more rows"
  },
  {
    "objectID": "slides/slides-09.html#count-token-frequency",
    "href": "slides/slides-09.html#count-token-frequency",
    "title": "🔨 Text as data in R",
    "section": "Count token frequency",
    "text": "Count token frequency\nSummarize all tokens over all tweets\n\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized %&gt;% \n    print(n = 15)\n\n\n# A tibble: 87,172 × 2\n   text             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 t.co         57530\n 2 https        52890\n 3 digitaldetox 46642\n 4 digital       8521\n 5 detox         6623\n 6 time          6000\n 7 http          4674\n 8 phone         4213\n 9 unplug        4021\n10 day           2939\n11 life          2548\n12 social        2449\n13 mindfulness   2408\n14 media         2264\n15 technology    2065\n# ℹ 87,157 more rows"
  },
  {
    "objectID": "slides/slides-09.html#the-unavoidable-word-cloud",
    "href": "slides/slides-09.html#the-unavoidable-word-cloud",
    "title": "🔨 Text as data in R",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\nVisualization of Top 100 token\n\ntweets_summarized %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 30) +\n    theme_minimal()"
  },
  {
    "objectID": "slides/slides-09.html#more-than-just-single-words",
    "href": "slides/slides-09.html#more-than-just-single-words",
    "title": "🔨 Text as data in R",
    "section": "More than just single words",
    "text": "More than just single words\nModeling realtionships between words: n-grams and correlations\n\n\nMany interesting text analyses are based on the relationships between words\n\nwhether examining which words tend to follow others immediately (n-grams),\nor that tend to co-occur within the same documents (correlation)\n\n\n\n\n\n(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/slides-09.html#combinations-of-words",
    "href": "slides/slides-09.html#combinations-of-words",
    "title": "🔨 Text as data in R",
    "section": "Combinations of words",
    "text": "Combinations of words\nCount word pairs within tweets\n\n\n# Create word paris\ntweets_word_pairs &lt;- tweets_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        tweet_id,\n        sort = TRUE)\n\n# Preview\ntweets_word_pairs %&gt;% \n    print(n = 15)\n\n\n# A tibble: 3,466,216 × 3\n   item1        item2            n\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;\n 1 t.co         digitaldetox 40640\n 2 digitaldetox t.co         40640\n 3 t.co         https        36605\n 4 https        t.co         36605\n 5 https        digitaldetox 36486\n 6 digitaldetox https        36486\n 7 digital      digitaldetox  7791\n 8 digitaldetox digital       7791\n 9 t.co         digital       7365\n10 digital      t.co          7365\n11 https        digital       6856\n12 digital      https         6856\n13 detox        digitaldetox  6004\n14 digitaldetox detox         6004\n15 t.co         detox         5672\n# ℹ 3,466,201 more rows"
  },
  {
    "objectID": "slides/slides-09.html#correlation-of-words",
    "href": "slides/slides-09.html#correlation-of-words",
    "title": "🔨 Text as data in R",
    "section": "Correlation of words",
    "text": "Correlation of words\nSummarize and correlate tokens within tweets\n\n\n# Create word correlation\ntweets_pairs_corr &lt;- tweets_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        tweet_id, \n        sort = TRUE)\n\n# Preview\ntweets_pairs_corr %&gt;% \n    print(n = 15)\n\n\n# A tibble: 41,820 × 3\n   item1           item2           correlation\n   &lt;chr&gt;           &lt;chr&gt;                 &lt;dbl&gt;\n 1 jocelyn         brewer                0.999\n 2 brewer          jocelyn               0.999\n 3 jocelyn         discusses             0.983\n 4 discusses       jocelyn               0.983\n 5 discusses       brewer                0.982\n 6 brewer          discusses             0.982\n 7 icphenomenallyu discuss               0.981\n 8 discuss         icphenomenallyu       0.981\n 9 taniamulry      wealth                0.979\n10 wealth          taniamulry            0.979\n11 jocelyn         nutrition             0.968\n12 nutrition       jocelyn               0.968\n13 nutrition       brewer                0.967\n14 brewer          nutrition             0.967\n15 discusses       nutrition             0.951\n# ℹ 41,805 more rows"
  },
  {
    "objectID": "slides/slides-09.html#correlates-of-detox-and-digital",
    "href": "slides/slides-09.html#correlates-of-detox-and-digital",
    "title": "🔨 Text as data in R",
    "section": "Correlates of detox and digital",
    "text": "Correlates of detox and digital\nDisplay correlates for specific token\n\n\ntweets_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\"detox\", \"digital\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#04316A\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#lets-talk-about-sentiments",
    "href": "slides/slides-09.html#lets-talk-about-sentiments",
    "title": "🔨 Text as data in R",
    "section": "Let’s talk about sentiments",
    "text": "Let’s talk about sentiments\nDictionary based approach of text analysis\n\n(Silge & Robinson, 2017)\n\n\n\n\n\nAtteveldt et al. (2021) argue that sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries."
  },
  {
    "objectID": "slides/slides-09.html#the-meaning-of-positivenegative",
    "href": "slides/slides-09.html#the-meaning-of-positivenegative",
    "title": "🔨 Text as data in R",
    "section": "The meaning of “positive/negative”",
    "text": "The meaning of “positive/negative”\nMost commmon positive and negative words\n\n\ntweets_sentiment_count &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\ntweets_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007D29\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#enrich-the-original-data",
    "href": "slides/slides-09.html#enrich-the-original-data",
    "title": "🔨 Text as data in R",
    "section": "Enrich the original data",
    "text": "Enrich the original data\nLink word sentiment to tidy data\n\ntweets_sentiment &lt;- tweets_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(tweet_id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\ntweets_sentiment \n\n# A tibble: 24,372 × 4\n   tweet_id            positive negative sentiment\n   &lt;chr&gt;                  &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 1000009901563838465        3        0         3\n 2 1000038819520008193        1        0         1\n 3 1000042717492187136        4        0         4\n 4 1000043574673715203        1        1         0\n 5 1000075155891281925        4        0         4\n 6 1000086637987139590        1        0         1\n 7 1000094334660825088        1        0         1\n 8 1000133715194920960        1        0         1\n 9 1000255467434729472        0        1        -1\n10 1000271209353895938        1        1         0\n# ℹ 24,362 more rows"
  },
  {
    "objectID": "slides/slides-09.html#slightly-more-positive-tweets-than-negative",
    "href": "slides/slides-09.html#slightly-more-positive-tweets-than-negative",
    "title": "🔨 Text as data in R",
    "section": "Slightly more positive tweets than negative",
    "text": "Slightly more positive tweets than negative\nOverall distribution sentiment by tweets\n\ntweets_sentiment %&gt;% \n  ggplot(aes(as.factor(sentiment))) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"Sentiment (sum) of tweet\", \n    y = \"Number of tweets\"\n  ) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-09.html#a-trend-towards-positivity",
    "href": "slides/slides-09.html#a-trend-towards-positivity",
    "title": "🔨 Text as data in R",
    "section": "A trend towards positivity?",
    "text": "A trend towards positivity?\nDevelopment of tweet sentiment over the years\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Number of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- tweets_correct %&gt;% \n  filter(tweet_id %in% tweets_sentiment$tweet_id) %&gt;% \n  left_join(tweets_sentiment) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"-8:-1=negative; 0=neutral; 1:8=positive\") %&gt;%\n  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Proportion of tweets\", \n      fill = \"Sentiment (sum) of tweet\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "slides/slides-09.html#and-now-you-clean-and-repeat",
    "href": "slides/slides-09.html#and-now-you-clean-and-repeat",
    "title": "🔨 Text as data in R",
    "section": "🧪 And now … you: Clean and repeat!",
    "text": "🧪 And now … you: Clean and repeat!\nRedo the tidy text analysis pipeline with cleaned data\n\n\n\n\n\n\nObjective of this exercise\n\n\n\nBrush up basic knowledge of working with R and the tidyverse\nGet to know the typical steps of tidy text analysis, from tokenisation and summarisation to visualisation.\n\n\n\n\nNext steps\n\nDownload files provided on StudOn or shared drives for the sessions\nUnzip the archive at a destination of your choice.\nDouble click on the Exercise-Text_as_data.Rproj to open the RStudio project. This ensures that all dependencies are working correctly.\nOpen the exercise.qmd file and follow the instructions.\nTip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs)."
  },
  {
    "objectID": "slides/slides-09.html#references",
    "href": "slides/slides-09.html#references",
    "title": "🔨 Text as data in R",
    "section": "References",
    "text": "References\n\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\n\n\nBarrie, C., & Ho, J. (2021). academictwitteR: An r package to access the twitter academic research product track v2 API endpoint. Journal of Open Source Software, 6(62), 3272. https://doi.org/10.21105/joss.03272\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-03.html#seminarplan",
    "href": "slides/slides-03.html#seminarplan",
    "title": "🔨 Working with R",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-03.html#kurzes-organisatorische-update",
    "href": "slides/slides-03.html#kurzes-organisatorische-update",
    "title": "🔨 Working with R",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Prüfungsleistungen\n\nAlle Zertifikat vom R-Basiskurs erhalten 🎉\nInformationen zum Kursablauf wurden geupdatet, Update zu Prüfungleistungen folgt noch\nFür alle Prüfungleistungen gilt: 🇩🇪 ist immer möglich, aber gerne 🇬🇧\n🗣️ 2. Präsentationsgruppe: Denken Sie bitte\n\nan die Zusendung des Entwurf der Präsentationsfolien bis spätestens nächste Woche Dienstag 11:00!\ndas Feedbackgespräch am Mittwoch im Anschluss an das Seminar."
  },
  {
    "objectID": "slides/slides-03.html#buliding-best-practice",
    "href": "slides/slides-03.html#buliding-best-practice",
    "title": "🔨 Working with R",
    "section": "Buliding best practice",
    "text": "Buliding best practice\nWillkommen (zurück) zu R\n\n\n\nHow most academics learn R:\n\n\n\n\n\n\n\nHow you should learn R:\n\nVersuchen Sie R nicht systematisch zu lernen, sondern spezifisch anzuwenden.\nOrganisieren Sie Ihre Arbeit in R (mit Projekten)\nSchreiben Sie lesbaren und nachvollziehbaren Code!\nFragen Sie nach!"
  },
  {
    "objectID": "slides/slides-03.html#ein-repository-voller-daten",
    "href": "slides/slides-03.html#ein-repository-voller-daten",
    "title": "🔨 Working with R",
    "section": "Ein Repository voller Daten",
    "text": "Ein Repository voller Daten\nBeispiel für Übung durch Anwendung:  tidytuesday (social data project)\n\n\n\nData is posted to social media every Monday morning.\nExplore the data, watching out for interesting relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "slides/slides-03.html#beispiele-für-tidytuesday",
    "href": "slides/slides-03.html#beispiele-für-tidytuesday",
    "title": "🔨 Working with R",
    "section": "Beispiele für #tidytuesday",
    "text": "Beispiele für #tidytuesday"
  },
  {
    "objectID": "slides/slides-03.html#everything-you-need-in-one-place",
    "href": "slides/slides-03.html#everything-you-need-in-one-place",
    "title": "🔨 Working with R",
    "section": "Everything you need in one place",
    "text": "Everything you need in one place\nOrganisation der Arbeit mit RStudio-Projekten\n\n\n\n\n\n\n\n\nEmpfehlungen:\n\nFür jedes Projekt ein RStudio-Projekt.\nSicherung und Organisation von Daten, Skripte und Ouput an einem Ort, z.B. mit Unterstützung durch R-Pakete wie z.B. prodigenr\nVerwenden Sie immer nur relative, keine absoluten Pfade. Empfehlung: here R-Paket"
  },
  {
    "objectID": "slides/slides-03.html#versionskontrolle-als-kür",
    "href": "slides/slides-03.html#versionskontrolle-als-kür",
    "title": "🔨 Working with R",
    "section": "Versionskontrolle als Kür",
    "text": "Versionskontrolle als Kür\nCrashkurs zu Git(Hub)\n\n MalikaIhle\nVersionkontrolle für Code, gesichert in der Cloud\nVollständige Rückverfolgbarkeit von (gesicherten) Änderungen\nGreat effort, great return."
  },
  {
    "objectID": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "href": "slides/slides-03.html#run-chunks-not-whole-scripts",
    "title": "🔨 Working with R",
    "section": "Run chunks, not (whole) scripts",
    "text": "Run chunks, not (whole) scripts\nOutputorientiertes Coding mit Quarto"
  },
  {
    "objectID": "slides/slides-03.html#rscript-rmarkdown-quarto",
    "href": "slides/slides-03.html#rscript-rmarkdown-quarto",
    "title": "🔨 Working with R",
    "section": "RScript ≤ RMarkdown ≤ Quarto",
    "text": "RScript ≤ RMarkdown ≤ Quarto\nDer Weg vom Code zum Output\n\n\nGrundidee von Quarto : ein Quelldokument kann in eine Vielzahl von Ausgabeformaten umgewandelt werden\nMarkdown-Syntax für Text, verschiedene Programmiersprachen (wie z.B. R und Python) in einem Dokument"
  },
  {
    "objectID": "slides/slides-03.html#develop-your-style",
    "href": "slides/slides-03.html#develop-your-style",
    "title": "🔨 Working with R",
    "section": "Develop your style",
    "text": "Develop your style\nWichtigkeit der Codeformatierung und -dokumentierung\n\n# Strive for \nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHTFTS &lt;- flights |&gt; filter(air_time &lt; 60)\n\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\n\n\nDie Entwicklung (oder Aneignung) eines Codestils ist wichtig!\nWas sich zunächst willkürlich anfühlt, hilft Ihnen mit der Zeit sehr\nUnterstützung durch den tidyverse style guide bzw. die Pakete styler oder lintr"
  },
  {
    "objectID": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "href": "slides/slides-03.html#empfehlung-tidyverse-is-your-friend",
    "title": "🔨 Working with R",
    "section": "Empfehlung: tidyverse is your friend!",
    "text": "Empfehlung: tidyverse is your friend!\nVerschiedenen Paketen für alle Schritte eines Projektes\n\nQuelle: RStudio"
  },
  {
    "objectID": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "href": "slides/slides-03.html#the-friend-of-your-friend-easystats",
    "title": "🔨 Working with R",
    "section": "The friend of your friend: easystats",
    "text": "The friend of your friend: easystats\nFokus auf die Analyse\n\nQuelle: Lüdecke et al. (2022)"
  },
  {
    "objectID": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "href": "slides/slides-03.html#am-anfang-steht-die-theorie",
    "title": "🔨 Working with R",
    "section": "Am Anfang steht die Theorie",
    "text": "Am Anfang steht die Theorie\nTypischer “data science process” als Kontext der Sitzung\n\nQuelle: Wickham et al. (2023)\n\nAdditional steps (add if necessary):"
  },
  {
    "objectID": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "href": "slides/slides-03.html#age-difference-in-years-between-move-love-interests",
    "title": "🔨 Working with R",
    "section": "Age difference in years between move love interests",
    "text": "Age difference in years between move love interests\nDatengrundlage für die Beispiele: Hollywood Age Gap ( |  )\n\n\n\n\n\n\n\n\n\n\n“An informational site showing the age gap between movie love interests.”\nCommunity-Projekt\n\nGuidlines for participation/submission:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "slides/slides-03.html#explore-adapt-repeat",
    "href": "slides/slides-03.html#explore-adapt-repeat",
    "title": "🔨 Working with R",
    "section": "Explore ➞ Adapt ➞ Repeat ⟳",
    "text": "Explore ➞ Adapt ➞ Repeat ⟳\nProzess der Datenaufbereitung\n\n\nnimmt in der Regel den Großteil der Zeit der Datenanalyse in Anspruch\nhäufig bedarf es der mehrfachen Wiederholung dreier Schritte:\n\nder (explorativen) Erkundung,\nder Standartdisierung und\nder (erneuten) Bereinung der Daten"
  },
  {
    "objectID": "slides/slides-03.html#drei-stufen-der-datenqualität",
    "href": "slides/slides-03.html#drei-stufen-der-datenqualität",
    "title": "🔨 Working with R",
    "section": "Drei Stufen der Datenqualität",
    "text": "Drei Stufen der Datenqualität\nTypische Strategien zur Datenbereinigung nach Pearson (2018)\n\n\n\n\n\n\nQuelle: Jonge & Loo (2013)\n\n\n\n\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\nWie viele Fälle sind enthalten? Wie viele Variablen?\nWie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\nUntersuchung deskriptiver Statistiken für jede Variable;\nExplorative Visualisierung;\nVerschiedene Verfahren zur Suche nach Anomalien in den Daten;\nUntersuchung der Beziehungen zwischen Schlüsselvariablen mit Hilfe von Scatterplots/Boxplots/Mosaic-Plots;\nDokumentation des Vorgehens und der Ergebnisse (z.B. mit .rmd-Dokument). Dient als Grundlage für die anschließende Analyse und Erläuterung der Ergebnisse."
  },
  {
    "objectID": "slides/slides-03.html#direkter-download-via-url",
    "href": "slides/slides-03.html#direkter-download-via-url",
    "title": "🔨 Working with R",
    "section": "Direkter Download via URL",
    "text": "Direkter Download via URL\nDatenimport und -preview\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n🔍 Wie viele Fälle sind enthalten? Wie viele Variablen?\n🔍 Wie lauten die Variablennamen? Sind sie sinnvoll?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") \nage_gaps \n\n# A tibble: 1,177 × 12\n   `Movie Name`       `Release Year` Director    `Age Difference` `Actor 1 Name`\n   &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         \n 1 Harold and Maude             1971 Hal Ashby                 52 Bud Cort      \n 2 Venus                        2006 Roger Mich…               50 Peter O'Toole \n 3 The Quiet American           2002 Phillip No…               49 Michael Caine \n 4 The Big Lebowski             1998 Joel Coen                 45 David Huddles…\n 5 Beginners                    2010 Mike Mills                43 Christopher P…\n 6 Poison Ivy                   1992 Katt Shea                 42 Tom Skerritt  \n 7 Whatever Works               2009 Woody Allen               40 Larry David   \n 8 Entrapment                   1999 Jon Amiel                 39 Sean Connery  \n 9 Husbands and Wives           1992 Woody Allen               38 Woody Allen   \n10 Magnolia                     1999 Paul Thoma…               38 Jason Robards \n# ℹ 1,167 more rows\n# ℹ 7 more variables: `Actor 1 Gender` &lt;chr&gt;, `Actor 1 Birthdate` &lt;date&gt;,\n#   `Actor 1 Age` &lt;dbl&gt;, `Actor 2 Name` &lt;chr&gt;, `Actor 2 Gender` &lt;chr&gt;,\n#   `Actor 2 Birthdate` &lt;chr&gt;, `Actor 2 Age` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/slides-03.html#let-the-cleaning-beginn",
    "href": "slides/slides-03.html#let-the-cleaning-beginn",
    "title": "🔨 Working with R",
    "section": "Let the cleaning beginn",
    "text": "Let the cleaning beginn\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n🔍 Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps %&lt;&gt;% janitor::clean_names()\nage_gaps %&gt;% glimpse()\n\nRows: 1,177\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"…\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992…\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel…\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, …\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David…\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma…\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192…\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, …\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", …\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", …\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0…\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, …"
  },
  {
    "objectID": "slides/slides-03.html#building-the-habits",
    "href": "slides/slides-03.html#building-the-habits",
    "title": "🔨 Working with R",
    "section": "Building the habits!",
    "text": "Building the habits!\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\nWie viele eindeutige Werte hat jede Variable?\nWelcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\nGibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nReminder\n\nVeränderungen nicht im selben Datensatz speichern\nVerständliche Benennung & Kommentierung der Daten\nBearbeitungsschritte kommentieren"
  },
  {
    "objectID": "slides/slides-03.html#kontrolle-der-lageparameter",
    "href": "slides/slides-03.html#kontrolle-der-lageparameter",
    "title": "🔨 Working with R",
    "section": "Kontrolle der Lageparameter",
    "text": "Kontrolle der Lageparameter\nErste Schritte der Datenbereinigung\n\n\n\nBewertung allgemeiner Merkmale des Datensatzes, z. B.:\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n🔍 Wie viele eindeutige Werte hat jede Variable?\n🔍 Welcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\n🔍 Gibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\n\nage_gaps_correct %&gt;% sjmisc::descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1177      0 2000.74 16.67 0.49 2004\n age_difference numeric age_difference 1177      0   10.48  8.53 0.25    8\n    actor_1_age numeric    actor_1_age 1177      0   39.97 10.90 0.32   39\n    actor_2_age numeric    actor_2_age 1177      0   31.27  8.50 0.25   30\n trimmed          range iqr  skew\n 2003.65 88 (1935-2023)  15 -1.68\n    9.41      52 (0-52)  12  1.19\n   39.41     64 (17-81)  15  0.53\n   30.42     64 (17-81)   9  1.39"
  },
  {
    "objectID": "slides/slides-03.html#lets-start-exploring",
    "href": "slides/slides-03.html#lets-start-exploring",
    "title": "🔨 Working with R",
    "section": "Let’s start exploring!",
    "text": "Let’s start exploring!\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(age_difference)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#a-recent-past",
    "href": "slides/slides-03.html#a-recent-past",
    "title": "🔨 Working with R",
    "section": "A recent past …",
    "text": "A recent past …\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,177 × 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ℹ 1,167 more rows"
  },
  {
    "objectID": "slides/slides-03.html#or-still-present",
    "href": "slides/slides-03.html#or-still-present",
    "title": "🔨 Working with R",
    "section": "… or still present?",
    "text": "… or still present?\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name)\n\n# A tibble: 12 × 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal…\n 2 Oppenheimer                         20         2023 Cillian Mur… Florence Pu…\n 3 The Northman                        20         2022 Alexander S… Anya Taylor…\n 4 The Lost City                       16         2022 Channing Ta… Sandra Bull…\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb…\n 6 Everything Everywhere …              9         2022 Ke Huy Quan  Michelle Ye…\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co…\n 8 Oppenheimer                          7         2023 Cillian Mur… Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc… Zoë Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi… Reese Withe…\n11 Your Place or Mine                   2         2023 Ashton Kutc… Reese Withe…\n12 You People                           1         2023 Jonah Hill   Lauren Lond…"
  },
  {
    "objectID": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "href": "slides/slides-03.html#durchschnitts-unterschied-nach-jahren",
    "title": "🔨 Working with R",
    "section": "(Durchschnitts-)Unterschied nach Jahren",
    "text": "(Durchschnitts-)Unterschied nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-03.html#verteilung-nach-jahren",
    "href": "slides/slides-03.html#verteilung-nach-jahren",
    "title": "🔨 Working with R",
    "section": "Verteilung nach Jahren",
    "text": "Verteilung nach Jahren\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\"\n  ) +\n  # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))"
  },
  {
    "objectID": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "href": "slides/slides-03.html#ein-blick-auf-die-korrelation",
    "title": "🔨 Working with R",
    "section": "Ein Blick auf die Korrelation",
    "text": "Ein Blick auf die Korrelation\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1175) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.68 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1177"
  },
  {
    "objectID": "slides/slides-03.html#mit-kanonen-auf-spatzen-schießen",
    "href": "slides/slides-03.html#mit-kanonen-auf-spatzen-schießen",
    "title": "🔨 Working with R",
    "section": "Mit Kanonen auf Spatzen schießen",
    "text": "Mit Kanonen auf Spatzen schießen\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\n# Schätzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n\n\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1175) |      p\n------------------------------------------------------------------------\n(Intercept)  |      234.30 | 29.15 | [177.11, 291.50] |    8.04 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.68 | &lt; .001\n\n\n\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8334.623 | 8334.643 | 8349.835 | 0.048 |     0.047 | 8.324 | 8.331"
  },
  {
    "objectID": "slides/slides-03.html#convenience-wrapper",
    "href": "slides/slides-03.html#convenience-wrapper",
    "title": "🔨 Working with R",
    "section": "Convenience wrapper",
    "text": "Convenience wrapper\nGibt es einen Zusammenhang zwischen Altersunterschied und Releasedatum?\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1175) = 58.96, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 234.30 (95% CI [177.11, 291.50], t(1175) = 8.04, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1175) = -7.68, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "slides/slides-03.html#try---fail---repeat",
    "href": "slides/slides-03.html#try---fail---repeat",
    "title": "🔨 Working with R",
    "section": "Try - fail - repeat",
    "text": "Try - fail - repeat\nKurzes Fazit der heutigen Sitzung\n\n\n\n\nWenn R, dann mit RStudio + Quarto!\nAnschauen - nachmachen - ausprobieren\nKeep it tidy\n(Gute) Routinen bilden\n“There is almost always a package for that …”"
  },
  {
    "objectID": "slides/slides-03.html#literatur",
    "href": "slides/slides-03.html#literatur",
    "title": "🔨 Working with R",
    "section": "Literatur",
    "text": "Literatur\n\n\nJonge, E. de, & Loo, M. van der. (2013). An introduction to data cleaning with R.\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Thériault, R., & Makowski, D. (2022). Easystats: Framework for easy statistical modeling, visualization, and reporting. CRAN. https://easystats.github.io/easystats/\n\n\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: import, tidy, transform, visualize, and model data (2nd edition). O’Reilly.\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-01.html#und-nun-zu-ihnen",
    "href": "slides/slides-01.html#und-nun-zu-ihnen",
    "title": "Kick-Off",
    "section": "Und nun zu Ihnen!",
    "text": "Und nun zu Ihnen!\nVorstellungsrunde\n\nWie heißen Sie?\nWas und wo haben Sie im Bachelor studiert?\nWas studieren Sie aktuell?\nWelches soziale Netzwerk/Medium haben Sie letzte Woche am meisten genutzt und warum?\n\n\n\nHintegrund und Vorwissen\nMediennutzung"
  },
  {
    "objectID": "slides/slides-01.html#was-verstehen-sie-unter-digital-behavioral-data",
    "href": "slides/slides-01.html#was-verstehen-sie-unter-digital-behavioral-data",
    "title": "Kick-Off",
    "section": "Was verstehen Sie unter Digital Behavioral Data?",
    "text": "Was verstehen Sie unter Digital Behavioral Data?\nBitte an Umfrage teilnehmen\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/alhdxw6x3a6e\nTemporary Access Code: 2677 1451"
  },
  {
    "objectID": "slides/slides-01.html#ergebnis",
    "href": "slides/slides-01.html#ergebnis",
    "title": "Kick-Off",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "href": "slides/slides-01.html#ein-definitionsversuch-von-dbd",
    "title": "Kick-Off",
    "section": "Ein Definitionsversuch von DBD",
    "text": "Ein Definitionsversuch von DBD\nnach Weller (2021)\n\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-01.html#lernziele",
    "href": "slides/slides-01.html#lernziele",
    "title": "Kick-Off",
    "section": "Lernziele",
    "text": "Lernziele\nDie Studierenden werden …\n\neinen Überblick über die zentralen Möglichkeiten von DBD und die damit verbundenen Herausforderungen bei der Datenerhebung und -aufbereitung bekommen\nlernen die Stärken und Schwächen verschiedener Methoden zur Erhebung von DBD bewerten\nzentrale Anforderungen an Datenschutz, Forschungsethik und Datenqualität kennen und verstehen lernen\nzentrale sozialwissenschaftliche Methoden zur Analyse von DBD kennenlernen\ndas Wissen über DBD, Statistik und Datenanalyse in eigenen kleinen Projekten zu üben und anzuwenden"
  },
  {
    "objectID": "slides/slides-01.html#aber-was-ist-mit-.",
    "href": "slides/slides-01.html#aber-was-ist-mit-.",
    "title": "Kick-Off",
    "section": "Aber was ist mit …. ?",
    "text": "Aber was ist mit …. ?\nKurzes FAQ mit häufig gestellten Fragen\n\nWelchen Vorkenntnisse sind für den Kurs vorausgesetzt? Interesse an sozialwissenschaftlichen Perspektiven auf Medien, Kommunikation und digitale Technologien & Grundkenntnisse in der Arbeit mit Statistikprogrammen (z.B. R, Python, Stata, SPSS)\nWerden wir praktisch mit Statistikprogrammen arbeiten? Ja. Dazu werden wir R bzw. RStudio nutzen.\n\nDeswegen: Bitte 💻 mitbringen!\n\nWerden wir die mathematische Grundlagen der vorgestellten Methoden lernen? Ja und Nein. Der Kurs konzentriert sich in erster Linie auf die Anwendung; einige mathematische Parameter der vorgestellten Methoden werden jedoch für die Anwendung benötigt und deswegen kurz erörtert.\n\n\n\nLaptop mit RStudio für alle möglich?"
  },
  {
    "objectID": "slides/slides-01.html#different-tools-for-different-tasks",
    "href": "slides/slides-01.html#different-tools-for-different-tasks",
    "title": "Kick-Off",
    "section": "Different tools for different tasks",
    "text": "Different tools for different tasks\nKursorganisation & -kommunikation\n\nGithub-Kursseite: Informationen zu Kurs (Semesterplan, Syllabus, Prüfungleistungen etc). & Sitzungen (Slides, Literatur und ggf. Übungsmaterial)\nZulip: Wichtige Ankündigungen, asynchrone Unterhaltungen, Fragen zum Kurs & zu R\nStudOn: Kursmaterialien, ggf. Beispieldatensätze und Präsentationsaufnahmen\nE-Mail: persönliche Anliegen\n\n\n\nKurze Vorführung der Webseite\nZulip Frage/Probleme bei Registrierung?\nOptional: Github/OSF"
  },
  {
    "objectID": "slides/slides-01.html#what-is-expected",
    "href": "slides/slides-01.html#what-is-expected",
    "title": "Kick-Off",
    "section": "What is expected",
    "text": "What is expected\nLeistungsanforderungen & Prüfungsleistungen\n\n\n\n\n\n\n\nRegelmäßige Teilnahme\n\nmindestens 80% der Sitzungen\nmax. 2 unentschuldigte Fehltermine\n\n\n\n\n\n\n\nPortfolio\n\nVerschiedene Teilleistungen (ins. 100 Punkte)\n\n\n\n\n\nFrage: Syllabus gelesen?\nFrage: Assignments angeschaut?"
  },
  {
    "objectID": "slides/slides-01.html#schritt-für-schritt-zum-ziel",
    "href": "slides/slides-01.html#schritt-für-schritt-zum-ziel",
    "title": "Kick-Off",
    "section": "Schritt für Schritt zum Ziel",
    "text": "Schritt für Schritt zum Ziel\nDas Portfolio im Überblick\n\n\nAusführliche Informationen zu den einzelnen Portfolio-Elementen finden Sie auf der Kursseite unter Assignments.\n\n\n\nNur Short Report am Ende des Semesters\nGgf. wird Bewertungsschema angepasst\n“Ideal-Konzept” vs. Realität (bedingt durch Diskrepanz zwischen Anmeldungen & Teilnahme)"
  },
  {
    "objectID": "slides/slides-01.html#präsentation",
    "href": "slides/slides-01.html#präsentation",
    "title": "Kick-Off",
    "section": "👥 Präsentation",
    "text": "👥 Präsentation\n💡 Theoretische & methodische Grundlage für das Mini-Projekt\n\nUmfang: maximal 30 Minuten\nZiel: Überblick über das Thema der zentralen Texte geben, z.B. zentrale Begriffe, Definitionen und Merkmale der jeweiligen Plattform, Methode und/oder des Tools\nLiteratur wird zur Verfügung gestellt\n\n\nBessonderheit: vorheriges Feedbackgespräch"
  },
  {
    "objectID": "slides/slides-01.html#project-topic-ideas",
    "href": "slides/slides-01.html#project-topic-ideas",
    "title": "Kick-Off",
    "section": "👥 Project topic idea(s)",
    "text": "👥 Project topic idea(s)\n💡 Projektidee vorstellen & weiterentwickeln\n\nUmfang: maximal 10 Minuten & 5 Slides\nZiel: Idee für Gruppenprojekt präsentieren, offene Fragen klären und Zeit für Diskussion & Feedback\nRaum für Fragen und Austausch zwischen den verschiedenen Projektgruppen\n\n\nBessonderheit: Einzige Leistung, die für beide Projekte erbracht werden muss."
  },
  {
    "objectID": "slides/slides-01.html#project-proposal",
    "href": "slides/slides-01.html#project-proposal",
    "title": "Kick-Off",
    "section": "👥 Project proposal",
    "text": "👥 Project proposal\n💡 Erster Entwurf des Written short report\n\nUmfang: mindestens 500 Wörter\nZiel: Forschungsfrage (weiter-)entwickeln und verschriftlichen sowie frühzeitige Entwicklung einer spezifischen Analysestrategie\nFokus auf drei “Abschnitte”: Einleitung, Datengrundlage und methodisches Vorgehen\n\n\nBessonderheit: Grundlage für das Peer Review\n\n\n(Teilweise) Während des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#peer-review",
    "href": "slides/slides-01.html#peer-review",
    "title": "Kick-Off",
    "section": "👤 Peer Review",
    "text": "👤 Peer Review\n💡 Feedback für Bericht geben & bekommmen\n\nUmfang: Durcharbeiten eines Peer-Review-Formulars\nZiel(e):\n\nLernen, andere Projekte zu bewerten und konstruktives Feedback zu schreiben\nZusätzliches Feedback über das eigene Projekte erhalten, dass für den finalen Written short report berücksichtigt werden kann\n\n\n\nBesonderheit: Individuelles Assigment!\n\n\n(Teilweise) Während des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#short-report",
    "href": "slides/slides-01.html#short-report",
    "title": "Kick-Off",
    "section": "👥 Short Report",
    "text": "👥 Short Report\n💡 Zusammenführung der einzelnen Teileistungen\n\n\nUmfang:\n\n750 bis 1000 Wörter pro Person umfassen.\nBei einem Gruppenbericht skaliert die Anzahl der Wörter mit einem Faktor von 0,8 pro Person (z. B. sollte eine Zweiergruppe 1200 bis 1600 Wörter schreiben, eine Dreiergruppe 1800 bis 2400 Wörter).\n\nZiel(e):\n\nmindestens eine der vorgestellten Methoden oder Daten verwendet, um ein Thema Ihrer Wahl zu erforschen.\nKenntnisse der in diesem Kurs behandelten Themen (und darüber hinaus, wenn Sie möchten!) unter Beweis stellen und diese auf einen Datensatz anwenden, um ihn auf sinnvolle Weise zu analysieren.\n\n\n\nBesonderheit: Abgabe als Quarto-Dokument (& PDF)\n\n\nFinale Abgabe erst nach Ende des Semesters"
  },
  {
    "objectID": "slides/slides-01.html#drei-themenblöcke",
    "href": "slides/slides-01.html#drei-themenblöcke",
    "title": "Kick-Off",
    "section": "Drei Themenblöcke",
    "text": "Drei Themenblöcke\nStruktur und Aufbau des Seminars"
  },
  {
    "objectID": "slides/slides-01.html#about-r",
    "href": "slides/slides-01.html#about-r",
    "title": "Kick-Off",
    "section": "About R …",
    "text": "About R …"
  },
  {
    "objectID": "slides/slides-01.html#trust-the-process",
    "href": "slides/slides-01.html#trust-the-process",
    "title": "Kick-Off",
    "section": "Trust the process",
    "text": "Trust the process\nDer Einsatz von R bzw. RStudio im Kurs\nWarum?\n\nKostenlose Software mit vielen nützlichen und beginner-friendly Tutorials\nR or Python? Both!\n\nIm Kurs:\n\nBestehende R-Kenntnisse sind förderlich, aber nicht zwigend notwendig, wichtiger sind praktische Erfahrung im syntaxbasierten Arbeiten\nLearn to code by example: Code von Sitzungen & Beispielen wird bereitgestellt (ggf. durch Showcases)\nPflicht: Basiskurs R/RStudio der FAU\nNützliche Quellen auf Kursseite unter Computing"
  },
  {
    "objectID": "slides/slides-01.html#theorie-meets-praxis",
    "href": "slides/slides-01.html#theorie-meets-praxis",
    "title": "Kick-Off",
    "section": "Theorie meets Praxis",
    "text": "Theorie meets Praxis\nProjektarbeit in Kleingruppen\nDurchführung von zwei Miniprojekte (📁) mit je vier Sitzungen:\n\n📚 Theoretische Grundlage aus der Kommunikationswissenschaft\n📦 Zentrale Methode der Datenerhebung im Kontext des Miniprojektes\n🔨 Vorstellung & Anwendung von Methoden & Analysestrategie\n📊 Vorstellung & Diskussion von Projektideen\n\n💡 Idee:\n\nWissen aneignen ➞ anwenden ➞ teilen/präsentieren ➞ diskutieren 🔄"
  },
  {
    "objectID": "slides/slides-01.html#typische-session",
    "href": "slides/slides-01.html#typische-session",
    "title": "Kick-Off",
    "section": "Typische Session",
    "text": "Typische Session\nfür 📚, 📦 & 🔨: Erst Präsentation, dann Vertiefung\n\n\nPräsentation (ca. 30-45 Min)\n\nUmfasst eine bzw. Ihre Präsentation (inkl. Zeit für Fragen und Diskussionen)\nOption auf weitere, offenere Diskussion im Kurs\n\n\nGroup Activity (ca. 45 - 60 Min)\n\nkleine Gruppenarbeiten zur Vertiefung\nvariiert abhängig vom Thema der jeweiligen Sitzung\nBeispiele:\n\nAnwendung von Tool/Methode mit anschließender kritschen Diskussion\nErstellung eines einfachen Forschungs- oder Analysedesign"
  },
  {
    "objectID": "slides/slides-01.html#typische-session-1",
    "href": "slides/slides-01.html#typische-session-1",
    "title": "Kick-Off",
    "section": "Typische Session",
    "text": "Typische Session\nfür 📊: Pitch ➞ Diskussion ➞ Repeat\n\n\nProject topic idea(s) (ca 5-10 Min)\n\nkurzer Überblick über Thema, Forschungsfrage oder Motivation & ausgewählte Daten(teil)stichprobe (2 Folien),\nkurze Beschreibung von Methode und (geplanter) Analyse (1 Folie)\nErgebnisse und/oder eine Herausforderung aufzeigen, die im Kurs diskutiert werden soll (2 Folien).\n\n\nFragen & Diskussion (ca 5-10 Min)\n\nZeit für Fragen, entweder von der Gruppe an den Kurs oder umgekehrt.\n\n\n\n\n🔁 für jede Gruppe\n\n\n\nStatus bzw. erster Ergebnisse der Project Proposal & Short Repots (max. 5 Folien)"
  },
  {
    "objectID": "slides/slides-01.html#vorläufiger-seminarplan",
    "href": "slides/slides-01.html#vorläufiger-seminarplan",
    "title": "Kick-Off",
    "section": "(Vorläufiger) Seminarplan",
    "text": "(Vorläufiger) Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nDBD: Overview\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n📂 Project 1\n\nAnalysis of media content\n\n\n    4\n\n22.11.2023\n\n📚 Digital disconnection\n\n\n    5\n\n29.11.2023\n\n📦 Data collection methods\n\n\n    6\n\n06.12.2023\n\n🔨 Text as data\n\nChristoph Adrian\n\n    7\n\n13.12.2023\n\n📊 Presentation & Discussion\n\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project 2\n\nAnalysis of media usage\n\n\n    9\n\n10.01.2024\n\n📚 Media habits & routines\n\n\n    10\n\n17.01.2024\n\n📦 Data donation methods\n\n\n    11\n\n24.01.2024\n\n🔨 Working data logs\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-01.html#talking-about-disconnecting",
    "href": "slides/slides-01.html#talking-about-disconnecting",
    "title": "Kick-Off",
    "section": "Talking about disconnecting",
    "text": "Talking about disconnecting\nProjekt 1: #digitaldetox auf sozialen Medien\n\n\n\n\n\n\nQuelle: dup-magazin.de\n\n\n\n\nProjektaufbau\n\nTheoretische Hintergrund: Digital disconnection\nAnalyse von Social Media Post mit #digitaldetox\nArchiv-Daten (Twitter) oder eigene Datenerhebung\nFokus auf Inhalt (Diskurs, andere Hashtags) oder “Akteure”/Accounts"
  },
  {
    "objectID": "slides/slides-01.html#studying-problems-not-problematic-usage",
    "href": "slides/slides-01.html#studying-problems-not-problematic-usage",
    "title": "Kick-Off",
    "section": "Studying problems, not problematic usage?",
    "text": "Studying problems, not problematic usage?\nProjekt 2: Mediennutzungsgewohnheiten und Wohlbefinden\n\n\n\n\n\n\n\n\n\n\nProjektaufbau\n\nHabitualisierte / routinemäßige Mediennutzung\nErhebung, Aufbereitung und Analyse von Logging-Daten\nEigene Erhebung, Data Download Packages, API-Zugang\nFokus auf Datenerhebungs & -aufbereitungsprozess"
  },
  {
    "objectID": "slides/slides-01.html#please-state-your-preference",
    "href": "slides/slides-01.html#please-state-your-preference",
    "title": "Kick-Off",
    "section": "Please state your preference",
    "text": "Please state your preference\nVergabe der Präsentationsthemen\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link und geben Sie Ihre Themenpräferenz an:\n\nhttps://simpleassign.com/poll/-NhW_EY5JXFZ25ZcYZbx"
  },
  {
    "objectID": "slides/slides-01.html#lets-spin-the-wheel",
    "href": "slides/slides-01.html#lets-spin-the-wheel",
    "title": "Kick-Off",
    "section": "Let’s spin the wheel?!",
    "text": "Let’s spin the wheel?!\nZuteilung der Präsentationsthemen\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n    \n  \n  \n    4\n\n22.11.2023\n\n📚 Digital disconnection\n\n    5\n\n29.11.2023\n\n📦 Data collection methods\n\n    9\n\n10.01.2024\n\n📚 Media habits & routines\n\n    10\n\n17.01.2024\n\n📦 Data donation methods\n\n  \n  \n  \n\n\n\n\n\nIn case of emergency: Wheel of Names"
  },
  {
    "objectID": "slides/slides-01.html#before-we-meet-again",
    "href": "slides/slides-01.html#before-we-meet-again",
    "title": "Kick-Off",
    "section": "Before we meet again",
    "text": "Before we meet again\nHinweise und offene Fragen\nHinweise:\n\nLernen Sie die Kursseite & Zulip kennen! Und checken Sie die Infos () zur nächten Sitzung.\nBis zum 13.11.: Basiskurs R/RStudio durcharbeiten. Bitte senden Sie das Kurszertifikat an christoph.adrian@fau.de\n\nFragen:\n\nWhy no English? 🤷\nHaben Sie noch Fragen?"
  },
  {
    "objectID": "slides/slides-01.html#literatur",
    "href": "slides/slides-01.html#literatur",
    "title": "Kick-Off",
    "section": "Literatur",
    "text": "Literatur\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-10.html#schedule",
    "href": "slides/slides-10.html#schedule",
    "title": "🔨 Automatic text analysis in R",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis 🎥\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-10.html#social-media-as-or",
    "href": "slides/slides-10.html#social-media-as-or",
    "title": "🔨 Automatic text analysis in R",
    "section": "Social Media as 💊, 👹 or 🍩 ?",
    "text": "Social Media as 💊, 👹 or 🍩 ?\nDiscussion about digital disconnection on twitter\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nCollect all tweets (until 31.12.2022) via Twitter Academic Research Product Track v2 API & academictwitteR package (Barrie & Ho, 2021) that mention or discuss digital detox (and similar terms)\nDataset for session is a subsample (n = 46670) with only tweets that contain #digitaldetox."
  },
  {
    "objectID": "slides/slides-10.html#the-tidy-text-format-pipeline-basics",
    "href": "slides/slides-10.html#the-tidy-text-format-pipeline-basics",
    "title": "🔨 Automatic text analysis in R",
    "section": "The tidy text format pipeline basics",
    "text": "The tidy text format pipeline basics\nFocus on single words and their relationship documents & sentiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#expansion-of-the-pipeline",
    "href": "slides/slides-10.html#expansion-of-the-pipeline",
    "title": "🔨 Automatic text analysis in R",
    "section": "Expansion of the pipeline",
    "text": "Expansion of the pipeline\nFocus on modeling the realtionships between words & documents\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#quick-recap-on-document-term-matrix-dtm",
    "href": "slides/slides-10.html#quick-recap-on-document-term-matrix-dtm",
    "title": "🔨 Automatic text analysis in R",
    "section": "Quick recap on Document-Term Matrix [DTM]",
    "text": "Quick recap on Document-Term Matrix [DTM]\nMost common structure for (classic) text mining\n\n\nA matrix where:\n\neach row represents one document (such as a tweet),\neach column represents one term, and\neach value (typically) contains the number of appearances of that term in that document.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation",
    "href": "slides/slides-10.html#step-by-step-dtm-creation",
    "title": "🔨 Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\ntweets_tidy %&gt;% \n  select(tweet_id, user_name, text) %&gt;% \n  print(n = 15)\n\n\n# A tibble: 639,459 × 3\n   tweet_id   user_name      text        \n   &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;       \n 1 5777201122 Pete Blackshaw blackberry  \n 2 5777201122 Pete Blackshaw iphone      \n 3 5777201122 Pete Blackshaw read        \n 4 5777201122 Pete Blackshaw pew         \n 5 5777201122 Pete Blackshaw report      \n 6 5777201122 Pete Blackshaw teens       \n 7 5777201122 Pete Blackshaw distracted  \n 8 5777201122 Pete Blackshaw driving     \n 9 5777201122 Pete Blackshaw http        \n10 5777201122 Pete Blackshaw bit.ly      \n11 5777201122 Pete Blackshaw 4abr5p      \n12 5777201122 Pete Blackshaw digitaldetox\n13 4814687834 Andrew Gerrard dawn_wylie  \n14 4814687834 Andrew Gerrard prompted    \n15 4814687834 Andrew Gerrard question    \n# ℹ 639,444 more rows"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation-1",
    "href": "slides/slides-10.html#step-by-step-dtm-creation-1",
    "title": "🔨 Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize ▶️ Summarize\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Preview \ntweets_summarized %&gt;% \n  print(n = 15)\n\n\n# A tibble: 592,499 × 3\n   tweet_id            text              n\n   &lt;chr&gt;               &lt;chr&gt;         &lt;int&gt;\n 1 1000009901563838465 bite              1\n 2 1000009901563838465 detox             1\n 3 1000009901563838465 digital           1\n 4 1000009901563838465 digitaldetox      1\n 5 1000009901563838465 enjoy             2\n 6 1000009901563838465 fly               1\n 7 1000009901563838465 happitizer        1\n 8 1000009901563838465 happitizers       1\n 9 1000009901563838465 https             1\n10 1000009901563838465 inspiration       1\n11 1000009901563838465 mindgourmet       1\n12 1000009901563838465 mindgourmet’s     1\n13 1000009901563838465 sized             1\n14 1000009901563838465 t.co              1\n15 1000009901563838465 taste             1\n# ℹ 592,484 more rows"
  },
  {
    "objectID": "slides/slides-10.html#step-by-step-dtm-creation-2",
    "href": "slides/slides-10.html#step-by-step-dtm-creation-2",
    "title": "🔨 Automatic text analysis in R",
    "section": "Step-by-step DTM creation",
    "text": "Step-by-step DTM creation\nAlong the tidy text pipeline: Tokenize ▶️ Summarize ▶️ DTM\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dtm &lt;- tweets_summarized %&gt;% \n  cast_dtm(tweet_id, text, n)\n\n# Preview\ntweets_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 46670, terms: 87172)&gt;&gt;\nNon-/sparse entries: 592499/4067724741\nSparsity           : 100%\nMaximal term length: 49\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/slides-10.html#choose-or-combine-styles",
    "href": "slides/slides-10.html#choose-or-combine-styles",
    "title": "🔨 Automatic text analysis in R",
    "section": "Choose or combine styles",
    "text": "Choose or combine styles\nSimple with tidytext, precise with quanteda\n\n\n\n\n# Common HTML entities\nremove_reg &lt;- \"&amp;|&lt;|&gt;\"\n\n# Create tidy data\ntweets_tidy &lt;- tweets_detox %&gt;% \n  mutate(\n    text = str_remove_all(text, remove_reg)) %&gt;% \n    tidytext::unnest_tokens(\"text\", text) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\ntweets_summarized &lt;- tweets_tidy %&gt;% \n  count(tweet_id, text) \n\n# Create DTM\ntweets_dtm &lt;- tweets_summarized %&gt;% \n  cast_dtm(tweet_id, text, n)\n\n# Preview\ntweets_dtm\n\n\n\n# Create corpus\nquanteda_corpus &lt;- tweets_detox %&gt;% \n  quanteda::corpus(\n    docid_field = \"tweet_id\", \n    text_field = \"text\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags",
    "href": "slides/slides-10.html#an-example-network-of-hashtags",
    "title": "🔨 Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n# Extract hashtags\ntweets_hashtags &lt;- tweets_detox %&gt;% \n  mutate(hashtags = str_extract_all(\n    text, \"#\\\\S+\")) %&gt;%\n  unnest(hashtags) \n\n# Extract most common hashtags\ntop50_hashtags_tidy &lt;- tweets_hashtags %&gt;% \n  count(hashtags, sort = TRUE) %&gt;% \n  slice_head(n = 50) %&gt;% \n  pull(hashtags)\n\n# Visualize\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags-1",
    "href": "slides/slides-10.html#an-example-network-of-hashtags-1",
    "title": "🔨 Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n# Extract DFM with only hashtags\nquanteda_dfm_hashtags &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_select(pattern = \"#*\") \n\n# Extract most common hashtags \ntop50_hashtags_quanteda &lt;- quanteda_dfm_hashtags %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of hashtags\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#an-example-network-of-hashtags-2",
    "href": "slides/slides-10.html#an-example-network-of-hashtags-2",
    "title": "🔨 Automatic text analysis in R",
    "section": "An example: Network of hashtags",
    "text": "An example: Network of hashtags\nComparison between tidytext & quanteda\n\n\n\n\nExpand for full code\ntweets_hashtags %&gt;% \n  count(tweet_id, hashtags, sort = TRUE) %&gt;% \n  cast_dfm(tweet_id, hashtags, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_hashtags_tidy,\n    case_insensitive = FALSE\n    ) %&gt;% \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\nExpand for full code\nquanteda_dfm_hashtags %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_hashtags_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/slides-10.html#a-new-input-in-the-pipeline",
    "href": "slides/slides-10.html#a-new-input-in-the-pipeline",
    "title": "🔨 Automatic text analysis in R",
    "section": "A new input in the pipeline",
    "text": "A new input in the pipeline\nUnsupervised learning example: Topic modeling\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/slides-10.html#building-a-shared-vocabulary-again",
    "href": "slides/slides-10.html#building-a-shared-vocabulary-again",
    "title": "🔨 Automatic text analysis in R",
    "section": "Building a shared vocabulary … again",
    "text": "Building a shared vocabulary … again\nImportant terms and definitions\n\nTopic Modeling: Form of unsupervised machine learning method used to exploratively identify topics in a corpus. Often, these are so-called mixed-membership models.\nK: Number of topics to be calculated for a given a topic model.\nWord-Topic-Matrix: Matrix describing the conditional probability (beta) with which a feature is prevalent in a given topic.\nDocument-Topic-Matrix: Matrix describing the conditional probability (gamma) with which a topic is prevalent in a given document."
  },
  {
    "objectID": "slides/slides-10.html#beyond-lda",
    "href": "slides/slides-10.html#beyond-lda",
    "title": "🔨 Automatic text analysis in R",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nDifferent topic modeling approaches\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) is a probabilistic generative model that assumes each document in a corpus is a mix of topics and each word in the document is attributable to one of the document’s topics.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) extends LDA by incorporating document-level covariates, allowing for the modeling of how external factors influence topic prevalence.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) represent words as continuous vectors in a high-dimensional space, capturing semantic relationships between words based on their context in the data.\nTopic Modeling with Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) leverages deep learning architectures to automatically learn latent topics from textual data.\n\n\n\nspecify the presumed number of topics K thatyou expect to find in a corpus (e.g., K = 5, i.e., 5 topics)\nthe model then tries to inductively identify 5 topics in the corpus based on the distribution of frequently co-occurring features.\nan algorithm is used for this purpose, which is why topic modeling is a type of “machine learning”."
  },
  {
    "objectID": "slides/slides-10.html#preparation-is-everything",
    "href": "slides/slides-10.html#preparation-is-everything",
    "title": "🔨 Automatic text analysis in R",
    "section": "Preparation is everything",
    "text": "Preparation is everything\nSuggested pre-processing steps (based on Maier et al. (2018))\n\n\n\n⚠️ Deduplication;\n✅ tokenization;\n✅ transforming all characters to lowercase;\n✅ removing punctuation and special characters;\n✅ Removing stop-words;\n⚠️ term unification (lemmatizing or stemming);\n🏗️ relative pruning (attributed to Zipf’s law);\n\n\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(tweets_detox),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\n\nZipf’s law states that the frequency that a word appears is inversely proportional to its rank."
  },
  {
    "objectID": "slides/slides-10.html#how-to-find-k",
    "href": "slides/slides-10.html#how-to-find-k",
    "title": "🔨 Automatic text analysis in R",
    "section": "How to find K",
    "text": "How to find K\nThe most important question of model selection\n\nThe choice of K (whether the model is instructed to identify 5, 15, or 100 topics), has a substantial impact on results:\n\nThe smaller K, the more fine-grained and usually the more exclusive topics;\nthe larger K, the more clearly topics identify individual events or issues.\n\nThe stm package (Roberts et al., 2019) has two build in solution to find the optimal K\n\nsearchK() function\nsetting K = 0 when estimating the model\n\nRecommendation for stm: (Manual) training and evaluation!\n\n\nHowever, with a larger K topics are oftentimes less exclusive, meaning that they somehow overlap."
  },
  {
    "objectID": "slides/slides-10.html#train-and-evaluate-topic-models",
    "href": "slides/slides-10.html#train-and-evaluate-topic-models",
    "title": "🔨 Automatic text analysis in R",
    "section": "Train and evaluate topic models",
    "text": "Train and evaluate topic models\nBetter than searchK(): Manual exploration\n\n\n\n# Set up parallel processing using furrr\nfuture::plan(future::multisession()) \n\n# Estimate multiple models\nstm_exploration &lt;- tibble(\n  k = seq(from = 5, to = 85, by = 5)\n  ) %&gt;%\n  mutate(\n    mdl = furrr::future_map(\n      k, \n      ~stm::stm(\n        documents = quanteda_stm$documents,\n        vocab = quanteda_stm$vocab, \n        K = ., \n        seed = 42,\n        max.em.its = 1000,\n        init.type = \"Spectral\",\n        verbose = FALSE),\n    .options = furrr_options(seed = 42))\n  )\n\n\n\nstm_exploration$mdl\n\n[[1]]\nA topic model with 5 topics, 46574 documents and a 8268 word dictionary.\n\n[[2]]\nA topic model with 10 topics, 46574 documents and a 8268 word dictionary.\n\n[[3]]\nA topic model with 15 topics, 46574 documents and a 8268 word dictionary.\n\n[[4]]\nA topic model with 20 topics, 46574 documents and a 8268 word dictionary.\n\n[[5]]\nA topic model with 25 topics, 46574 documents and a 8268 word dictionary.\n\n[[6]]\nA topic model with 30 topics, 46574 documents and a 8268 word dictionary.\n\n[[7]]\nA topic model with 35 topics, 46574 documents and a 8268 word dictionary.\n\n[[8]]\nA topic model with 40 topics, 46574 documents and a 8268 word dictionary.\n\n[[9]]\nA topic model with 45 topics, 46574 documents and a 8268 word dictionary.\n\n[[10]]\nA topic model with 50 topics, 46574 documents and a 8268 word dictionary.\n\n[[11]]\nA topic model with 55 topics, 46574 documents and a 8268 word dictionary.\n\n[[12]]\nA topic model with 60 topics, 46574 documents and a 8268 word dictionary.\n\n[[13]]\nA topic model with 65 topics, 46574 documents and a 8268 word dictionary.\n\n[[14]]\nA topic model with 70 topics, 46574 documents and a 8268 word dictionary.\n\n[[15]]\nA topic model with 75 topics, 46574 documents and a 8268 word dictionary.\n\n[[16]]\nA topic model with 80 topics, 46574 documents and a 8268 word dictionary.\n\n[[17]]\nA topic model with 85 topics, 46574 documents and a 8268 word dictionary."
  },
  {
    "objectID": "slides/slides-10.html#semantic-coherence-as-the-key",
    "href": "slides/slides-10.html#semantic-coherence-as-the-key",
    "title": "🔨 Automatic text analysis in R",
    "section": "Semantic coherence as the key",
    "text": "Semantic coherence as the key\nDifferent model statistics for evaluation\n\n\nExpand for full code\nstm_results %&gt;%\n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %&gt;%\n  gather(Metric, Value, -k) %&gt;%\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n         y = NULL,\n         title = \"Model diagnostics by number of topics\"\n    ) +\n    theme_pubr() +\n    # add highlights \n    geom_vline(aes(xintercept =  5), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 10), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#C77CFF\", alpha = .5) \n\n\n\n\nSemantic Coherence: tells you how coherent topics are, i.e., how often features describing a topic co-occur and topics thus appear to be internally coherent.\nExclusivity: tells you how exclusive topics are, i.e., how much they differ from each other and topics thus appear to describe different things."
  },
  {
    "objectID": "slides/slides-10.html#finding-the-best-trade-off",
    "href": "slides/slides-10.html#finding-the-best-trade-off",
    "title": "🔨 Automatic text analysis in R",
    "section": "Finding the best trade-off",
    "text": "Finding the best trade-off\nComparison of selected models based on exclusivty and semantic coherence\n\n\nExpand for full code\n# Models for comparison\nmodels_for_comparison = c(5, 10, 40)\n\n# Create figures\nstm_results %&gt;% \n  # Edit data\n  select(k, exclusivity, semantic_coherence) %&gt;%\n  filter(k %in% models_for_comparison) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence))  %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  # Build graph\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n    geom_point(size = 2, alpha = 0.7) +\n    labs(\n      x = \"Semantic coherence\",\n      y = \"Exclusivity\"\n      # title = \"Comparing exclusivity and semantic coherence\",\n      # subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n      ) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/slides-10.html#a-first-overview",
    "href": "slides/slides-10.html#a-first-overview",
    "title": "🔨 Automatic text analysis in R",
    "section": "A first overview",
    "text": "A first overview\nUnderstanding the ‘final’ model (k = 10)\n\ntpm %&gt;% plot(type = \"summary\")"
  },
  {
    "objectID": "slides/slides-10.html#a-more-detailed-overview",
    "href": "slides/slides-10.html#a-more-detailed-overview",
    "title": "🔨 Automatic text analysis in R",
    "section": "A more detailed overview",
    "text": "A more detailed overview\nUnderstanding the ‘final’ model (k = 10)\n\n\nExpand for full code\n# Create data\ntop_gamma &lt;- tpm %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- tpm %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n  \n    \n    \n      Topic\n      Top Terms (based on beta)\n      Gamma\n    \n  \n  \n    10\nphone, via, much, #screentime, know, check, people, phones, #parenting, online\n0.122\n    9\nhelp, read, devices, use, world, tech, family, taking, kids, sleep\n0.114\n    3\nneed, great, week, going, listen, #unplugging, discuss, @icphenomenallyu, away, well\n0.111\n    2\n#unplug, just, weekend, #travel, unplug, enjoy, #nature, really, join, nature\n0.103\n    8\nnew, technology, smartphone, retreat, work, addiction, year, health, internet, without\n0.099\n    7\ncan, #mindfulness, feel, #switchoff, #digitalwellbeing, #wellness, things, #phonefree, #disconnecttoreconnect, #digitalminimalism\n0.098\n    4\namp, day, take, #mentalhealth, go, try, #wellbeing, now, give, every\n0.092\n    1\nsocial, media, get, life, back, like, good, #socialmedia, find, see\n0.091\n    5\nus, today, days, next, put, happy, may, facebook, share, hour\n0.089\n    6\none, break, tips, make, screen, love, #technology, free, looking, getting\n0.080"
  },
  {
    "objectID": "slides/slides-10.html#results-in-a-different-context",
    "href": "slides/slides-10.html#results-in-a-different-context",
    "title": "🔨 Automatic text analysis in R",
    "section": "Results in a different context",
    "text": "Results in a different context\nMerge back with original data for further analysis and comparison\n\n\nExpand for full code\ntop_gamma %&gt;% \n  ggplot(aes(as.factor(topic), gamma)) +\n  geom_col(fill = \"#F57350\") +\n  labs(\n    x = \"Topic\",\n    y = \"Mean gamma\"\n  ) +\n  coord_flip() +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"top\") +\n  theme_pubr()\ntweets_detox_topics %&gt;% \n  mutate(across(top_topic, as.factor)) %&gt;% \n  ggplot(aes(top_topic)) +\n  geom_bar(fill = \"#1DA1F2\") +\n  labs(\n    x = \"\", \n    y = \"Number of tweets\"\n  ) +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/slides-10.html#most-representative-tweets-for-topic-10",
    "href": "slides/slides-10.html#most-representative-tweets-for-topic-10",
    "title": "🔨 Automatic text analysis in R",
    "section": "Most representative tweets for Topic 10",
    "text": "Most representative tweets for Topic 10\nCheck interpretability and relevance of topics\n\n\nExpand for full code\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  arrange(-top_gamma) %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(tweet_id, user_username, created_at, text, top_gamma) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() %&gt;% \n  gt::tab_options(table.font.size = \"10px\")\n\n\n\n\n\n\n  \n    \n    \n      tweet_id\n      user_username\n      created_at\n      text\n      top_gamma\n    \n  \n  \n    1496707135794827266\nbeckygrantstr\n2022-02-24 04:42:43\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/EcV7yKn4WX\n0.8192142\n    1496343978672898048\nbeckygrantstr\n2022-02-23 04:39:39\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/o9e6WQIpm5\n0.8192142\n    1495981434665947137\nbeckygrantstr\n2022-02-22 04:39:02\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/rQgcPltrMt\n0.8192142\n    1499612427503247360\nbeckygrantstr\n2022-03-04 05:07:18\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/DQRQlN1iyq\n0.8192142\n    1499249315381977089\nbeckygrantstr\n2022-03-03 05:04:26\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/L7ly66J2fq\n0.8192142\n    1498886117394980865\nbeckygrantstr\n2022-03-02 05:01:13\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/uJPIMYY1Id\n0.8192142\n    1498522796611321864\nbeckygrantstr\n2022-03-01 04:57:30\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/z4Fv2bmeag\n0.8192142\n    1498159674008510465\nbeckygrantstr\n2022-02-28 04:54:35\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/SdZFAJy6kZ\n0.8192142\n    1497796525929472003\nbeckygrantstr\n2022-02-27 04:51:34\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/OKLMzwfQUA\n0.8192142\n    1497433419512524802\nbeckygrantstr\n2022-02-26 04:48:42\nDo you have a blind spot when it comes to what your kids are doing online? #screentime #parenting #digitaldetox Parents Have a Blind Spot When it Comes to Kids and Screens | by Becky Grant | A Parent Is Born | Feb, 2022 | Medium - via @pensignal  https://t.co/yTnqhTPV8h\n0.8192142"
  },
  {
    "objectID": "slides/slides-10.html#users-with-most-tweets-about-topic-10",
    "href": "slides/slides-10.html#users-with-most-tweets-about-topic-10",
    "title": "🔨 Automatic text analysis in R",
    "section": "Users with most tweets about Topic 10",
    "text": "Users with most tweets about Topic 10\nCheck interpretability and relevance of topics\n\n\ntweets_detox_topics %&gt;% \n  filter(top_topic == 10) %&gt;% \n  count(user_username, sort = TRUE) %&gt;% \n  mutate(\n    prop = round(n/sum(n)*100, 2)) %&gt;% \n  slice_head(n = 10) %&gt;% \n  gt() %&gt;% \n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n  \n    \n    \n      user_username\n      n\n      prop\n    \n  \n  \n    TimeToLogOff\n2384\n28.83\n    punkt\n390\n4.72\n    petitstvincent\n175\n2.12\n    tanyagoodin\n135\n1.63\n    ditox_unplug\n111\n1.34\n    OurHourOff\n99\n1.20\n    beckygrantstr\n63\n0.76\n    CreeEscape\n56\n0.68\n    ConsciDigital\n51\n0.62\n    phubboo\n50\n0.60"
  },
  {
    "objectID": "slides/slides-10.html#validate-validate-validate",
    "href": "slides/slides-10.html#validate-validate-validate",
    "title": "🔨 Automatic text analysis in R",
    "section": "Validate, validate, validate!",
    "text": "Validate, validate, validate!\nThings to remember about topic models\n\ntopic models are a useful tool for automated content analysis, both when exploring a large amount of data and when it comes to systematically identifying relationships between topics and other variables\ncertain prerequisites such as minimum size and variety of the corpus (namely on the level of words and documents and their relation to each other) need to be met for a conclusive model\neverything above a certain degree of word frequencies is considered a “topic,” even if it is not a topic in human interpretation\nReading the tea leaves (Chang et al., 2009) or (again): validate, validate, validate (e.g. with oolong package (Chan & Sältzer, 2020))"
  },
  {
    "objectID": "slides/slides-10.html#and-now-you-model-away",
    "href": "slides/slides-10.html#and-now-you-model-away",
    "title": "🔨 Automatic text analysis in R",
    "section": "🧪 And now … you: Model away!",
    "text": "🧪 And now … you: Model away!\n\n\n\n\n\n\nObjective of this exercise\n\n\n\nBrief review of the contents of the last session\nTeaching the basic steps for creating and analyzing document feature matrices and stm topic models\n\n\n\n\nNext steps\n\nDownload files provided on StudOn or shared drives for the sessions\nUnzip the archive at a destination of your choice.\nDouble click on the Exercise-Automatic_text_analysis.Rproj to open the RStudio project. This ensures that all dependencies are working correctly.\nOpen the exercise.qmd file and follow the instructions.\nTip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs)."
  },
  {
    "objectID": "slides/slides-10.html#references",
    "href": "slides/slides-10.html#references",
    "title": "🔨 Automatic text analysis in R",
    "section": "References",
    "text": "References\n\n\nBarrie, C., & Ho, J. (2021). academictwitteR: An r package to access the twitter academic research product track v2 API endpoint. Journal of Open Source Software, 6(62), 3272. https://doi.org/10.21105/joss.03272\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nChan, C., & Sältzer, M. (2020). Oolong: An r package for validating automated content analysis tools. Journal of Open Source Software, 5(55), 2461. https://doi.org/10.21105/joss.02461\n\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288–296.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988–1003. https://doi.org/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O’Reilly.\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-06.html#seminarplan",
    "href": "slides/slides-06.html#seminarplan",
    "title": "📚 Digital disconnection",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-06.html#kurzes-organisatorisches-update",
    "href": "slides/slides-06.html#kurzes-organisatorisches-update",
    "title": "📚 Digital disconnection",
    "section": "Kurzes organisatorisches Update",
    "text": "Kurzes organisatorisches Update\nInformationen zur nächsten Session\n\nBesondere Vorbereitung für die Session in der nächsten Woche:\n\nInstallation Zeeschuimer-Plugin (mind. 1 Person der Grupppe)\n4CAT-Logindaten via Zulip - Bitte testen!\nInformationen auf der Infopage zur nächsten Session"
  },
  {
    "objectID": "slides/slides-06.html#or",
    "href": "slides/slides-06.html#or",
    "title": "📚 Digital disconnection",
    "section": "💊, 👹 or 🍩 ?",
    "text": "💊, 👹 or 🍩 ?\nTheoretische Betrachtung von Digital Detox nach Vanden Abeele et al. (2022)\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      \n      \n        Social Media as a …\n      \n    \n    \n      Drug\n      Demon\n      Donut\n    \n  \n  \n    What is at stake?\n\nAddiction/health\n\nDistraction\n\nWell-being\n\n    Root cause of problem\n\nIndividual susceptibility\n\nAddictive design\n\nInadequate fit\n\n    User agency\n\nAgency is limited due to innate susceptibilities\n\nAgency needs to be reclaimed from social media platforms\n\nUser has agency, but it is challenged by person-, technology- and context-specific elements\n\n    Focus of disconnection\n\nComplete abstinence, re-training of the ‘faulty brain’ to break the dopamine link\n\nRemoving/weakening the distracting potential of tech, using persuasive design to support exerting social media self-control\n\nDisconnection interventions tailored to persons and/or contexts to ‘optimize the balance’ between benefits and drawbacks of connectivity, mindful use\n\n    Digital disconnection examples\n\nDigital detox, cognitive behavioral therapy\n\nMuting phone, disabling notifications, putting phone in grey-scale, using apps that reward abstinence (e.g., Forest)\n\nLocative disconnection, disconnection apps that extensive tailoring to persons and contexts, mindfulness training"
  },
  {
    "objectID": "slides/slides-06.html#design-your-own-research-design",
    "href": "slides/slides-06.html#design-your-own-research-design",
    "title": "📚 Digital disconnection",
    "section": "🧪 Design your own research (design)",
    "text": "🧪 Design your own research (design)\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nIntegrieren Sie die Untersuchung von (der Absicht zu) “digital detox” in Ihr Forschungsdesign(konzept) der letzten Sitzung, sowohl via Abfrage als auch über DBD.\n\n\n\nNächste Schritte\n\n\nFinden Sie sich in Ihren Gruppen zusammen\nÜberlegen Sie sich, wie Sie die Absicht oder bestehende Strategien zu “digital detox” in Ihr Untersuchungsdesign der letzter Woche integrieren können. Machen Sie mindestens einen Vorschlag für eine Integration via Befragung und einen via digitale Verhaltensdaten.\nHalten Sie Ihre Fragen & Desginideen auf der für Ihre Gruppe vorgesehenen Folienvorlage (nächste Slide) fest.\n\n\n\n\n\n−+\n25:00"
  },
  {
    "objectID": "slides/slides-06.html#let-the-work-beginn",
    "href": "slides/slides-06.html#let-the-work-beginn",
    "title": "📚 Digital disconnection",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Präsentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-06.html#literatur",
    "href": "slides/slides-06.html#literatur",
    "title": "📚 Digital disconnection",
    "section": "Literatur",
    "text": "Literatur\n\n\nVanden Abeele, M. M. P., Halfmann, A., & Lee, E. W. J. (2022). Drug, demon, or donut? Theorizing the relationship between social media use, digital well-being and digital disconnection. Current Opinion in Psychology, 45, 101295. https://doi.org/10.1016/j.copsyc.2021.12.007\n\n\n\n\n\nHome"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 13:15 - 14:15\nFG 2.031"
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Christoph Adrian (he/him)     is a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesday 13:15 - 14:15\nFG 2.031"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Behavioral Data",
    "section": "",
    "text": "Note\n\n\n\nThis page contains an outline of the topics, contents, and assignments for the semester. Please note that the contents of the course will be updated as the semester progresses, with all changes documented here."
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Digital Behavioral Data",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License."
  },
  {
    "objectID": "slides/slides-07.html#seminarplan",
    "href": "slides/slides-07.html#seminarplan",
    "title": "📦 Data collection methods",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-07.html#what-is-it-who-made-it",
    "href": "slides/slides-07.html#what-is-it-who-made-it",
    "title": "📦 Data collection methods",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen 4CAT (Peeters & Hagen, 2022)\n\n\n\nTool zur Analyse und Verarbeitung von Daten aus sozialen Online-Plattformen\nZiel ist es, die Erfassung und Analyse von Daten aus diesen Plattformen über eine Webschnittstelle zugänglich zu machen, ohne dass Programmier- oder Web-Scraping-Kenntnisse erforderlich sind."
  },
  {
    "objectID": "slides/slides-07.html#what-is-it-who-made-it-1",
    "href": "slides/slides-07.html#what-is-it-who-made-it-1",
    "title": "📦 Data collection methods",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen Zeeschuimer (Peeters, 2022)\n\n\n\nBrowsererweiterung, die während des Besuchs einer Social-Media-Website Daten über die Elemente sammelt, die in der Weboberfläche einer Plattform zu sehen sind\nDerzeit werden die folgenden Plattformen unterstützt:\n\n über https://www.tiktok.com\n über https://www.instagram.com\n\nErgänzung zu 4CAT (Peeters, 2022)\n\n\n\n\n\n\n\n\n\n\nDie Zielgruppe sind Forscher, die systematisch Inhalte auf Social-Media-Plattformen untersuchen wollen, die sich dem herkömmlichen Scraping oder der API-basierten Datenerfassung widersetzen.\n\nSie können z. B. TikTok durchsuchen und später eine Liste aller Beiträge in der Reihenfolge exportieren, in der Sie sie gesehen haben. Die Daten können als JSON-Datei exportiert oder zur Analyse und Speicherung in eine 4CAT-Instanz exportiert werden. Zeeschuimer ist in erster Linie als Ergänzung zu 4CAT gedacht, aber Sie können seine Ausgabe auch in Ihre eigene Analysepipeline integrieren.\nDie Plattformunterstützung erfordert regelmäßige Wartung, um mit den Änderungen auf den Plattformen Schritt zu halten. Wenn etwas nicht funktioniert, freuen wir uns über Probleme und Pull Request\nDie Erweiterung stört Sie nicht beim normalen Surfen und lädt niemals automatisch Daten hoch, sondern nur, wenn Sie sie ausdrücklich dazu auffordern."
  },
  {
    "objectID": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram",
    "href": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram",
    "title": "📦 Data collection methods",
    "section": "@tagesschau auf ",
    "text": "@tagesschau auf \nDatenerhebung mit Zeeschuimer"
  },
  {
    "objectID": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram-1",
    "href": "slides/slides-07.html#tagesschau-auf-fa-brands-instagram-1",
    "title": "📦 Data collection methods",
    "section": "@tagesschau auf ",
    "text": "@tagesschau auf \nDaten & Analyse in 🐈🐈 4CAT 🐈🐈"
  },
  {
    "objectID": "slides/slides-07.html#posts-im-zeitverlauf",
    "href": "slides/slides-07.html#posts-im-zeitverlauf",
    "title": "📦 Data collection methods",
    "section": "Posts im Zeitverlauf",
    "text": "Posts im Zeitverlauf\n🐈🐈 4CAT 🐈🐈: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#analyse-der-verwendeten-hashtags",
    "href": "slides/slides-07.html#analyse-der-verwendeten-hashtags",
    "title": "📦 Data collection methods",
    "section": "Analyse der verwendeten Hashtags",
    "text": "Analyse der verwendeten Hashtags\n🐈🐈 4CAT 🐈🐈: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#wordcloud-der-top-hashtags",
    "href": "slides/slides-07.html#wordcloud-der-top-hashtags",
    "title": "📦 Data collection methods",
    "section": "Wordcloud der Top-Hashtags",
    "text": "Wordcloud der Top-Hashtags\n🐈🐈 4CAT 🐈🐈: @tagesschau auf"
  },
  {
    "objectID": "slides/slides-07.html#and-now-you-scraping-with-zeeschuimer-4cat",
    "href": "slides/slides-07.html#and-now-you-scraping-with-zeeschuimer-4cat",
    "title": "📦 Data collection methods",
    "section": "🧪 And now … you: Scraping with Zeeschuimer & 4CAT!",
    "text": "🧪 And now … you: Scraping with Zeeschuimer & 4CAT!\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nScrapen Sie mit dem Zeeschuimer-Plugin Posts zu einem Thema und auf einer Social-Media-Webseite Ihrer Wahl\n\n\n\nNächste Schritte\n\n\nNutzen Sie das Zeeschuimer-Plugin um Post zu einem Thema und von einer der verfügbaren Social-Media-Plattform Ihrer Wahl zu sammeln.\nBitte beachten: Damit Sie die mit Zeeschuimer gesammelten Daten an den 🐈🐈 4CAT 🐈🐈 Server schicken können, müssen Sie\n\nsich vorher im selben Browser auf dem 🐈🐈 4CAT 🐈🐈 Server einloggen\nIm Feld 4CAT server URL folgendes eintragen: http://10.204.20.178:80\n\nAnalysieren Sie die Daten mit Hilfe von 🐈🐈 4CAT 🐈🐈.\n\nErstellen Sie eine Übersicht über den Posts im Zeitverlauf\nFinden Sie die Top Hashtags.\nDokumentieren Sie die Ergebnisse für Ihre Gruppe in der dafür vorgesehenen Folienvorlage (nächste Slide).\n\n\n\n\n\n\n−+\n25:00"
  },
  {
    "objectID": "slides/slides-07.html#let-the-work-beginn",
    "href": "slides/slides-07.html#let-the-work-beginn",
    "title": "📦 Data collection methods",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Präsentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-07.html#literatur",
    "href": "slides/slides-07.html#literatur",
    "title": "📦 Data collection methods",
    "section": "Literatur",
    "text": "Literatur\n\n\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-04.html#seminarplan",
    "href": "slides/slides-04.html#seminarplan",
    "title": "📚 Media routines & habits",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-04.html#kurzes-organisatorische-update",
    "href": "slides/slides-04.html#kurzes-organisatorische-update",
    "title": "📚 Media routines & habits",
    "section": "Kurzes organisatorische Update",
    "text": "Kurzes organisatorische Update\nInformationen zu Kursdetails und Prüfungsleistungen\n\n\nStudOn oder Zulip: Wo sollen Ihre Präsentationen hochgeladen werden?\n\n\n\n\nKooperation mit Kurs (bzw. der Übung) Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing\n\nHintergrund: Thematische Überschneidungen, deswegen Bündelung der Kompetenzen\n🔨 Text as data in R & 🔨 Topic Modeling in R werden auch für Studierende der Übung des Kurses angeboten\nKonsequenz: Sitzungen finden im PC-Pool in der Langen Gasse statt\nFrage: English ok?"
  },
  {
    "objectID": "slides/slides-04.html#mehr-als-nur-stimulus-response",
    "href": "slides/slides-04.html#mehr-als-nur-stimulus-response",
    "title": "📚 Media routines & habits",
    "section": "Mehr als nur Stimulus & Response",
    "text": "Mehr als nur Stimulus & Response\nDie Rolle der Medien\n\n\n\n\n\n\nMedia effects\n\n\nthe deliberate and nondeliberate short and long term within person changes in cognitions (including beliefs), emotions, attitudes, and behavior that result from media use\n(Valkenburg et al., 2016)\n\n\n\n\nDifferential Susceptibility to Media Effects Model (Valkenburg & Peter, 2013)"
  },
  {
    "objectID": "slides/slides-04.html#aber-was-ist-eigentlich-mediennutzung",
    "href": "slides/slides-04.html#aber-was-ist-eigentlich-mediennutzung",
    "title": "📚 Media routines & habits",
    "section": "Aber was ist eigentlich Mediennutzung?",
    "text": "Aber was ist eigentlich Mediennutzung?\nKombination aus kanal- und kommunikationsorientierter Ansatz\n\nThe hierarchical CMC taxonomy (Meier & Reinecke, 2021)\nAusganngspunkt: - Mediennutzung hat eine Wirkung bzw. Effekte (z.B. auf Emotionen, Einstellung und Verhalten)"
  },
  {
    "objectID": "slides/slides-04.html#viele-messwiederholung-in-kurzen-abständen",
    "href": "slides/slides-04.html#viele-messwiederholung-in-kurzen-abständen",
    "title": "📚 Media routines & habits",
    "section": "(Viele) Messwiederholung in (kurzen) Abständen",
    "text": "(Viele) Messwiederholung in (kurzen) Abständen\nIntensive Longitudinal Designs (IDL) im Fokus\n\n\n\n“an intensive longitudinal design involves sequential measurements on five or more occasions during which a change process is expected to unfold within each subject (e.g., person or other sampling)”\n(Bolger & Laurenceau, 2013)"
  },
  {
    "objectID": "slides/slides-04.html#layers-and-layers",
    "href": "slides/slides-04.html#layers-and-layers",
    "title": "📚 Media routines & habits",
    "section": "Layers and layers",
    "text": "Layers and layers\nIDL im Fokus: Zusammenhang der Ebenen"
  },
  {
    "objectID": "slides/slides-04.html#verschiedene-varianten-des-situationssamplings",
    "href": "slides/slides-04.html#verschiedene-varianten-des-situationssamplings",
    "title": "📚 Media routines & habits",
    "section": "Verschiedene Varianten des Situationssamplings",
    "text": "Verschiedene Varianten des Situationssamplings\nSystematisierung nach Masur (2019)"
  },
  {
    "objectID": "slides/slides-04.html#personen--undoder-situationsebene",
    "href": "slides/slides-04.html#personen--undoder-situationsebene",
    "title": "📚 Media routines & habits",
    "section": "Personen- und/oder Situationsebene?",
    "text": "Personen- und/oder Situationsebene?\nVerschiedene Arten von Forschungsfragen mit Beispiel\n\n\nFragen auf Personenebene (between-subject): Daten über Messzeitpunkte aggregiert\n\nPersonenmittelwert: Wie ist das durchschnittliche Wohlbefinden (Y) in der Personenstichprobe?\nVarianz der Personenmittelwerte: Welche Unterschiede im durchschnittlichen Wohlbefinden (Y) gibt es zwischen Personen?\nKorrelation auf Personenebene: Hängen Unterschiede im Wohlbefinden (Y) mit Unterschieden in der durchschnittlichen sozialen Interaktion (X) zusammen?\nKausalzusammenhang: Erklärt eine experimentelle Manipulation der sozialen Interaktion (X) die Unterschiede im Wohlbefinden (Y)?\n\nFragen auf Situationsebene (within-subject): Daten mehrerer Messzeitpunkte einer Person\n\nVarianz der Situationswerte: Wie stark weicht das situative Wohlbefinden (Y) vom Durchschnitt einer Person ab?\nKorrelation auf Situationsebene: Hängen diese Abweichungen im situativen Wohlbefinden (Y) mit situativen Unterschieden in der sozialen Interaktion (X) einer Person zusammen?\nGranger Kausalzusammenhang: Erklärt die soziale Interaktion einer Person in der Mitte des Tages (X) die Unterschiede im Wohlbefinden dieser Person am Ende des Tages (Y)?"
  },
  {
    "objectID": "slides/slides-04.html#just-one-more-scroll",
    "href": "slides/slides-04.html#just-one-more-scroll",
    "title": "📚 Media routines & habits",
    "section": "“Just one more scroll”",
    "text": "“Just one more scroll”\nMediengewohnheiten und Ihre Effekte\n\n\n\n\n\n\n\n\n\n\n\n\nProkrastination\n\n\n“the voluntary delay of an intended and necessary and/or [personally] impoortant activity, despite expecting potential negative consequences that outweight the positive consequences of the delay” Klingsieck (2013)\n\n\n\n\nklare Unterscheidung zu “strategischem Aufschieben”\nweit verbreitetes Phänomen\nZusammenhang mit Wohlbefinden\n\n\n\n\nVerzögerung ist - unnötig oder irrational - trotz Bewusstsein über mögliche negative Konsequenzen - resultiert in negativen Konsequenzen (“schlechtes Gefühl”)\nActivites == Aufgaben & Entscheidungen"
  },
  {
    "objectID": "slides/slides-04.html#design-your-own-research-design",
    "href": "slides/slides-04.html#design-your-own-research-design",
    "title": "📚 Media routines & habits",
    "section": "🧪 Design your own research (design)",
    "text": "🧪 Design your own research (design)\nGruppenarbeit (ca 25. Min) mit Ergebnisvorstellung & -diskussion (ca. 15 Min)\n\n\n\nZiel der Group Activity\n\n\nErstellung ein Forschungsdesign(konzept) zur Untersuchung des Einflusses von Mediennutzung(sroutinen) auf Wohlsein und/oder Procrastination im Rahmen dieses Kurses.\n\n\n\nNächste Schritte\n\n\nFinden Sie sich in Ihren Gruppen zusammen\nFormulieren Sie zwei Forschungsfragen (eine auf Personen- eine auf Situationsebene), die mit Hilfe eines ILD (siehe Systematik von Masur) untersucht werden kann. Nutzen Sie das CMC um die Mediennutzung möglichst genau zu operationalisieren.\nÜberlegen Sie, wie bzw. welche digitalen Verhaltensdaten Ihre Untersuchung unterstüzen könnten\nHalten Sie Ihre Fragen & Desginideen auf der für Ihre Gruppe vorgesehenen Folienvorlage (nächste Slide) fest."
  },
  {
    "objectID": "slides/slides-04.html#let-the-work-beginn",
    "href": "slides/slides-04.html#let-the-work-beginn",
    "title": "📚 Media routines & habits",
    "section": "Let the work beginn",
    "text": "Let the work beginn\nBitte nutzen Sie die Präsentationsvorlage Ihrer Gruppe\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-04.html#literatur",
    "href": "slides/slides-04.html#literatur",
    "title": "📚 Media routines & habits",
    "section": "Literatur",
    "text": "Literatur\n\n\nBolger, N., & Laurenceau, J.-P. (2013). Intensive longitudinal methods: An introduction to diary and experience sampling research. Guilford Press.\n\n\nKlingsieck, K. B. (2013). Procrastination: When Good Things Don’t Come to Those Who Wait. European Psychologist, 18(1), 24–34. https://doi.org/10.1027/1016-9040/a000138\n\n\nMasur, P. K. (2019). Capturing situational dynamics: Strength and pitfalls of the experience sampling method (P. Müller, S. Geiß, T. K. Naab, & C. Peter, Eds.; Vol. 15). Herbert von Halem Verlag. https://osf.io/vx5ha\n\n\nMeier, A., & Reinecke, L. (2021). Computer-Mediated Communication, Social Media, and Mental Health: A Conceptual and Empirical Meta-Review. Communication Research, 48(8), 1182–1209. https://doi.org/gjf96r\n\n\nValkenburg, P. M., & Peter, J. (2013). The Differential Susceptibility to Media Effects Model: Differential Susceptibility to Media Effects Model. Journal of Communication, 63(2), 221–243. https://doi.org/10.1111/jcom.12024\n\n\nValkenburg, P. M., Peter, J., & Walther, J. B. (2016). Media Effects: Theory and Research. Annual Review of Psychology, 67(1), 315–338. https://doi.org/10.1146/annurev-psych-122414-033608\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-11.html#schedule",
    "href": "slides/slides-11.html#schedule",
    "title": "Q&A",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis 🎥\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Automatic analysis of text in R\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-11.html#what-is-expected-update",
    "href": "slides/slides-11.html#what-is-expected-update",
    "title": "Q&A",
    "section": "What is expected (Update)",
    "text": "What is expected (Update)\nLeistungsanforderungen & Prüfungsleistungen\n\n\n\nFeedback?"
  },
  {
    "objectID": "slides/slides-11.html#fa-person-chalkboard-project-topic-ideas",
    "href": "slides/slides-11.html#fa-person-chalkboard-project-topic-ideas",
    "title": "Q&A",
    "section": " Project topic idea(s)",
    "text": "Project topic idea(s)\nProjektidee vorstellen & weiterentwickeln\n\nUmfang: maximal 10 Minuten & 5 Slides pro Gruppe\nZiel: Idee für Gruppenprojekt präsentieren, offene Fragen klären und Zeit für Diskussion & Feedback\nDeadline 31.01.\n\n\n\n\n\n\n     Gruppe A\n\n\n\n\n\n     Gruppe B\n\n\n\n\n\n     Gruppe C\n\n\n\n\n\n     Gruppe D"
  },
  {
    "objectID": "slides/slides-11.html#pitch-diskussion-repeat",
    "href": "slides/slides-11.html#pitch-diskussion-repeat",
    "title": "Q&A",
    "section": "Pitch ➞ Diskussion ➞ Repeat",
    "text": "Pitch ➞ Diskussion ➞ Repeat\nAblauf der Sitzung in der nächsten Woche\n\n\nProject topic idea(s) (ca 5-10 Min)\n\nkurzer Überblick über Thema, Forschungsfrage oder Motivation & ausgewählte Daten(teil)stichprobe (2 Folien),\nkurze Beschreibung von Methode und (geplanter) Analyse (1 Folie)\nErgebnisse und/oder eine Herausforderung aufzeigen, die im Kurs diskutiert werden soll(en) (2 Folien).\n\n\nFragen & Diskussion (ca 5-10 Min)\n\nZeit für Fragen, entweder von der Gruppe an den Kurs oder umgekehrt.\n\n\n\n\n🔁 für jede Gruppe\n\n\n\nStatus bzw. erster Ergebnisse der Project Proposal & Short Repots (max. 5 Folien)"
  },
  {
    "objectID": "slides/slides-11.html#fa-file-lines-project-proposal",
    "href": "slides/slides-11.html#fa-file-lines-project-proposal",
    "title": "Q&A",
    "section": " Project proposal",
    "text": "Project proposal\nErster Entwurf des short report\n\n\n\nUmfang: mindestens 500 Wörter\nZiel: Forschungsfrage (weiter-)entwickeln und verschriftlichen sowie frühzeitige Entwicklung einer spezifischen Analysestrategie\nDeadline 11.02.\nZusendung von Google Docs Vorlage"
  },
  {
    "objectID": "slides/slides-11.html#fa-comments-peer-review",
    "href": "slides/slides-11.html#fa-comments-peer-review",
    "title": "Q&A",
    "section": " Peer Review",
    "text": "Peer Review\nFeedback für Bericht geben & bekommmen\n\n\n\nUmfang: Schriftliches Feedback via Peer-Review-Formular\nZiel: Konstruktives Feedback schreiben\nDeadline: 18.02.\nVerwendung eines Google Forms"
  },
  {
    "objectID": "slides/slides-11.html#fa-file-signature-short-report",
    "href": "slides/slides-11.html#fa-file-signature-short-report",
    "title": "Q&A",
    "section": " Short Report",
    "text": "Short Report\nZusammenführung der einzelnen Teileistungen\n\n\n\nUmfang: Grupppenbericht zu Kursthema mit 1200 bis 2400 Wörter (abhängig von Gruppengröße)\nZiel(e): Anwendung vorgestellter Methoden & Daten\nDeadline: 10.03.\nVerwendung eines Quarto Journal Templates"
  },
  {
    "objectID": "slides/slides-11.html#a-work-in-pogress",
    "href": "slides/slides-11.html#a-work-in-pogress",
    "title": "Q&A",
    "section": "A work in pogress",
    "text": "A work in pogress\nStatus des Datensatzes: Mehr Vorarbeit, weniger Issues\n\nAktueller Fokus: Datensatzbereinigung und Komprimierung\nAktuelles Problem: Duplikatsidentifikation\nDokumentation der Überarbeitung wird zur Verfügung gestellt"
  },
  {
    "objectID": "slides/slides-11.html#fragen-über-fragen",
    "href": "slides/slides-11.html#fragen-über-fragen",
    "title": "Q&A",
    "section": "Fragen über Fragen",
    "text": "Fragen über Fragen\nR-Sitzungen & Kurseevaluation\n\nWie waren die praktischen R-Sitzungen für Sie?\nWie sind Sie zurechtgekomment mit …\n\nR/RStudio?\nden Inhalten (Textanalyse, Topic Modeling,etc)?\nEnglisch?\n\n\n. . .\n\nHaben Sie an der Kursevaluation teilgenommen?"
  },
  {
    "objectID": "slides/slides-11.html#literatur",
    "href": "slides/slides-11.html#literatur",
    "title": "Q&A",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-02.html#kurzes-update",
    "href": "slides/slides-02.html#kurzes-update",
    "title": "Einführung & Überblick",
    "section": "Kurzes Update",
    "text": "Kurzes Update\nAllgemeine Infos zum Kurs\n\n📖 Basisliteratur zu den Präsentationen auf StudOn verfügbar!\n⏰ Regelmäßige Kontrolle von Zulip\n🧮 Denken Sie an die Deadline für das Zertifikat vom R-Basiskurs (13.11.2023)\n🗣️ 1. Präsentationsgruppe: Denken Sie an die Zusendung des Entwurf der Präsentationsfolien und das Feedbackgespräch nächste Woche!\n\n\n\nZulip: Haben alle die Zuteilung der Themen/Gruppen gesehen?"
  },
  {
    "objectID": "slides/slides-02.html#die-würfel-sind-gefallen",
    "href": "slides/slides-02.html#die-würfel-sind-gefallen",
    "title": "Einführung & Überblick",
    "section": "Die Würfel sind gefallen",
    "text": "Die Würfel sind gefallen\nKurzer Überblick der Gruppenaufteilung\n\n\n\n\n\n\n\n  \n    \n    \n      Gruppe\n      Thema\n      Studierende\n    \n  \n  \n    A\n📚 Digital disconnection\nKofer, Rieger\n    B\n📦 Automatic text analysis (Topic Modeling & Netzwerkanalyse)\nKöbler, Mühlmeister, Neudecker\n    C\n📚 Media routines & habits\nBudak, Knapp, Kuck\n    D\n📦 Data collections methods (mit Schwerpunkt Data Donations)\nJakob, Neumeier"
  },
  {
    "objectID": "slides/slides-02.html#abstimmung-des-semesterplans",
    "href": "slides/slides-02.html#abstimmung-des-semesterplans",
    "title": "Einführung & Überblick",
    "section": "Abstimmung des Semesterplans",
    "text": "Abstimmung des Semesterplans\n\n\n\n\n\n\n\n\n  \n    \n       Option A\n    \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n📂 Project 1\n\nAnalysis of media content\n\n\n    4\n\n22.11.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    5\n\n29.11.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    6\n\n06.12.2023\n\n🔨 Text as data in R\n\nChristoph Adrian\n\n    7\n\n13.12.2023\n\n📊 Presentation & Discussion\n\nAll groups\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project 2\n\nAnalysis of media usage\n\n\n    9\n\n10.01.2024\n\n📚 Media routines & habits\n\nGroup C\n\n    10\n\n17.01.2024\n\n📦 Data collection methods\n\nGroup D\n\n    11\n\n24.01.2024\n\n🔨 Working data logs\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n    \n       Option B\n    \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    6\n\n06.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    7\n\n13.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    8\n\n20.12.2023\n\nBuffer Session\n\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-02.html#tldr-von-option-b",
    "href": "slides/slides-02.html#tldr-von-option-b",
    "title": "Einführung & Überblick",
    "section": "TL;DR von Option B",
    "text": "TL;DR von Option B\nZusammenfassung der Änderungen\n\n\nPräsentationen in 2023 & Projektarbeit in 2024\nnur ein Präsentationstermin mit “Project topic idea(s)”\nFokus auf Analyse von Medieninhalten (Project 1), keine Analyse der Logging-Daten (Project 2)\nAber (Optional): Project 2 light\n\nDurchführung einer Mini-Studie im Kurs (benötigt Einverständnis)\nFokus des Projektberichts auf Datenaufbereitung & -exploration\n\n\n\n\n\n\nFragen? Anmerkungen?"
  },
  {
    "objectID": "slides/slides-02.html#please-vote",
    "href": "slides/slides-02.html#please-vote",
    "title": "Einführung & Überblick",
    "section": "Please vote!",
    "text": "Please vote!\nWelche Option des Seminarplans bevorzugen Sie?\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/al3qwznms52z\nTemporary Access Code: 5623 9279\n\n\n\n\n\n\n\n    \n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/slides-02.html#ergebnis",
    "href": "slides/slides-02.html#ergebnis",
    "title": "Einführung & Überblick",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-02.html#was-ist-das-eigentlich",
    "href": "slides/slides-02.html#was-ist-das-eigentlich",
    "title": "Einführung & Überblick",
    "section": "Was ist das eigentlich?",
    "text": "Was ist das eigentlich?\nRückblick auf einen Definitionversuch von Weller (2021)\n\n\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen (häufig sogar “nur” als Nebenprodukt)\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "slides/slides-02.html#und-im-kontext-des-seminars",
    "href": "slides/slides-02.html#und-im-kontext-des-seminars",
    "title": "Einführung & Überblick",
    "section": "Und im Kontext des Seminars?",
    "text": "Und im Kontext des Seminars?\nArbeitsdefinition & Kernbereiche (GESIS) von DBD\n\n\n\n\nDBD umfasst digitale Beobachtungen menschlichen und algorithmischen Verhaltens,\nwie sie z.B. von Online-Plattformen (wie Google, Facebook oder dem World Wide Web) oder\nSensoren (wie Smartphones, RFID-Sensoren, Satelliten oder Street View-Kameras) erfasst werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchwerpunkt: Nutzung und Inhalte von soziale Medien\nComputational Social Science [CSS] Verfahren, z.B. zur Erhebung, Verarbeitung, Auswertung und Präsentation\n\n\n\n\n\n\n\nUnterschiedliche Heraus- bzw. Anforderungen (je nach Bereich)"
  },
  {
    "objectID": "slides/slides-02.html#css-dbd",
    "href": "slides/slides-02.html#css-dbd",
    "title": "Einführung & Überblick",
    "section": "CSS 🖇️ DBD",
    "text": "CSS 🖇️ DBD\nKurzer Exkurs zur Bedeutung von Computational Social Science\n\n\n\nDefinition (Computational Social Science).\nWe define CSS as the development and application of computational methods to complex, typically large-scale, human (sometimes simulated) behavioral data.” (Lazer et al., 2020)\n\n\n\nhilft dabei …\n\ngenuine digitale Phänomene zu untersuchen\ndigitale Verhaltensdaten zu sammeln und vorzuverarbeiten\nneue Methoden zur Analyse von großen Datensätzen anzuwenden\n\n\nCSS = neues Teilgebiet der Sozialwissenschaften oder neuer “Werkzeugkasten” zur Ergänzung der traditionellen sozialwissenschaftlichen Ansätze"
  },
  {
    "objectID": "slides/slides-02.html#von-verhalten-bis-interkation",
    "href": "slides/slides-02.html#von-verhalten-bis-interkation",
    "title": "Einführung & Überblick",
    "section": "Von Verhalten bis Interkation",
    "text": "Von Verhalten bis Interkation\nBeispiel für untersuchbare Phänomene samt Einschränkungen\n\n\n\n\n\nQuelle: (Keusch & Kreuter, 2021)\n\n\n\n\nEinschränkungen\n\nSelektive Nutzung von bestimmten digitalen Geräten bzw. Funktionen\nKategorisierung ist Momentaufnahme und nicht überschneidungsfrei\n\n\n\n\n\nEinige inhärent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)\nFehlen digitaler Spurendaten in all diesen Quadranten für bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Geräte."
  },
  {
    "objectID": "slides/slides-02.html#mehr-daten-durch-technologischen-fortschritt",
    "href": "slides/slides-02.html#mehr-daten-durch-technologischen-fortschritt",
    "title": "Einführung & Überblick",
    "section": "Mehr Daten durch technologischen Fortschritt",
    "text": "Mehr Daten durch technologischen Fortschritt\nBeispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren\n\n\nGraphik aus Struminskaya et al. (2020)"
  },
  {
    "objectID": "slides/slides-02.html#verfügbarkeit-als-pluspunkt",
    "href": "slides/slides-02.html#verfügbarkeit-als-pluspunkt",
    "title": "Einführung & Überblick",
    "section": "Verfügbarkeit als Pluspunkt",
    "text": "Verfügbarkeit als Pluspunkt\nDBD als wertvolle Quelle bei aktuellen, sensiblen & unvorhersehbaren Themen\n\nEinsatz besonders Vorteilhaft bei Themen bzw. Untersuchungen …\n\n… für die es schwierig ist, Studienteilnehmer*innen zu rekrutieren\n… bei denen Beobachtungen vorteilhafter sind als Befragungen\n\n\n\nBeispiel: Streaming und/oder Mining von Inhalten aus bestehenden digitalen Kommunikationsströmen\n\nZeitnaher als die Erstellung einer Umfrage\nZusätzlicher Nutzen als Archiv bei unvorhersehbaren Ereignissen\n\n\n\n🔔 weitere Beispiele? - Well-being auf Basis von Instagram-Bildern & Texten"
  },
  {
    "objectID": "slides/slides-02.html#die-power-von-social-sensing",
    "href": "slides/slides-02.html#die-power-von-social-sensing",
    "title": "Einführung & Überblick",
    "section": "Die Power von Social Sensing",
    "text": "Die Power von Social Sensing\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Zukunft: Linking"
  },
  {
    "objectID": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "href": "slides/slides-02.html#mit-fokus-auf-die-platform",
    "title": "Einführung & Überblick",
    "section": "Mit Fokus auf die Platform",
    "text": "Mit Fokus auf die Platform\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)"
  },
  {
    "objectID": "slides/slides-02.html#online-plattformen-prägen-die-gesellschaft",
    "href": "slides/slides-02.html#online-plattformen-prägen-die-gesellschaft",
    "title": "Einführung & Überblick",
    "section": "Online-Plattformen prägen die Gesellschaft",
    "text": "Online-Plattformen prägen die Gesellschaft\nGründe für den Fokus auf Onlineplattformen (Ulloa, 2021)\n\n\nvermitteln & formen menschliche Kommunikation (z.B. Tweet mit 280 Zeichen)\npolitische (Miss-)Nutzung\nGatekeeper für Informationen (z.B. “Dr.Google”)\ntägliche algorithmische Empfehlungen und Werbung: Nachrichten, Produkte, Jobangebote, Bewerbungen, Versicherungen, Hotels, …\n\n\n\nABER: Berücksichtigung der Art und Weise, wie Sie die Daten gesammelt werden!"
  },
  {
    "objectID": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "href": "slides/slides-02.html#der-weg-bestimmt-das-ergebnis",
    "title": "Einführung & Überblick",
    "section": "Der Weg bestimmt das Ergebnis",
    "text": "Der Weg bestimmt das Ergebnis\nEinfluss der Erhebung auf die Daten(-form) (Davidson et al., 2023)"
  },
  {
    "objectID": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "href": "slides/slides-02.html#wenn-der-vorteil-zum-nachteil-wird",
    "title": "Einführung & Überblick",
    "section": "Wenn der Vorteil zum Nachteil wird",
    "text": "Wenn der Vorteil zum Nachteil wird\nAmbivalenz der Unaufdringlichkeit (Keusch & Kreuter, 2021)\n\nUnterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookies, Browserplugins & APIs) erhobenen Daten\nBewertung und Erwartung an Datensammlung ist abhängig vom Kontext (z.B. Amazon vs. Researchgate)\n\n\n\nParadoxes Dilemma\nEinerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne ohne Wissen um deren Weiterverarbeitung, andererseits häufig Bedenken bezüglich Datenschutz & Privatsphäre bei wissenschaftlichen Studien, die über Verwendung der Daten aufklären.\n\n\nWarum? Persönlicher Nutzen?"
  },
  {
    "objectID": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "href": "slides/slides-02.html#eine-kleine-lobeshymne-auf-dbd",
    "title": "Einführung & Überblick",
    "section": "Eine kleine Lobeshymne auf DBD",
    "text": "Eine kleine Lobeshymne auf DBD\nZwischenfazit\n\nDigitale Geräte oder Sensoren können sich an bestimmte Fakten besser “erinnern” als das menschliche Gedächtnis.\nSensoren sind oft bereits in alltägliche Technologie eingebaut und produzieren digitale Verhaltensdaten als ein “Nebenprodukt”.\nUnaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung für Teilnehmer*Innen\nKombination mit Umfragedaten möglich (und bereichernd!)\n\n\n\nAber: Berücksichtigung der Rahmenbedingungen!\nZur erfolgreichen Nutzung müssen Forschungsziele & verfügbare Daten in Einklang gebracht, mögliche Biases und methodische Probleme berücksichtigt sowie die Datenqualität evaluiert werden.\n\n\nBietet die Plattform Zugang zu den benötigten Daten?\n\nWenn nicht, gibt es alternative Weg um an die Daten zu gelangen?\nWenn ja, ist dies legal/ethisch?"
  },
  {
    "objectID": "slides/slides-02.html#the-end-of-theory",
    "href": "slides/slides-02.html#the-end-of-theory",
    "title": "Einführung & Überblick",
    "section": "The End of Theory",
    "text": "The End of Theory\n\n\nZur Wichtigkeit von konzipierte Messungen & Designs\n\n\n\n“Who knows why people do what they do? The point is they do it, and we can track and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves.” (Anderson, 2008)\n\n\n\n\nWas denken Sie?\n\n\n\n“Size alone does not necessarily make the data better” (boyd & Ellison, 2007)\n\n\n“There are a lot of small data problems that occur in big data [which] don’t disappear because you’ve got lots of the stuff. They get worse.” (Harford, 2014)"
  },
  {
    "objectID": "slides/slides-02.html#we-need-to-talk-about-biases",
    "href": "slides/slides-02.html#we-need-to-talk-about-biases",
    "title": "Einführung & Überblick",
    "section": "We need to talk about biases",
    "text": "We need to talk about biases\nSpezifische und allgemeine Herausforderungen für die Forschung mit DBD\nHintergrund: (Big) Data ist zunehmend Grundlage für politische Maßnahmen, die Gestaltung von Produkten und Dienstleistungen und für die automatisierte Entscheidungsfindung\n\nHerausforderungen in Bezug auf DBD-Forschung: fehlender Konsens über ein Vokabular oder eine Taxonomie, häufig nur impliziter Bezug in der Forschung\nGenerelle Herausforderung: bias ist ein weit gefasster & in unterschiedlichen Disziplinen genutzter Begriff"
  },
  {
    "objectID": "slides/slides-02.html#was-verstehen-sie-unter-bias",
    "href": "slides/slides-02.html#was-verstehen-sie-unter-bias",
    "title": "Einführung & Überblick",
    "section": "Was verstehen Sie unter “bias”?",
    "text": "Was verstehen Sie unter “bias”?\nPlease participate!\n\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/al6nj5peoi88\nTemporary Access Code: 2359 9316\n\n\n\n\n\n\n\n    \n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/slides-02.html#ergebnis-1",
    "href": "slides/slides-02.html#ergebnis-1",
    "title": "Einführung & Überblick",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/slides-02.html#eher-konzept-als-begriff",
    "href": "slides/slides-02.html#eher-konzept-als-begriff",
    "title": "Einführung & Überblick",
    "section": "Eher Konzept als Begriff",
    "text": "Eher Konzept als Begriff\nZur Ambigutität des Begriffes bias und dessen Bedeutung im Seminar\n\n\nProblem: keine klare Grenzen zwischen den eher normativen Konnotationen (z.B. confirmation bias) und der statistischen Bedeutung des Begriffs (z.B. selection bias)\nDeswegen: Bewusstsein für Ambiguität des Begriffes\n\nVerwendung in vielen Disziplinen wie der Sozialwissenschaft, der kognitiven Psychologie oder dem Recht\nUntersuchung von verschiedenen Phänomenen, wie kognitive Voreingenommenheiten (Croskerry, 2002) sowie systemische, diskriminierende Ergebnisse (Friedman & Nissenbaum, 1996) oder Schäden (Barocas et al., 2017), aktuell z.B. bei der Verwendung von Machine Learning oder AI.\n\n\n\n\n\n\nVerwendung des Begriff hauptsächlich in seiner statistischen Bedeutung, um auf Verzerrungen in sozialen Daten und deren Analysen hinzuweisen."
  },
  {
    "objectID": "slides/slides-02.html#know-your-bias",
    "href": "slides/slides-02.html#know-your-bias",
    "title": "Einführung & Überblick",
    "section": "Know your bias!",
    "text": "Know your bias!\nFramework zur Minimierung von Fehlern und Problemen (Olteanu et al., 2019)\n\n\nBeschreibung:\n\nDie Analyse sozialer Daten beginnt mit bestimmten Zielen (Abschnitt 2.1), wie dem Verständnis oder der Beeinflussung von Phänomenen, die für soziale Plattformen spezifisch sind (Typ I) und/oder von Phänomenen, die über soziale Plattformen hinausgehen (Typ II).\nDiese Ziele erfordern, dass die Forschung bestimmte Validitätskriterien erfüllt, die weiter oben beschrieben wurden (Abschnitt 2.2).\nDiese Kriterien können ihrerseits durch eine Reihe von allgemeinen Verzerrungen und Problemen beeinträchtigt werden (Abschnitt 3).\nDiese Herausforderungen können von den Merkmalen der einzelnen Datenplattformen (Abschnitt 4) abhängen - die oft nicht unter der Kontrolle der Forschenden stehen - und von den Entscheidungen des Forschungsdesigns entlang einer Datenverarbeitungspipeline (Abschnitte 5 bis 8) - die oft unter der Kontrolle des Forschers stehen.\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken"
  },
  {
    "objectID": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "href": "slides/slides-02.html#the-biggest-problem-of-them-all",
    "title": "Einführung & Überblick",
    "section": "The biggest problem of them all",
    "text": "The biggest problem of them all\nPotentielle Probleme mit der Qualität der Daten\n\n\n\nDefinition (Data bias) (Olteanu et al., 2019)\nA systematic distortion in the sampled data that compromises its representativeness.\n\n\n\n\nSparsity: Häufig Heavy-Tail-Verteilung, was Analyse am “Kopf” (in Bezug auf häufige Elemente oder Phänomene) erleichtert, am “Schwanz” (wie seltene Elemente oder Phänomene) jedoch erschwert (Baeza-Yates, 2013)\nNoise: Unvollständige, beschädigte, unzuverlässige oder unglaubwürdige Inhalte (boyd & Crawford, 2012; Naveed et al., 2011)\n\nAber: Unterscheidung von “Noise” und “Signal” ist oft unklar und hängt von der Forschungsfrage ab (Salganik, 2018)\n\nOrganische vs gemessene Daten: Fragen zur Repräsentativität (vs. Stichprobenbeschreibung), Kausalität (vs. Korrelation) und Vorhersagegüte"
  },
  {
    "objectID": "slides/slides-02.html#bias-at-the-source",
    "href": "slides/slides-02.html#bias-at-the-source",
    "title": "Einführung & Überblick",
    "section": "Bias at the source",
    "text": "Bias at the source\nPotentielle Probleme mit der Datenquelle oder -herkunft\n\nBiases, die auf das Design und die Möglichkeiten der Plattformen zurückzuführen sind (functional biases).\nVerhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).\nFaktoren, die außerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen können (external biases)\nVorhandensein von nicht-individuellen Konten ein (non-individuals).\n\n\nfunctional biases:\n- Platform-specific design and features shape user behavior (z.B. Emojis) - Algorithms used for organizing and ranking content influence user behavior - Content presentation influences user behavior (z.B. UI)\nnormative biases:\n\nNorms are shaped by the attitudes and behaviors of online communities, which may be context-dependent (z.B. Partyfotos auf Instagram, aber nicht LinkedIn)\nThe awareness of being observed by others impacts user behavio (Anonymität vs Klarnamen)\nSocial conformity and “herding” happen in social platforms, and such behavioral traits shape user behavior (z.B. Ratings beinflussen eigenes Rating)\n\nexternal biase:\n\nCultural elements and social contexts are reflected in social datasets. (Zeichenlimit Japan vs. Deutschland)\nMisinformation and disinformation.\nContents on different topics are treated differently.\nHigh-impact events, whether anticipated or not, are reflected on social media (z.B. Feiertage)\n\nnon-individual-accounts: Organizational accounts, Bots"
  },
  {
    "objectID": "slides/slides-02.html#gefangen-im-spannungsverhältnis",
    "href": "slides/slides-02.html#gefangen-im-spannungsverhältnis",
    "title": "Einführung & Überblick",
    "section": "Gefangen im Spannungsverhältnis",
    "text": "Gefangen im Spannungsverhältnis\nForschungethik bei digitalen Daten\nHintergrund: Die Herausforderung besteht in der Kombination von zwei extremen Sichtweisen, der Betrachtung der Forschung mit sozialen Daten als “klinische” Forschung oder als Computerforschung\n\nDie Sozialdatenforschung unterscheidet sich von klinischen Versuchen.\nEthische Entscheidungen in der Sozialdatenforschung müssen gut überlegt sein, da oft sind mehrere Werte betroffen, die miteinander in Konflikt stehen können\nDiskussion des Spannungsverhältnisses am Beispiel von drei spezifischer ethischer Kriterien: Autonomie, Wohltätigkeit und Gerechtigkeit\n\n\nHintergrund:\n\nDie Sozialdatenforschung ähnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer Fähigkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden\ndie Sozialdatenforschung ähnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.\n\nPunkt 2: Schäden, die die üblichen Arten der Sozialdatenforschung ( z. B. die Verletzung der Privatsphäre oder der Anblick verstörender Bilder)verursachen können, oft nicht mit Schäden von klinischen Versuchen gleichzusetzen\nPunkt 3: Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten Lösungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015)."
  },
  {
    "objectID": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "href": "slides/slides-02.html#achtung-der-individuellen-autonomie",
    "title": "Einführung & Überblick",
    "section": "Achtung der individuellen Autonomie",
    "text": "Achtung der individuellen Autonomie\nDiskussion der Informierte Zustimmung als Indikator autonomer Entscheidung\n\n\n\n\n\n\nEinwilligung nach Aufklärung setzt voraus, dass\n\n\n\n\ndie Forscher*Innen den potenziellen Teilnehmenden alle relevanten Informationen offenlegen;\ndie potenziellen Teilnehmenden in der Lage sind, diese Informationen zu bewerten;\ndie potenziellen Teilnehmenden freiwillig entscheiden können, ob sie teilnehmen wollen oder nicht;\ndie Teilnehmenden den Forschernden ihre ausdrückliche Erlaubnis erteilen, häufig in schriftlicher Form; und\ndie Teilnehmende die Möglichkeit haben, ihre Einwilligung jederzeit zurückzuziehen.\n\n\n\n\n\nPotentielle Probleme mit Blick auf DBD\n\nDie Zustimmung von Millionen von Nutzern einzuholen ist nicht praktikabel.\nDie Nutzungsbedingungen sozialer Plattformen stellen möglicherweise keine informierte Zustimmung zur Forschung dar.\nDas öffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung."
  },
  {
    "objectID": "slides/slides-02.html#no-no-yes",
    "href": "slides/slides-02.html#no-no-yes",
    "title": "Einführung & Überblick",
    "section": "No “No” ≠ “Yes”!",
    "text": "No “No” ≠ “Yes”!\nEthische Erwägungen bei DBD-Forschung\n\nAus öffentlicher Zugänglich- bzw. Verfügbarkeit von Daten leitet sich nicht automatisch ethische Verwertbarkeit ab (boyd & Crawford, 2012; Zimmer, 2010)\n\nVerletzung der Privatsphäre der Nutzer (Goroff, 2015)\nErmöglichung von rassischem, sozioökonomischem oder geschlechtsspezifischem Profiling (Barocas & Selbst, 2016)\n\nNegative Beispiele\n\nFacebook contagion experiment (2012-2014): Feeds von Nutzer*Innen so manipulierten, dass sie je nach den geäußerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014)\nEncore-Forschungsprojekt: Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett & Feamster, 2014)\n\n\n\nHintergrund:\n\nEthische Fragen bisher epistemische Bedenken (Verwendung von nicht schlüssigen oder fehlgeleiteten Beweisen), jetzt normativ Bedenken (Folgen der Forschung)\nForschung grundsätzlich in vielen Ländern gesetztlich geregelt\n\nNegativbeispiele:\n\nFacebook contagion experiment: Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a).\nEncore-Forschngsprojekt: Menschen in einigen Ländern durch diese Zugriffsversuche möglicherweise gefährdet wurden\n\nFolgende Abschnitte:\n\nzentrales Spannungsverhältnis in der Forschungsethik digitaler Daten dargestellt.\nAnschließend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohltätigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4)."
  },
  {
    "objectID": "slides/slides-02.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "href": "slides/slides-02.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "title": "Einführung & Überblick",
    "section": "Wohltätigkeit und Unschädlichkeit als Ziel",
    "text": "Wohltätigkeit und Unschädlichkeit als Ziel\nBewertung von Risken & Nutzen\nHintergrund: Nicht nur Fokus auf den Nutzen der Forschung, sondern auch auf die möglichen Arten von Schäden, die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden können . (Sweeney, 2013)\n\nPotentielle Probleme\n\nDaten über Einzelpersonen können ihnen schaden, wenn sie offengelegt werden.\nForschungsergebnisse können verwendet werden, um Schaden anzurichten.\n“Dual-Use”- und Sekundäranalysen sind in der Sozialdatenforschung immer häufiger anzutreffen.\n\n\nDie Forschung zu sozialen Daten wird mit bestimmten Arten von Schäden in Verbindung gebracht, von denen die Verletzung der Privatsphäre vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).\nBeispiel 1: Einige prominente Beispiele sind die Datenpanne bei Ashley Madison im Jahr 2015, bei der einer Website, die sich als Dating-Netzwerk für betrügerische Ehepartner anpreist, Kontoinformationen (einschließlich der vollständigen Namen der Nutzer) gestohlen und online gestellt wurden (Thomsen, 2015), sowie die jüngsten Datenpannen bei Facebook, bei denen Hunderte Millionen von Datensätzen mit Kommentaren, Likes, Reaktionen, Kontonamen, App-Passwörtern und mehr öffentlich gemacht wurden.\nzu 1: - Stalking, Diskriminierung, Erpressung oder Identitätsdiebstahl (Gross und Acquisti, 2005). - Zu lange Archivierung personenbezogener Daten oder die öffentliche Freigabe schlecht anonymisierter Datensätze kann zu Verletzungen der Privatsphäre führen, da diese Daten mit anderen Quellen kombiniert werden können, um Erkenntnisse über Personen ohne deren Wissen zu gewinnen (Crawford und Finn, 2014; Goroff, 2015; Horvitz und Mulligan, 2015)\nzu 2: Abgesehen von der Tatsache, dass aus sozialen Daten gezogene Rückschlüsse in vielerlei Hinsicht falsch sein können, wie in dieser Studie hervorgehoben wird, können zu präzise Rückschlüsse dazu führen, dass Menschen in immer kleinere Gruppen eingeteilt werden können (Barocas, 2014).\nzu 3: Daten, Instrumente und Schlussfolgerungen, die für einen bestimmten Zweck gewonnen wurden, für einen anderen Zweck verwendet werden (Hovy und Spruit, 2016; Benton et al., 2017)"
  },
  {
    "objectID": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "href": "slides/slides-02.html#faire-verteilung-von-risiken-nutzen",
    "title": "Einführung & Überblick",
    "section": "Faire Verteilung von Risiken & Nutzen",
    "text": "Faire Verteilung von Risiken & Nutzen\nRecht & Gerechtigkeit\nHintergrund: Häufig wird unterstellt bzw. angenommen, dass es von Anfang an bekannt, wer durch die Forschung belastet und wer von den Ergebnissen profitieren wird.\n\nPotentielle Probleme\n\nDie digitale Kluft kann das Forschungsdesign beeinflussen (z.B. WEIRD Samples)\nAlgorithmen und Forschungsergebnisse können zu Diskriminierung führen.\nForschungsergebnisse sind möglicherweise nicht allgemein zugänglich.\nNicht alle Interessengruppen werden über die Verwendung von Forschungsergebnissen konsultiert.\n\n\nzu 1: Data divide: mangelnde Verfügbarkeit von hochwertigen Daten über Entwicklungsländer und unterprivilegierte Gemeinschaften (Cinnamon und Schuurman, 2013). WEIRD = White, Educated, Industrialized, Rich, and Democratic\nzu 3: Idealerweise sollten die Menschen Zugang zu den Forschungsergebnissen und Artefakten haben, die aus der Untersuchung ihrer persönlichen Daten entstanden sind (Gross und Acquisti, 2005; Crawford und Finn, 2014).\nzu 4: In die Überlegungen darüber, wie, für wen und wann Forschungsergebnisse umgesetzt werden, sollten diejenigen einbezogen werden, die möglicherweise betroffen sind oder deren Daten verwendet werden (Costanza-Chock, 2018; Design Justice, 2018; Green, 2018)"
  },
  {
    "objectID": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "href": "slides/slides-02.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "title": "Einführung & Überblick",
    "section": "Zwei Trends, Drei Fragen, Vier Empfehlungen",
    "text": "Zwei Trends, Drei Fragen, Vier Empfehlungen\nZusammenfassung und Ausblick\nTrend 1: Skepsis gegenüber einfachen Antworten\n\n\nWie einstehen die Daten, was enthalten sie tatsächlich und wie sind die Arbeitsdatensätze zusammengestellt?\nWird deutlich, was ausgewertet wird?\nWird die Verwendung von vorhandenen Datensätzen und Modellen des maschinellen Lernens hinterfragt?\n\n\nTrend 2: Wechsel von der Thematisierung zur Adressieung von Bedenken\n\n\nDetaillierte Dokumentation und kritische Prüfung der Datensatz- und Modellerstellung\nDBD-Studien auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden\nTransparenzmechanismen zu schaffen, die es ermöglichen, Online-Plattformen zu überprüfen und Verzerrungen in Daten an der Quelle zu evaluieren\nForschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre Übernahme zu fördern.\n\n\n\nSchließlich gibt es angesichts der Komplexität der inhärent kontextabhängigen, anwendungs- und bereichsabhängigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitslösungen - bei der Bewertung und Bekämpfung von Verzerrungen ist Nuancierung entscheidend."
  },
  {
    "objectID": "slides/slides-02.html#literatur",
    "href": "slides/slides-02.html#literatur",
    "title": "Einführung & Überblick",
    "section": "Literatur",
    "text": "Literatur\n\n\nAnderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. https://www.wired.com/2008/06/pb-theory/\n\n\nBaeza-Yates, R. A. (2013). Big data or right data.\n\n\nBarocas, S., Crawford, K., Shapiro, A., & Wallach, H. (2017). The problem with bias: From allocative to representational harms in machine learning. Special interest group for computing. Information and Society (SIGCIS), 2.\n\n\nBarocas, S., & Selbst, A. D. (2016). Big Data’s Disparate Impact. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2477899\n\n\nboyd, danah m., & Crawford, K. (2012). Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, Communication & Society, 15(5), 662–679. https://doi.org/10.1080/1369118X.2012.678878\n\n\nboyd, danah m., & Ellison, N. B. (2007). Social Network Sites: Definition, History, and Scholarship. Journal of Computer-Mediated Communication, 13(1), 210–230. https://doi.org/10.1111/j.1083-6101.2007.00393.x\n\n\nBurnett, S., & Feamster, N. (2014). Encore: Lightweight measurement of web censorship with cross-origin requests. https://doi.org/10.48550/ARXIV.1410.1211\n\n\nCroskerry, P. (2002). Achieving Quality in Clinical Decision Making: Cognitive Strategies and Detection of Bias. Academic Emergency Medicine, 9(11), 1184–1204. https://doi.org/10.1197/aemj.9.11.1184\n\n\nDavidson, B. I., Wischerath, D., Racek, D., Parry, D. A., Godwin, E., Hinds, J., Linden, D. van der, Roscoe, J. F., Ayravainen, L. E. M., & Cork, A. (2023). Platform-controlled social media APIs threaten open science. http://dx.doi.org/10.31234/osf.io/ps32z\n\n\nFlöck, F., & Sen, I. (2022). Digital traces of human behaviour in online platforms  research design and error sources. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meet_the_experts_Digitaltraces_humanbehaviour.pdf\n\n\nFriedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems, 14(3), 330–347. https://doi.org/10.1145/230538.230561\n\n\nGoroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. Science, 347(6221), 479–480. https://doi.org/10.1126/science.aaa3483\n\n\nHarford, T. (2014). Big data: A big mistake? Significance, 11(5), 14–19. https://doi.org/10.1111/j.1740-9713.2014.00778.x\n\n\nKeusch, F., & Kreuter, F. (2021). Digital trace data. In Handbook of Computational Social Science, Volume 1 (1st ed., pp. 100–118). Routledge. https://doi.org/10.4324/9781003024583-8\n\n\nKramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788–8790. https://doi.org/10.1073/pnas.1320040111\n\n\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060–1062. https://doi.org/10.1126/science.aaz8170\n\n\nNaveed, N., Gottron, T., Kunegis, J., & Alhadi, A. C. (2011). the 20th ACM international conference. 183. https://doi.org/10.1145/2063576.2063607\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\nStruminskaya, B., Lugtig, P., Keusch, F., & Höhne, J. K. (2020). Augmenting Surveys With Data From Sensors and Apps: Opportunities and Challenges. Social Science Computer Review, 089443932097995. https://doi.org/10.1177/0894439320979951\n\n\nSweeney, L. (2013). Discrimination in Online Ad Delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue, 11(3), 10–29. https://doi.org/10.1145/2460276.2460278\n\n\nUlloa, R. (2021). Introduction to online data acquisition. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZimmer, M. (2010). “But the data is already public”: on the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313–325. https://doi.org/10.1007/s10676-010-9227-5\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/slides-08.html#seminarplan",
    "href": "slides/slides-08.html#seminarplan",
    "title": "📦 Automatic text analysis",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n  \n    \n    \n      Session\n      Datum\n      Topic\n      Presenter\n    \n  \n  \n    \n\nIntroduction\n\n\n    1\n\n25.10.2023\n\nKick-Off\n\nChristoph Adrian\n\n    \n01.11.2023\n\n🎃 Holiday (No Lecture)\n\n\n    2\n\n08.11.2023\n\nEinführung in DBD\n\nChristoph Adrian\n\n    3\n\n15.11.2023\n\n🔨 Working with R\n\nChristoph Adrian\n\n    \n🗣️\n\nPresentations\n\n\n    4\n\n22.11.2023\n\n📚 Media routines & habits\n\nGroup C\n\n    5\n\n29.11.2023\n\n📚 Digital disconnection\n\n\n    6\n\n06.12.2023\n\n📚 Digital disconnection\n\nGroup A\n\n    7\n\n13.12.2023\n\n📦 Data collection methods\n\nGroup D\n\n    8\n\n20.12.2023\n\n📦 Automatic text analysis\n\nGroup B\n\n    \n\n🎄Christmas Break (No Lecture)\n\n\n    \n📂 Project\n\nAnalysis of media content\n\n\n    9\n\n10.01.2024\n\n🔨 Text as data\n\nChristoph Adrian\n\n    10\n\n17.01.2024\n\n🔨 Topic Modeling\n\nChristoph Adrian\n\n    11\n\n24.01.2024\n\n🔨 Q&A\n\nChristoph Adrian\n\n    12\n\n31.01.2024\n\n📊 Presentation & Discussion\n\nAll groups\n\n    13\n\n07.02.2024\n\n🏁 Recap, Evaluation & Discussion\n\nChristoph Adrian"
  },
  {
    "objectID": "slides/slides-08.html#kurzes-organisatorisches-update",
    "href": "slides/slides-08.html#kurzes-organisatorisches-update",
    "title": "📦 Automatic text analysis",
    "section": "Kurzes organisatorisches Update",
    "text": "Kurzes organisatorisches Update\nInformationen zu praktischen Sessions\n\n\n\n⏰ Reminder\n\n\nNächsten zwei Sitzungen (10.1. & 17.01) in Koorperatin mit Kurs Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing\n\n\n\n\nVortrag auf Englisch\nim CIP-Pool LG 0.215\nentweder zur Zeit des Seminars (11:30 - 13:00) oder im Zeitslot Ihrer eigenen Übung"
  },
  {
    "objectID": "slides/slides-08.html#zeit-für-feedback-und-anmerkungen",
    "href": "slides/slides-08.html#zeit-für-feedback-und-anmerkungen",
    "title": "📦 Automatic text analysis",
    "section": "Zeit für Feedback und Anmerkungen",
    "text": "Zeit für Feedback und Anmerkungen\nEvaluation der Veranstaltung Digital behavioral data\n\n\nBitte nehmen Sie über den QR Code oder folgenden Link an der Evaluation teil:\n\nLink: eva.fau.de\nLosung: WADPG"
  },
  {
    "objectID": "slides/slides-08.html#literatur",
    "href": "slides/slides-08.html#literatur",
    "title": "📦 Automatic text analysis",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "sessions/session-03.html",
    "href": "sessions/session-03.html",
    "title": "Session 3",
    "section": "",
    "text": "✍️ Dealine (13.11.2023) für Kurszertifikat des Basiskurs R/RStudio. Bitte via E-Mail an christoph.adrian@fau.de\n🔍 Checken Sie die Ressourcen unter Computing (z.B. R-Textbooks & R-Cheatsheets)"
  },
  {
    "objectID": "sessions/session-03.html#prepare",
    "href": "sessions/session-03.html#prepare",
    "title": "Session 3",
    "section": "",
    "text": "✍️ Dealine (13.11.2023) für Kurszertifikat des Basiskurs R/RStudio. Bitte via E-Mail an christoph.adrian@fau.de\n🔍 Checken Sie die Ressourcen unter Computing (z.B. R-Textbooks & R-Cheatsheets)"
  },
  {
    "objectID": "sessions/session-03.html#participate",
    "href": "sessions/session-03.html#participate",
    "title": "Session 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 03"
  },
  {
    "objectID": "sessions/session-03.html#practice",
    "href": "sessions/session-03.html#practice",
    "title": "Session 3",
    "section": "Practice",
    "text": "Practice\n🔧 Quarto installieren und ausprobieren.\n\n\n\n\n\n\nUseful resource\n\n\n\nSehr hilfreiches  Quarto Video Tutorial von Andy Field\n\n\n📋 Exercise 03 - Hollywood Age Gap"
  },
  {
    "objectID": "sessions/session-03.html#suggested-readings",
    "href": "sessions/session-03.html#suggested-readings",
    "title": "Session 3",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBauer, P. C., & Landesvatter, C. (2023). Writing a reproducible paper with RStudio and quarto. https://osf.io/ur4xn\nJonge, E. de, & Loo, M. van der (2013). An introduction to data cleaning with R.\nPearson, R. K. (2018). Exploratory data analysis using r. CRC Press/Taylor & Francis Group."
  },
  {
    "objectID": "sessions/session-03.html#useful-resources",
    "href": "sessions/session-03.html#useful-resources",
    "title": "Session 3",
    "section": "Useful resources",
    "text": "Useful resources\n📖 Nützliches und ausführliches Tutorial zur Nutzung von Git/Github in RStudio\n📖 Happy Git and GitHub for the useR\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-01.html",
    "href": "sessions/session-01.html",
    "title": "Session 1",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure that you answered the survey send to you by (StudOn) mail."
  },
  {
    "objectID": "sessions/session-01.html#prepare",
    "href": "sessions/session-01.html#prepare",
    "title": "Session 1",
    "section": "Prepare",
    "text": "Prepare\n✍️ Fill out the short survey before the first session (see  StudOn for Link)\n📨 Join  Zulip using the invitation (please check your email address used for StudOn).\n📖 Read the syllabus"
  },
  {
    "objectID": "sessions/session-01.html#participate",
    "href": "sessions/session-01.html#participate",
    "title": "Session 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 01 - Kick Off\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-10.html",
    "href": "sessions/session-10.html",
    "title": "Session 10",
    "section": "",
    "text": "🖥️ Session 10\n🧷 Showcase"
  },
  {
    "objectID": "sessions/session-10.html#participate",
    "href": "sessions/session-10.html#participate",
    "title": "Session 10",
    "section": "",
    "text": "🖥️ Session 10\n🧷 Showcase"
  },
  {
    "objectID": "sessions/session-10.html#practice",
    "href": "sessions/session-10.html#practice",
    "title": "Session 10",
    "section": "Practice",
    "text": "Practice\n📋 Exercise 10"
  },
  {
    "objectID": "sessions/session-10.html#suggested-readings",
    "href": "sessions/session-10.html#suggested-readings",
    "title": "Session 10",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nSilge, E. H., & Julia (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-07.html",
    "href": "sessions/session-07.html",
    "title": "Session 7",
    "section": "",
    "text": "🔨 Please install the 🏴‍☠️ Zeeschuimer browser plugin (only Firefox-based browsers) by following the instructions from the Github page.\n🔨 Please test your 🐈🐈 4CAT 🐈🐈 login details as soon as possible after receiving them via Zulip."
  },
  {
    "objectID": "sessions/session-07.html#prepare",
    "href": "sessions/session-07.html#prepare",
    "title": "Session 7",
    "section": "",
    "text": "🔨 Please install the 🏴‍☠️ Zeeschuimer browser plugin (only Firefox-based browsers) by following the instructions from the Github page.\n🔨 Please test your 🐈🐈 4CAT 🐈🐈 login details as soon as possible after receiving them via Zulip."
  },
  {
    "objectID": "sessions/session-07.html#participate",
    "href": "sessions/session-07.html#participate",
    "title": "Session 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Session 07\n🌐 🐈🐈 4CAT 🐈🐈 Server"
  },
  {
    "objectID": "sessions/session-07.html#mandatory-literature",
    "href": "sessions/session-07.html#mandatory-literature",
    "title": "Session 7",
    "section": "Mandatory literature",
    "text": "Mandatory literature\n\n\n\n\n\n\nAll articles of this section have to be read & used by the presenting group\n\n\n\n\nBaumgartner, S. E., Sumter, S. R., Petkevič, V., & Wiradhany, W. (2022). A Novel iOS Data Donation Approach: Automatic Processing, Compliance, and Reactivity in a Longitudinal Study. Social Science Computer Review, 089443932110710. https://doi.org/10.1177/08944393211071068\nOhme, J., Araujo, T., Boeschoten, L., Freelon, D., Ram, N., Reeves, B. B., & Robinson, T. N. (2023). Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking. Communication Methods and Measures, 1–18. https://doi.org/10.1080/19312458.2023.2181319\nSkatova, A., & Goulding, J. (2019). Psychology of personal data donation. PLOS ONE, 14(11), e0224240. https://doi.org/10.1371/journal.pone.0224240\nDriel, I. I. van, Giachanou, A., Pouwels, J. L., Boeschoten, L., Beyens, I., & Valkenburg, P. M. (2022). Promises and Pitfalls of Social Media Data Donations. Communication Methods and Measures, 1–17. https://doi.org/10.1080/19312458.2022.2109608\nYee, A. Z. H., Yu, R., Lim, S. S., Lim, K. H., Dinh, T. T. A., Loh, L., Hadianto, A., & Quizon, M. (2022). ScreenLife capture: An open-source and user-friendly framework for collecting screenome data from android smartphones. https://osf.io/sphq4\n\n\n🐈🐈 4CAT 🐈🐈-Toolkit\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://zenodo.org/record/6826877\n\n\n\nOther additional readings\n\nBreuer, J., Kmetty, Z., Haim, M., & Stier, S. (2022). User-centric approaches for collecting Facebook data in the ‘post-API age’: experiences from two studies and recommendations for future research. Information, Communication & Society, 1–20. https://doi.org/10.1080/1369118X.2022.2097015\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Application programming interfaces and web data for social research (1st ed., pp. 33–45). Routledge. https://www.taylorfrancis.com/books/9781003025245/chapters/10.4324/9781003025245-4\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). A brief history of APIs (1st ed., pp. 17–32). Routledge. https://www.taylorfrancis.com/books/9781003025245/chapters/10.4324/9781003025245-3\nJünger, J. (2022). Verhaltens-, Forschungs- oder Datenschnittstellen?: Application Programming Interfaces (APIs) aus diachron und synchron vergleichender Perspektive. https://doi.org/10.48541/DCR.V10.6\nLomborg, S., & Bechmann, A. (2014). Using APIs for Data Collection on Social Media. The Information Society, 30(4), 256–265. https://doi.org/10.1080/01972243.2014.915276\nOhme, J., Araujo, T., Vreese, C. H. de, & Piotrowski, J. T. (2021). Mobile data donations: Assessing self-report accuracy and sample biases with the iOS Screen Time function. Mobile Media & Communication, 9(2), 293–313. https://doi.org/10.1177/2050157920959106\nPuschmann, C., & Ausserhofer, J. (2017). Social data APIs (M. T. Schäfer & K. Es, van, Eds.). Amsterdam University Press. http://en.aup.nl/books/9789462981362-the-datafied-society.html\nSloan, L., & Quan-Haase, A. (2016). The role of APIs in data sampling from social media (pp. 146–160). SAGE Publications Ltd. http://methods.sagepub.com/book/the-sage-handbook-of-social-media-research-methods/i1486.xml\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/session-08.html",
    "href": "sessions/session-08.html",
    "title": "Session 8",
    "section": "",
    "text": "All articles of this section have to be read & used by the presenting group\n\n\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#mandatory-literature",
    "href": "sessions/session-08.html#mandatory-literature",
    "title": "Session 8",
    "section": "",
    "text": "All articles of this section have to be read & used by the presenting group\n\n\n\n\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754"
  },
  {
    "objectID": "sessions/session-08.html#other-additional-readings",
    "href": "sessions/session-08.html#other-additional-readings",
    "title": "Session 8",
    "section": "Other additional readings",
    "text": "Other additional readings\n\nTopic Modeling\n\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288–296.\n\n\n\nTwitter\n\nBanda, J. M., Tekumalla, R., Wang, G., Yu, J., Liu, T., Ding, Y., Artemova, E., Tutubalina, E., & Chowell, G. (2021). A Large-Scale COVID-19 Twitter Chatter Dataset for Open Scientific ResearchAn International Collaboration. Epidemiologia, 2(3), 315–324. https://doi.org/10.3390/epidemiologia2030024\nJungherr, A. (2016). Twitter use in election campaigns: A systematic literature review. Journal of Information Technology & Politics, 13(1), 72–91. https://doi.org/10.1080/19331681.2015.1132401\nKarami, A., Lundy, M., Webb, F., & Dwivedi, Y. K. (2020). Twitter and research: A systematic literature review through text mining. IEEE Access, 8, 67698–67717. https://doi.org/10.1109/ACCESS.2020.2983656\nMueller, S. D., & Saeltzer, M. (2022). Twitter made me do it! Twitter’s tonal platform incentive and its effect on online campaigning. Information, Communication & Society, 25(9), 1247–1272. https://doi.org/10.1080/1369118X.2020.1850841\nMünch, F. V., Thies, B., Puschmann, C., & Bruns, A. (2021). Walking Through Twitter: Sampling a Language-Based Follow Network of Influential Twitter Accounts. Social Media + Society, 7(1), 205630512098447. https://doi.org/10.1177/2056305120984475\nVasko, V., & Trilling, D. (2019). A permanent campaign? Tweeting differences among members of Congress between campaign and routine periods. Journal of Information Technology & Politics, 16(4), 342–359. https://doi.org/10.1080/19331681.2019.1657046\nYuan, S., Chen, Y., Vojta, S., & Chen, Y. (2022). More aggressive, more retweets? Exploring the effects of aggressive climate change messages on Twitter. New Media & Society, 146144482211222. https://doi.org/10.1177/14614448221122202\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "exercise/exercise-09.html",
    "href": "exercise/exercise-09.html",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-09.html#background",
    "href": "exercise/exercise-09.html#background",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-09.html#preparation",
    "href": "exercise/exercise-09.html#preparation",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-09.html#import-and-process-the-data",
    "href": "exercise/exercise-09.html#import-and-process-the-data",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/exercise-09.html#exercises",
    "href": "exercise/exercise-09.html#exercises",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrush up basic knowledge of working with R, tidyverse and ggplot2\nGet to know the typical steps of tidy text analysis with tidytext, from tokenisation and summarisation to visualisation.\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the “Run all chunks above”-button  of the next chunk.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\n📋 Exercise 1: Transform to ‘tidy text’\n\nDefine custom stopwords\n\nEdit the vector remove_custom_stopwords by defining additional words to be deleted based on the analysis presented in the session. (Use | as a logical OR operator in regular expressions. It allows you to specify alternatives, e.g. add |https.)\n\nCreate new dataset tweets_tidy_cleaned\n\nBased on the dataset tweets_detox,\n\nuse the str_remove_all function to remove the specified patterns of the remove_custom_stopwords vector. Use the mutate() function to edit the text variable.\nTokenize the ‘text’ column using unnest_tokens.\nFilter out stop words using filter and stopwords$words\n\nSave this transformation by creating a new dataset with the name tweets_tidy_cleaned.\n\nCheck if transformation was successful (e.g. by using the print() function)\n\n\n#| eval: false\n\n# Define custom stopwords\nremove_custom_stopwords &lt;- \"&amp;|&lt;|&gt; ADD NEW TERMS HERE\"\n\n# Create new dataset tweets_tidy_cleaned\n\n# Check\n\n\n\n📋 Exercise 2: Summarize tokens\n\nCreate summarized data\n\nBased on the dataset tweets_tidy_cleaned, summarize the frequency of the individual tokens by using the count()-function on the variable text. Use the argument sort = TRUE to sort the dataset based on descending frequency of the tokens.\nSave this transformation by creating a new dataset with the name tweets_summarized_cleaned.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nOptional: Check results with a wordcloud\n\nBased on the sorted dataset tweets_summarized_cleaned\n\nSelect only the 100 most frequent tokens, using the function top_n()\nCreate a ggplot()-base with label = text and size = n as aes() and\nUse ggwordcloud::geom_text_wordclout() to create the wordcloud.\nUse scale_size_are() to adopt the scaling of the wordcloud.\nUse theme_minimal() for clean visualisation.\n\n\n\n\n# Create summarized data\n\n\n# Preview Top 50 token\n\n\n# Check distribution parameters\n\n\n# Optional: Check results with a wordcloud\n\n\n\n📋 Exercise 3: Couting and correlating pairs of words\n\n3.1 Couting word pairs within tweets\n\nCouting word pairs among sections\n\nBased on the dataset tweets_tidy_cleaned, count word pairs using widyr::pairwise_count(), with the arguments item = text, feature = tweet_id and sort = TRUE.\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\n\n\n# Couting word pairs among sections\n\n# Check \n\n\n\n3.2 Pairwise correlation\n\nGetting pairwise correlation\n\nBased on the dataset tweets_tidy_cleaned,\n\ngroup the data with the function group_by() by the variable text\nuse filter(n() &gt;= X) to only use tokens that appear at least a certain amount of times (X).\nPlease feel free to select an X of your choice, however, I would strongly recommend an X &gt; 100, as otherwise the following function might not be able to compute.\ncreate word correlations using widyr::pairwise_cor(), with the arguments item = text,feature = tweet_id and sort = TRUE.\n\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck pairs with highest correlation by using the print() function.\n\n\n# Getting pairwise correlation \n\n\n# Check pairs with highest correlation\n\n\n\n3.3. Optinal: Visualization\n\nCustomize the parameters in the following code chunk:\n\nselected_words, for defining the words you want to get the correlates for\nnumber_of_correlates, to vary the number of correlates shown in the graph.\n\nSet #|eval : true to execute chunk\n\n\n# eval: false\n\n# Define parameters\nselected_words &lt;- c(\"digital\")\nnumber_of_correlates &lt;- 5\n\n# Visualize correlates\ntweets_pairs_corr_cleaned %&gt;% \n  filter(item1 %in% selected_words) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = number_of_correlates) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder(item2, correlation)) %&gt;% \n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n📋 Exercise 4: Sentiment\n\nApply afinn dictionary to get sentiment\n\nBased on the dataset tweets_tidy_cleaned,\n\nmatch the words from the afinn dictionary with the tokens in the tweets by using the inner_join() function. Within inner_join() , please use the get_sentiments() -function with the dictionary \"afinn\" for y , c(\"text\" = \"word\") for the by and \"many-to-many\" for the relationship argument.\nuse group_by() for grouping the tweets by the variable tweet_id\nand summarize() the sentiment for each tweet, by creating a new variable (within the summarize() function), called sentiment, that is the sum (use sum()) of the sentiment values assigned to the words of the dictionary (variable value ).\n\nSave this transformation by creating a new dataset with the name tweets_sentiment_cleaned.\n\nCheck if transformation was successful by using the print() function.\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nVisualize distribution\n\nBased on the newly created dataset tweets_sentiment_cleaned, create a ggplot with sentiment as aes() and by using geom_histogram().\n\n\n\n# Apply 'afinn' dictionary to get sentiment\n\n\n# Check transformation\n\n\n# Check distribution statistics \n\n\n# Visualize distribution"
  },
  {
    "objectID": "exercise/exercise-09_solution.html",
    "href": "exercise/exercise-09_solution.html",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "",
    "text": "Open session slides\n Download source file"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#background",
    "href": "exercise/exercise-09_solution.html#background",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nDefinition\n\n\n\n\n„Digital disconnection is a deliberate (i.e., chosen by the individual) form of non-use of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of restoring or improving one’s perceived overuse, social interactions, psychological well-being, productivity, privacy and/or perceived usefulness“. (Nassen et al. 2023)\n\n\n\n\nIncreasing trend towards more conscious use of digital media (devices), including (deliberate) non-use with the aim to restore or improve psychological well-being (among other factors)\nBut how do “we” talk about digital detox/disconnection: 💊 drug, 👹 demon or 🍩 donut?\n\n\n\n\n\n\n\nTodays’s data basis: Twitter dataset\n\n\n\n\nCollection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not 𝕏)\nInitial query is searching for “digital detox”, “#digitaldetox”, “digital_detox”\nAccess via official Academic-Twitter-API via academictwitteR (Barrie and Ho 2021) at the beginning of last year"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#preparation",
    "href": "exercise/exercise-09_solution.html#preparation",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  tidytext, textdata, widyr, # text processing\n  ggpubr, ggwordcloud, # visualization\n  tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#import-and-process-the-data",
    "href": "exercise/exercise-09_solution.html#import-and-process-the-data",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Import and process the data",
    "text": "Import and process the data\n\n# Import raw data from local\ntweets &lt;- qs::qread(here(\"local_data/tweets-digital_detox.qs\"))$raw %&gt;% \n  janitor::clean_names()\n\n# Initial data processing\ntweets_correct &lt;- tweets %&gt;% \n  mutate(\n    # reformat and create datetime variables\n    across(created_at, ~ymd_hms(.)), # convert to dttm format\n    year = year(created_at), \n    month = month(created_at), \n    day = day(created_at), \n    hour = hour(created_at),\n    minute = minute(created_at),\n    # create addtional variables\n    retweet_dy = str_detect(text, \"^RT\"), # identify retweets\n    detox_dy = str_detect(text, \"#digitaldetox\") \n  ) %&gt;% \n  distinct(tweet_id, .keep_all = TRUE)\n\n# Filter relevant tweets\ntweets_detox &lt;- tweets_correct %&gt;% \n  filter(\n    detox_dy == TRUE, # only tweets with #digitaldetox\n    retweet_dy == FALSE, # no retweets\n    lang == \"en\" # only english tweets\n    )"
  },
  {
    "objectID": "exercise/exercise-09_solution.html#exercises",
    "href": "exercise/exercise-09_solution.html#exercises",
    "title": "Exercise 09: 🔨 Text as data in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nObjective of this exercise\n\n\n\n\nBrush up basic knowledge of working with R, tidyverse and ggplot2\nGet to know the typical steps of tidy text analysis with tidytext, from tokenisation and summarisation to visualisation.\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\n\nBefore you start working on the exercise, please make sure to render all the chunks of the section Preparation. You can do this by using the “Run all chunks above”-button  of the next chunk.\nWhen in doubt, use the showcase (.qmd or .html) to look at the code chunks used to produce the output of the slides.\n\n\n\n\n📋 Exercise 1: Transform to ‘tidy text’\n\nDefine custom stopwords\n\nEdit the vector remove_custom_stopwords by defining additional words to be deleted based on the analysis presented in the session. (Use | as a logical OR operator in regular expressions. It allows you to specify alternatives, e.g. add |https.)\n\nCreate new dataset tweets_tidy_cleaned\n\nBased on the dataset tweets_detox,\n\nuse the str_remove_all function to remove the specified patterns of the remove_custom_stopwords vector. Use the mutate() function to edit the text variable.\nTokenize the ‘text’ column using unnest_tokens.\nFilter out stop words using filter and stopwords$words\n\nSave this transformation by creating a new dataset with the name tweets_tidy_cleaned.\n\nCheck if transformation was successful (e.g. by using the print() function)\n\n\n# Define custom stopwords\nremove_custom_stopwords &lt;- \"&amp;|&lt;|&gt;|http*|t.c|digitaldetox\"\n\n# Create new dataset clean_tidy_tweets\ntweets_tidy_cleaned &lt;- tweets_detox %&gt;% \n  mutate(text = str_remove_all(text, remove_custom_stopwords)) %&gt;% \n  tidytext::unnest_tokens(\"text\", text) %&gt;% \n  filter(\n    !text %in% tidytext::stop_words$word)\n\n# Check\ntweets_tidy_cleaned %&gt;% print()\n\n# A tibble: 476,176 × 37\n   tweet_id   user_username text       created_at          in_reply_to_user_id\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;              \n 1 5777201122 pblackshaw    blackberry 2009-11-16 22:03:12 &lt;NA&gt;               \n 2 5777201122 pblackshaw    iphone     2009-11-16 22:03:12 &lt;NA&gt;               \n 3 5777201122 pblackshaw    read       2009-11-16 22:03:12 &lt;NA&gt;               \n 4 5777201122 pblackshaw    pew        2009-11-16 22:03:12 &lt;NA&gt;               \n 5 5777201122 pblackshaw    report     2009-11-16 22:03:12 &lt;NA&gt;               \n 6 5777201122 pblackshaw    teens      2009-11-16 22:03:12 &lt;NA&gt;               \n 7 5777201122 pblackshaw    distracted 2009-11-16 22:03:12 &lt;NA&gt;               \n 8 5777201122 pblackshaw    driving    2009-11-16 22:03:12 &lt;NA&gt;               \n 9 5777201122 pblackshaw    bit.ly     2009-11-16 22:03:12 &lt;NA&gt;               \n10 5777201122 pblackshaw    4abr5p     2009-11-16 22:03:12 &lt;NA&gt;               \n# ℹ 476,166 more rows\n# ℹ 32 more variables: author_id &lt;chr&gt;, lang &lt;chr&gt;, possibly_sensitive &lt;lgl&gt;,\n#   conversation_id &lt;chr&gt;, user_created_at &lt;chr&gt;, user_protected &lt;lgl&gt;,\n#   user_name &lt;chr&gt;, user_verified &lt;lgl&gt;, user_description &lt;chr&gt;,\n#   user_location &lt;chr&gt;, user_url &lt;chr&gt;, user_profile_image_url &lt;chr&gt;,\n#   user_pinned_tweet_id &lt;chr&gt;, retweet_count &lt;int&gt;, like_count &lt;int&gt;,\n#   quote_count &lt;int&gt;, user_tweet_count &lt;int&gt;, user_list_count &lt;int&gt;, …\n\n\n\n\n📋 Exercise 2: Summarize tokens\n\nCreate summarized data\n\nBased on the dataset tweets_tidy_cleaned, summarize the frequency of the individual tokens by using the count()-function on the variable text. Use the argument sort = TRUE to sort the dataset based on descending frequency of the tokens.\nSave this transformation by creating a new dataset with the name tweets_summarized_cleaned.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nOptional: Check results with a wordcloud\n\nBased on the sorted dataset tweets_summarized_cleaned\n\nSelect only the 100 most frequent tokens, using the function top_n()\nCreate a ggplot()-base with label = text and size = n as aes() and\nUse ggwordcloud::geom_text_wordclout() to create the wordcloud.\nUse scale_size_are() to adopt the scaling of the wordcloud.\nUse theme_minimal() for clean visualisation.\n\n\n\n\n# Create summarized data\ntweets_summarized_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\ntweets_summarized_cleaned %&gt;% \n    print(n = 50)\n\n# A tibble: 87,756 × 2\n   text                 n\n   &lt;chr&gt;            &lt;int&gt;\n 1 digital           8521\n 2 detox             6623\n 3 time              6001\n 4 phone             4213\n 5 unplug            4021\n 6 day               3020\n 7 life              2548\n 8 social            2449\n 9 mindfulness       2408\n10 media             2264\n11 week              1848\n12 unplugging        1674\n13 weekend           1651\n14 health            1649\n15 hnology           1644\n16 mentalhealth      1611\n17 smartphone        1610\n18 screen            1557\n19 nature            1421\n20 travel            1411\n21 wellbeing         1397\n22 break             1386\n23 addiction         1355\n24 listen            1342\n25 kids              1339\n26 sleep             1315\n27 wellness          1301\n28 read              1297\n29 tips              1247\n30 retreat           1242\n31 family            1232\n32 discuss           1225\n33 socialmedia       1208\n34 devices           1184\n35 screentime        1183\n36 icphenomenallyu   1181\n37 offline           1072\n38 free              1071\n39 days              1068\n40 world             1068\n41 check             1062\n42 5                 1013\n43 disconnect        1003\n44 feel               980\n45 enjoy              950\n46 switchoff          949\n47 people             941\n48 digitalwellbeing   937\n49 book               932\n50 love               922\n# ℹ 87,706 more rows\n\n# Check distribution parameters \ntweets_summarized_cleaned %&gt;%\n  datawizard::describe_distribution()\n\nVariable | Mean |    SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-----------------------------------------------------------------------------------------\nn        | 5.43 | 62.69 |   0 | [1.00, 8521.00] |    67.71 |  6936.19 | 87756 |         0\n\n# Optional: Check results with a wordcloud\ntweets_summarized_cleaned %&gt;% \n    top_n(100) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n\n\n\n\n\n\n📋 Exercise 3: Couting and correlating pairs of words\n\n3.1 Couting word pairs within tweets\n\nCouting word pairs among sections\n\nBased on the dataset tweets_tidy_cleaned, count word pairs using widyr::pairwise_count(), with the arguments item = text, feature = tweet_id and sort = TRUE.\nSave this transformation by creating a new dataset with the name tweets_word_pairs_cleand.\n\nCheck if transformation was successful by using the print() function.\n\nUse the argument n = 50 to display the top 50 token (only possible if argument sort = TRUE was used when running the count() function)\n\n\n\n# Couting word pairs among sections\ntweets_word_pairs_cleand &lt;- tweets_tidy_cleaned %&gt;% \n  widyr::pairwise_count(\n    item = text,\n    feature = tweet_id,\n    sort = TRUE)\n\n# Check \ntweets_word_pairs_cleand %&gt;% print(n = 50)\n\n# A tibble: 2,958,758 × 3\n   item1                 item2                     n\n   &lt;chr&gt;                 &lt;chr&gt;                 &lt;dbl&gt;\n 1 detox                 digital                5195\n 2 digital               detox                  5195\n 3 media                 social                 1999\n 4 social                media                  1999\n 5 discuss               listen                 1185\n 6 listen                discuss                1185\n 7 listen                unplugging             1183\n 8 unplugging            listen                 1183\n 9 discuss               unplugging             1181\n10 icphenomenallyu       unplugging             1181\n11 icphenomenallyu       listen                 1181\n12 unplugging            discuss                1181\n13 icphenomenallyu       discuss                1181\n14 unplugging            icphenomenallyu        1181\n15 listen                icphenomenallyu        1181\n16 discuss               icphenomenallyu        1181\n17 time                  digital                 958\n18 digital               time                    958\n19 screen                time                    883\n20 time                  screen                  883\n21 unplug                detox                   799\n22 detox                 unplug                  799\n23 dqqdpucxoe            unplugging              793\n24 dqqdpucxoe            listen                  793\n25 dqqdpucxoe            discuss                 793\n26 dqqdpucxoe            icphenomenallyu         793\n27 unplugging            dqqdpucxoe              793\n28 listen                dqqdpucxoe              793\n29 discuss               dqqdpucxoe              793\n30 icphenomenallyu       dqqdpucxoe              793\n31 unplug                digital                 791\n32 digital               unplug                  791\n33 time                  detox                   789\n34 detox                 time                    789\n35 phonefree             disconnecttoreconnect   682\n36 disconnecttoreconnect phonefree               682\n37 phonefree             switchoff               673\n38 switchoff             phonefree               673\n39 digitalminimalism     phonebreakup            636\n40 phonebreakup          digitalminimalism       636\n41 digitalminimalism     phonefree               633\n42 phonefree             digitalminimalism       633\n43 time                  unplug                  628\n44 unplug                time                    628\n45 disconnecttoreconnect switchoff               617\n46 switchoff             disconnecttoreconnect   617\n47 time                  phone                   616\n48 phone                 time                    616\n49 phonefree             phonebreakup            615\n50 phonebreakup          phonefree               615\n# ℹ 2,958,708 more rows\n\n\n\n\n3.2 Pairwise correlation\n\nGetting pairwise correlation\n\nBased on the dataset tweets_tidy_cleaned,\n\ngroup the data with the function group_by() by the variable text\nuse filter(n() &gt;= X) to only use tokens that appear at least a certain amount of times (X).\nPlease feel free to select an X of your choice, however, I would strongly recommend an X &gt; 100, as otherwise the following function might not be able to compute.\ncreate word correlations using widyr::pairwise_cor(), with the arguments item = text,feature = tweet_id and sort = TRUE.\n\nSave this transformation by creating a new dataset with the name tweets_pairs_corr_cleaned.\n\nCheck pairs with highest correlation by using the print() function.\n\n\n# Getting pairwise correlation \ntweets_pairs_corr_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  group_by(text) %&gt;% \n  filter(n() &gt;= 250) %&gt;% \n  pairwise_cor(text, tweet_id, sort = TRUE)\n\n# Check pairs with highest correlation\ntweets_pairs_corr_cleaned %&gt;% print(n = 50)\n\n# A tibble: 62,250 × 3\n   item1             item2             correlation\n   &lt;chr&gt;             &lt;chr&gt;                   &lt;dbl&gt;\n 1 jocelyn           brewer                  0.999\n 2 brewer            jocelyn                 0.999\n 3 jocelyn           discusses               0.983\n 4 discusses         jocelyn                 0.983\n 5 igjzl0z4b7        locally                 0.982\n 6 locally           igjzl0z4b7              0.982\n 7 discusses         brewer                  0.982\n 8 brewer            discusses               0.982\n 9 icphenomenallyu   discuss                 0.981\n10 discuss           icphenomenallyu         0.981\n11 taniamulry        wealth                  0.979\n12 wealth            taniamulry              0.979\n13 locally           cabins                  0.977\n14 cabins            locally                 0.977\n15 igjzl0z4b7        cabins                  0.970\n16 cabins            igjzl0z4b7              0.970\n17 jocelyn           nutrition               0.968\n18 nutrition         jocelyn                 0.968\n19 nutrition         brewer                  0.967\n20 brewer            nutrition               0.967\n21 discusses         nutrition               0.951\n22 nutrition         discusses               0.951\n23 icphenomenallyu   listen                  0.937\n24 listen            icphenomenallyu         0.937\n25 discuss           listen                  0.923\n26 listen            discuss                 0.923\n27 screenlifebalance mindfulliving           0.920\n28 mindfulliving     screenlifebalance       0.920\n29 limited           locally                 0.919\n30 locally           limited                 0.919\n31 screenlifebalance socialmediafast         0.915\n32 socialmediafast   screenlifebalance       0.915\n33 igjzl0z4b7        limited                 0.913\n34 limited           igjzl0z4b7              0.913\n35 limited           cabins                  0.908\n36 cabins            limited                 0.908\n37 taniamulry        info                    0.905\n38 info              taniamulry              0.905\n39 socialmediafast   digitaladdiction        0.904\n40 digitaladdiction  socialmediafast         0.904\n41 media             social                  0.901\n42 social            media                   0.901\n43 socialmediafast   mindfulliving           0.893\n44 mindfulliving     socialmediafast         0.893\n45 jocelyn           digitalnutrition        0.893\n46 digitalnutrition  jocelyn                 0.893\n47 brewer            digitalnutrition        0.892\n48 digitalnutrition  brewer                  0.892\n49 wealth            info                    0.891\n50 info              wealth                  0.891\n# ℹ 62,200 more rows\n\n\n\n\n3.3. Optinal: Visualization\n\nCustomize the parameters in the following code chunk:\n\nselected_words, for defining the words you want to get the correlates for\nnumber_of_correlates, to vary the number of correlates shown in the graph.\n\nSet #|eval : true to execute chunk\n\n\n# Define parameters\nselected_words &lt;- c(\"digital\")\nnumber_of_correlates &lt;- 5\n\n# Visualize correlates\ntweets_pairs_corr_cleaned %&gt;% \n  filter(item1 %in% selected_words) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = number_of_correlates) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder(item2, correlation)) %&gt;% \n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n📋 Exercise 4: Sentiment\n\nApply afinn dictionary to get sentiment\n\nBased on the dataset tweets_tidy_cleaned,\n\nmatch the words from the afinn dictionary with the tokens in the tweets by using the inner_join() function. Within inner_join() , please use the get_sentiments() -function with the dictionary \"afinn\" for y , c(\"text\" = \"word\") for the by and \"many-to-many\" for the relationship argument.\nuse group_by() for grouping the tweets by the variable tweet_id\nand summarize() the sentiment for each tweet, by creating a new variable (within the summarize() function), called sentiment, that is the sum (use sum()) of the sentiment values assigned to the words of the dictionary (variable value ).\n\nSave this transformation by creating a new dataset with the name tweets_sentiment_cleaned.\n\nCheck if transformation was successful by using the print() function.\nCheck distribution\n\nUse the datawizard::describe_distribution() function to check different distribution parameters\n\nVisualize distribution\n\nBased on the newly created dataset tweets_sentiment_cleaned, create a ggplot with sentiment as aes() and by using geom_histogram().\n\n\n\n# Apply 'afinn' dictionary to get sentiment\ntweets_sentiment_cleaned &lt;- tweets_tidy_cleaned %&gt;% \n  inner_join(\n     y = get_sentiments(\"afinn\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  group_by(tweet_id) %&gt;% \n  summarize(sentiment = sum(value))\n\n# Check transformation\ntweets_sentiment_cleaned %&gt;% print()\n\n# A tibble: 23,956 × 2\n   tweet_id            sentiment\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 1000009901563838465         6\n 2 1000038819520008193         1\n 3 1000042717492187136        15\n 4 1000043574673715203        -1\n 5 1000075155891281925         2\n 6 1000094334660825088         3\n 7 1000104543395315713        -2\n 8 1000255467434729472        -1\n 9 1000305076286697472         4\n10 1000349838595248128         1\n# ℹ 23,946 more rows\n\n# Check distribution statistics \ntweets_sentiment_cleaned %&gt;%\n  datawizard::describe_distribution()\n\nVariable  | Mean |   SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-----------------------------------------------------------------------------------------\nsentiment | 1.24 | 2.77 |   4 | [-13.00, 21.00] |     0.40 |     1.66 | 23956 |         0\n\n# Visualize distribution\ntweets_sentiment_cleaned %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram() +\n  theme_pubr()"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html",
    "href": "exercise/local_exercise-03_solution.html",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "",
    "text": "Download source file: \nOpen this exercise in other interactive and executable environments:"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#background",
    "href": "exercise/local_exercise-03_solution.html#background",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Background",
    "text": "Background\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential\n\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#packages",
    "href": "exercise/local_exercise-03_solution.html#packages",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#codechunks-aus-der-sitzung",
    "href": "exercise/local_exercise-03_solution.html#codechunks-aus-der-sitzung",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste “Runde” der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,176 × 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and …         1971 Hal Ash…             52 Bud Cort     man           \n 2 Venus                2006 Roger M…             50 Peter O'Too… man           \n 3 The Quiet A…         2002 Phillip…             49 Michael Cai… man           \n 4 The Big Leb…         1998 Joel Co…             45 David Huddl… man           \n 5 Beginners            2010 Mike Mi…             43 Christopher… man           \n 6 Poison Ivy           1992 Katt Sh…             42 Tom Skerritt man           \n 7 Whatever Wo…         2009 Woody A…             40 Larry David  man           \n 8 Entrapment           1999 Jon Ami…             39 Sean Connery man           \n 9 Husbands an…         1992 Woody A…             38 Woody Allen  man           \n10 Magnolia             1999 Paul Th…             38 Jason Robar… man           \n# ℹ 1,166 more rows\n# ℹ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale Überprüfung der Daten\n\n\n\n\n\n\nSind die Daten “technisch korrekt”?\n\n\n\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n✅ Wie viele eindeutige Werte hat jede Variable?\n✅ Welcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\n✅ Gibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\nÜberblick über die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,176\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"…\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992…\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel…\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, …\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David…\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma…\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192…\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, …\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", …\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", …\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0…\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, …\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nÜberprüfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1176      0 2000.78 16.64 0.49 2004\n age_difference numeric age_difference 1176      0   10.46  8.52 0.25    8\n    actor_1_age numeric    actor_1_age 1176      0   39.99 10.92 0.32   39\n    actor_2_age numeric    actor_2_age 1176      0   31.25  8.44 0.25   30\n trimmed          range   iqr  skew\n 2003.68 88 (1935-2023) 15.00 -1.69\n    9.39      52 (0-52) 12.00  1.19\n   39.42     64 (17-81) 15.00  0.54\n   30.42     64 (17-81)  9.25  1.37\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,176 × 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ℹ 1,166 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 12 × 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal…\n 2 Oppenheimer                         20         2023 Cillian Mur… Florence Pu…\n 3 The Northman                        20         2022 Alexander S… Anya Taylor…\n 4 The Lost City                       16         2022 Channing Ta… Sandra Bull…\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb…\n 6 Everything Everywhere …              9         2022 Ke Huy Quan  Michelle Ye…\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co…\n 8 Oppenheimer                          7         2023 Cillian Mur… Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc… Zoë Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi… Reese Withe…\n11 Your Place or Mine                   2         2023 Ashton Kutc… Reese Withe…\n12 You People                           1         2023 Jonah Hill   Lauren Lond…\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\nÜberprüfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1174) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.56 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1176\n\n\n\n\nSchätzung OLS\n\n# Schätzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1174) |      p\n------------------------------------------------------------------------\n(Intercept)  |      231.15 | 29.20 | [173.86, 288.44] |    7.92 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.56 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8326.022 | 8326.043 | 8341.232 | 0.046 |     0.046 | 8.319 | 8.326\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1174) = 57.12, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 231.15 (95% CI [173.86, 288.44], t(1174) = 7.92, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1174) = -7.56, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#exercise-welche-rolle-spielt-das-geschlecht",
    "href": "exercise/local_exercise-03_solution.html#exercise-welche-rolle-spielt-das-geschlecht",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "📋 Exercise: Welche Rolle spielt das Geschlecht?",
    "text": "📋 Exercise: Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun ergänzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die “Gültigkeit” der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und Überarbeitungsschritte notwendig\n\n\n\n\nÜbeprüfung der _gender-Variablen\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\nage_gaps_correct %&gt;% \n  frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1176 valid N=1176 mean=1.01 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1160 | 98.64 |   98.64 |  98.64\nwoman |   16 |  1.36 |    1.36 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1176 valid N=1176 mean=1.99 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   17 |  1.45 |    1.45 |   1.45\nwoman | 1159 | 98.55 |   98.55 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1148\nwoman                            5    11\n\n\n\n\nSind die Daten “konsistent”?\n\nÜberprüfung der Sortierung\n\nNutzen Sie die Funktion dplyr::summarise um auf Basis des Datensatzes age_gaps_correct folgende Variabeln zu erstellen:\np1_older\n\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 × 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.811   0.986             0.5\n\n\n\n\n\nÜberprüfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=843 valid N=843 mean=1.40 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 610 | 72.36 |   72.36 |  72.36\n    2 | 160 | 18.98 |   18.98 |  91.34\n    3 |  54 |  6.41 |    6.41 |  97.75\n    4 |  14 |  1.66 |    1.66 |  99.41\n    5 |   3 |  0.36 |    0.36 |  99.76\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 × 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )"
  },
  {
    "objectID": "exercise/local_exercise-03_solution.html#die-zweite-datenexploration",
    "href": "exercise/local_exercise-03_solution.html#die-zweite-datenexploration",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Die zweite Datenexploration",
    "text": "Die zweite Datenexploration\n\nAlterskombinationen im Überblick\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1176 valid N=1176 mean=3.77 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.94 |    0.94 |   0.94\n    2 |  12 |  1.02 |    1.02 |   1.96\n    3 | 209 | 17.77 |   17.77 |  19.73\n    4 | 944 | 80.27 |   80.27 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1176 valid N=1153 mean=0.82 sd=0.39\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 209 | 17.77 |   18.13 |  18.13\n    1 | 944 | 80.27 |   81.87 | 100.00\n &lt;NA&gt; |  23 |  1.96 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nWie sind die Altersunterschiede unterteilt, unter Berücksichtiung des Geschlechts?\n\nGraphische Überprüfung\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero für das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\nÜberprüfung durch Modellierung\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters(standardize=\"basic\")\n\nParameter             | Std. Coef. |   SE |         95% CI | t(1150) |      p\n-----------------------------------------------------------------------------\n(Intercept)           |       0.00 | 0.00 | [ 0.00,  0.00] |    7.14 | &lt; .001\nrelease year          |      -0.19 | 0.03 | [-0.25, -0.14] |   -6.96 | &lt; .001\nolder male hetero [1] |       0.27 | 0.03 | [ 0.22,  0.33] |    9.87 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8050.434 | 8050.469 | 8070.635 | 0.125 |     0.124 | 7.914 | 7.925"
  },
  {
    "objectID": "exercise/exercise-03.html",
    "href": "exercise/exercise-03.html",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "",
    "text": "Download source file\n Open this exercise in interactive and executable environment"
  },
  {
    "objectID": "exercise/exercise-03.html#background",
    "href": "exercise/exercise-03.html#background",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Background",
    "text": "Background\n\nThe best way to learn R is by trying. This document tries to display a version of the “normal” data processing procedure.\nUse tidytuesday data as an example to showcase the potential\n\n\n\n\n\n\n\nTodays’s data basis: Hollywood Age Gaps\n\n\n\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\nThe youngest of the two actors is at least 17 years old\nNot animated characters"
  },
  {
    "objectID": "exercise/exercise-03.html#packages",
    "href": "exercise/exercise-03.html#packages",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, \n  magrittr, \n  tidyverse,\n  janitor,\n  easystats,\n  sjmisc,\n  ggpubr)"
  },
  {
    "objectID": "exercise/exercise-03.html#codechunks-aus-der-sitzung",
    "href": "exercise/exercise-03.html#codechunks-aus-der-sitzung",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDie erste “Runde” der Datenaufbereitung\n\nDatenimport via URL\n\nVariablennamen und -beschreibungen\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmovie_name\nName of the film\n\n\nrelease_year\nRelease year\n\n\ndirector\nDirector of the film\n\n\nage_difference\nAge difference between the characters in whole years\n\n\ncouple_number\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\nThe name of the older actor in this couple\n\n\nactor_2_name\nThe name of the younger actor in this couple\n\n\nactor_1_birthdate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\nThe age of the older actor when the film was released\n\n\nactor_2_age\nThe age of the younger actor when the film was released\n\n\n\n\n# Import data from URL\nage_gaps &lt;- read_csv(\"http://hollywoodagegap.com/movies.csv\") %&gt;% \n  janitor::clean_names()\n\n# Check data set\nage_gaps\n\n# A tibble: 1,177 × 12\n   movie_name   release_year director age_difference actor_1_name actor_1_gender\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;         \n 1 Harold and …         1971 Hal Ash…             52 Bud Cort     man           \n 2 Venus                2006 Roger M…             50 Peter O'Too… man           \n 3 The Quiet A…         2002 Phillip…             49 Michael Cai… man           \n 4 The Big Leb…         1998 Joel Co…             45 David Huddl… man           \n 5 Beginners            2010 Mike Mi…             43 Christopher… man           \n 6 Poison Ivy           1992 Katt Sh…             42 Tom Skerritt man           \n 7 Whatever Wo…         2009 Woody A…             40 Larry David  man           \n 8 Entrapment           1999 Jon Ami…             39 Sean Connery man           \n 9 Husbands an…         1992 Woody A…             38 Woody Allen  man           \n10 Magnolia             1999 Paul Th…             38 Jason Robar… man           \n# ℹ 1,167 more rows\n# ℹ 6 more variables: actor_1_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_name &lt;chr&gt;, actor_2_gender &lt;chr&gt;, actor_2_birthdate &lt;chr&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitiale Überprüfung der Daten\n\n\n\n\n\n\nSind die Daten “technisch korrekt”?\n\n\n\n\n✅ Wie viele Fälle sind enthalten? Wie viele Variablen?\n✅ Wie lauten die Variablennamen? Sind sie sinnvoll?\n✅ Welchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n✅ Wie viele eindeutige Werte hat jede Variable?\n✅ Welcher Wert tritt am häufigsten auf, und wie oft kommt er vor?\n✅ Gibt es fehlende Werte? Wenn ja, wie häufig ist dies der Fall?\n\n\n\n\nÜberblick über die Daten\n\nage_gaps %&gt;% glimpse()\n\nRows: 1,177\nColumns: 12\n$ movie_name        &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"…\n$ release_year      &lt;dbl&gt; 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 1992…\n$ director          &lt;chr&gt; \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joel…\n$ age_difference    &lt;dbl&gt; 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35, …\n$ actor_1_name      &lt;chr&gt; \"Bud Cort\", \"Peter O'Toole\", \"Michael Caine\", \"David…\n$ actor_1_gender    &lt;chr&gt; \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"ma…\n$ actor_1_birthdate &lt;date&gt; 1948-03-29, 1932-08-02, 1933-03-14, 1930-09-17, 192…\n$ actor_1_age       &lt;dbl&gt; 23, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65, …\n$ actor_2_name      &lt;chr&gt; \"Ruth Gordon\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", …\n$ actor_2_gender    &lt;chr&gt; \"woman\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", …\n$ actor_2_birthdate &lt;chr&gt; \"1896-10-30\", \"1982-06-03\", \"1982-10-01\", \"1975-11-0…\n$ actor_2_age       &lt;dbl&gt; 75, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30, …\n\n\n\n\nKorrekturen\n\nage_gaps_correct &lt;- age_gaps %&gt;% \n  mutate(\n    across(ends_with(\"_birthdate\"), ~as.Date(.)) # set dates to dates\n  )\n\n\n\nÜberprüfung Lageparameter\n\nage_gaps_correct %&gt;% descr()\n\n\n## Basic descriptive statistics\n\n            var    type          label    n NA.prc    mean    sd   se   md\n   release_year numeric   release_year 1177      0 2000.74 16.67 0.49 2004\n age_difference numeric age_difference 1177      0   10.48  8.53 0.25    8\n    actor_1_age numeric    actor_1_age 1177      0   39.97 10.90 0.32   39\n    actor_2_age numeric    actor_2_age 1177      0   31.27  8.50 0.25   30\n trimmed          range iqr  skew\n 2003.65 88 (1935-2023)  15 -1.68\n    9.41      52 (0-52)  12  1.19\n   39.41     64 (17-81)  15  0.53\n   30.42     64 (17-81)   9  1.39\n\n\n\n\n\n\nDie ersten Datenexplorationen\n\nWie sind die Altersunterschiede verteilt?\n\nage_gaps_correct %&gt;% \n    ggplot(aes(x = age_difference)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\nIn welchen Filmen ist der Altersunterschied am höchsten?\n\nage_gaps_correct %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(movie_name, age_difference, release_year) \n\n# A tibble: 1,177 × 3\n   movie_name         age_difference release_year\n   &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Harold and Maude               52         1971\n 2 Venus                          50         2006\n 3 The Quiet American             49         2002\n 4 The Big Lebowski               45         1998\n 5 Beginners                      43         2010\n 6 Poison Ivy                     42         1992\n 7 Whatever Works                 40         2009\n 8 Entrapment                     39         1999\n 9 Husbands and Wives             38         1992\n10 Magnolia                       38         1999\n# ℹ 1,167 more rows\n\n\n\nage_gaps_correct %&gt;% \n    filter(release_year &gt;= 2022) %&gt;% \n    arrange(desc(age_difference)) %&gt;% \n    select(\n        movie_name, age_difference, release_year, \n        actor_1_name, actor_2_name) \n\n# A tibble: 12 × 5\n   movie_name              age_difference release_year actor_1_name actor_2_name\n   &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 The Bubble                          21         2022 Pedro Pascal Maria Bakal…\n 2 Oppenheimer                         20         2023 Cillian Mur… Florence Pu…\n 3 The Northman                        20         2022 Alexander S… Anya Taylor…\n 4 The Lost City                       16         2022 Channing Ta… Sandra Bull…\n 5 Barbie                              10         2023 Ryan Gosling Margot Robb…\n 6 Everything Everywhere …              9         2022 Ke Huy Quan  Michelle Ye…\n 7 Top Gun: Maverick                    8         2022 Tom Cruise   Jennifer Co…\n 8 Oppenheimer                          7         2023 Cillian Mur… Emily Blunt \n 9 Your Place or Mine                   7         2023 Ashton Kutc… Zoë Chao    \n10 Your Place or Mine                   5         2023 Jesse Willi… Reese Withe…\n11 Your Place or Mine                   2         2023 Ashton Kutc… Reese Withe…\n12 You People                           1         2023 Jonah Hill   Lauren Lond…\n\n\n\n\nGibt es einen Zusammenhang zwischen Altersunterschied und Release?\n\n(Durchschnitts-)Unterschied nach Jahren\n\nage_gaps_correct %&gt;% \n    group_by(release_year) %&gt;% \n    summarise(age_difference_mean = mean(age_difference)) %&gt;% \n    ggplot(aes(release_year, age_difference_mean)) +\n    geom_col() +\n    theme_pubr()\n\n\n\n\n\n\nVerteilung nach Jahren\n\nggpubr::ggboxplot(\n    data = age_gaps_correct, \n    x = \"release_year\", \n    y = \"age_difference\", \n  ) + \n   # Rotate x-axis labels by 90 degrees\n  theme(\n    axis.text.x = element_text(\n        angle = 90,\n        vjust = 0.5,\n         hjust=1))  \n\n\n\n\n\n\nÜberprüfung der Korrelation\n\nage_gaps %&gt;%\n  select(release_year, age_difference) %&gt;% \n  correlation::correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |     Parameter2 |     r |         95% CI | t(1175) |         p\n----------------------------------------------------------------------------\nrelease_year | age_difference | -0.22 | [-0.27, -0.16] |   -7.68 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1177\n\n\n\n\nSchätzung OLS\n\n# Schätzung des Models\nmdl &lt;- lm(age_difference ~ release_year, data = age_gaps_correct)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter    | Coefficient |    SE |           95% CI | t(1175) |      p\n------------------------------------------------------------------------\n(Intercept)  |      234.30 | 29.15 | [177.11, 291.50] |    8.04 | &lt; .001\nrelease year |       -0.11 |  0.01 | [ -0.14,  -0.08] |   -7.68 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8334.623 | 8334.643 | 8349.835 | 0.048 |     0.047 | 8.324 | 8.331\n\n\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year (formula: age_difference ~ release_year). The model explains a\nstatistically significant and weak proportion of variance (R2 = 0.05, F(1,\n1175) = 58.96, p &lt; .001, adj. R2 = 0.05). The model's intercept, corresponding\nto release_year = 0, is at 234.30 (95% CI [177.11, 291.50], t(1175) = 8.04, p &lt;\n.001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.11, 95% CI [-0.14, -0.08], t(1175) = -7.68, p &lt; .001; Std. beta = -0.22, 95%\nCI [-0.27, -0.16])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "exercise/exercise-03.html#exercise-welche-rolle-spielt-das-geschlecht",
    "href": "exercise/exercise-03.html#exercise-welche-rolle-spielt-das-geschlecht",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "📋 Exercise: Welche Rolle spielt das Geschlecht?",
    "text": "📋 Exercise: Welche Rolle spielt das Geschlecht?\n\n\n\n\n\n\nSpielt das Geschlecht eine Rolle?\n\n\n\n\nDer folgende Abschitt befasst sich nun ergänzend mit der Frage, welche Rolle das Geschlecht mit Blick auf die “Gültigkeit” der vorherigen Ergebnisse spielt\nDazu sind jedoch weitere Explorations- und Überarbeitungsschritte notwendig\n\n\n\n\nÜbeprüfung der _gender-Variablen\n\n\n\n\n\n\nExercise 1\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_correct die Variablen actor_1_gender und actor_2_gender an.\n\n\n\nage_gaps_correct %&gt;% \n  frq(actor_1_gender, actor_2_gender)\n\nactor_1_gender &lt;character&gt; \n# total N=1177 valid N=1177 mean=1.01 sd=0.11\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   | 1162 | 98.73 |   98.73 |  98.73\nwoman |   15 |  1.27 |    1.27 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nactor_2_gender &lt;character&gt; \n# total N=1177 valid N=1177 mean=1.99 sd=0.12\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nman   |   16 |  1.36 |    1.36 |   1.36\nwoman | 1161 | 98.64 |   98.64 | 100.00\n&lt;NA&gt;  |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNutzen Sie die Funktion sjmisc::flat_talbe() und den Datensatz age_gaps_correct um eine Kreuztabelle der Variablen actor_1_gender und actor_2_gender zu erstellen.\n\n\n\nage_gaps_correct %&gt;% \n  select(actor_1_gender, actor_2_gender) %&gt;% \n  flat_table()\n\n               actor_2_gender  man woman\nactor_1_gender                          \nman                             12  1150\nwoman                            4    11\n\n\n\n\nSind die Daten “konsistent”?\n\nÜberprüfung der Sortierung\n\nage_gaps_correct %&gt;% \n  summarise(\n      p1_older = mean(actor_1_age &gt; actor_2_age), # older person first?\n      p1_male  = mean(actor_1_gender == \"man\"),  # male person first? \n      p_1_first_alpha = mean(actor_1_name &lt; actor_2_name) # alphabetical order?\n  )\n\n# A tibble: 1 × 3\n  p1_older p1_male p_1_first_alpha\n     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1    0.811   0.987           0.499\n\n\n\n\n\nÜberprüfung der Anzahl pro Paare pro Film\n\n# Create data\ncouples &lt;- age_gaps_correct %&gt;% \n  group_by(movie_name) %&gt;% \n  summarise(n = n()) \n\n# Distribution\ncouples %&gt;% frq(n)\n\nn &lt;integer&gt; \n# total N=844 valid N=844 mean=1.39 sd=0.75\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 | 611 | 72.39 |   72.39 |  72.39\n    2 | 160 | 18.96 |   18.96 |  91.35\n    3 |  54 |  6.40 |    6.40 |  97.75\n    4 |  14 |  1.66 |    1.66 |  99.41\n    5 |   3 |  0.36 |    0.36 |  99.76\n    6 |   1 |  0.12 |    0.12 |  99.88\n    7 |   1 |  0.12 |    0.12 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n# Movies with a loot of couples \ncouples %&gt;% \n  filter(n &gt; 3) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 19 × 2\n   movie_name                      n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Love Actually                   7\n 2 The Family Stone                6\n 3 A View to a Kill                5\n 4 He's Just Not That Into You     5\n 5 Mona Lisa Smile                 5\n 6 A Star Is Born                  4\n 7 American Pie                    4\n 8 Boogie Nights                   4\n 9 Book Club                       4\n10 Closer                          4\n11 Pushing Tin                     4\n12 Sex and the City                4\n13 Soul Food                       4\n14 Tag                             4\n15 The Favourite                   4\n16 The Girl on the Train           4\n17 The Other Woman                 4\n18 Tomorrow Never Dies             4\n19 Twilight                        4\n\n\n\n\nKorrekturen\n\nage_gaps_consistent &lt;- age_gaps_correct %&gt;% \n  # If multiple couples, assign couple number by movie\n  mutate(\n      couple_number = row_number(),\n      .by = \"movie_name\"\n  ) %&gt;% \n  # Change data structure (one line per actor in a coulpe of a movie)\n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  # Put older actor first\n  arrange(desc(age_difference), movie_name, birthdate) %&gt;% \n    mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) %&gt;% \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  ) %&gt;% \n  mutate(\n    couple_structure = case_when(\n      actor_1_gender == \"woman\" & actor_2_gender == \"woman\" ~ 1,\n      actor_1_gender == \"man\" & actor_2_gender == \"man\" ~ 2,\n      actor_1_gender != \"man\" ~ 3, \n      actor_1_gender == \"man\" ~ 4,\n    ),\n    older_male_hetero  = sjmisc::rec(\n      couple_structure, \n      rec=\"3=0; 4=1; ELSE=NA\", \n      to.factor = TRUE\n    )\n  )"
  },
  {
    "objectID": "exercise/exercise-03.html#die-zweite-datenexploration",
    "href": "exercise/exercise-03.html#die-zweite-datenexploration",
    "title": "Exercise 03: 🔨 Working with R",
    "section": "Die zweite Datenexploration",
    "text": "Die zweite Datenexploration\n\nAlterskombinationen im Überblick\n\n\n\n\n\n\nExercise 3\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz age_gaps_consistent die Variablen couple_structure und older_male_hetero an.\n\n\n\nage_gaps_consistent %&gt;% \n  frq(couple_structure, older_male_hetero)\n\ncouple_structure &lt;numeric&gt; \n# total N=1177 valid N=1177 mean=3.77 sd=0.50\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    1 |  11 |  0.93 |    0.93 |   0.93\n    2 |  12 |  1.02 |    1.02 |   1.95\n    3 | 209 | 17.76 |   17.76 |  19.71\n    4 | 945 | 80.29 |   80.29 | 100.00\n &lt;NA&gt; |   0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nolder_male_hetero &lt;categorical&gt; \n# total N=1177 valid N=1154 mean=0.82 sd=0.39\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\n    0 | 209 | 17.76 |   18.11 |  18.11\n    1 | 945 | 80.29 |   81.89 | 100.00\n &lt;NA&gt; |  23 |  1.95 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nWie sind die Altersunterschiede unterteilt, unter Berücksichtiung des Geschlechts?\n\nGraphische Überprüfung\n\n\n\n\n\n\nExercise 4\n\n\n\n\nErstellen Sie, auf Basis des Datensatzes age_gaps_consistent, einen ggplot.\nNutzen Sie im Argument aes() die Variable age_difference als x-Variable und older_male_hetero für das Argument fill.\nNutzen Sie geom_bar zur Erzeugung des Plots.\nOptional: Verwenden Sie theme_pubr\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\nage_gaps_consistent %&gt;% \n  ggplot(aes(age_difference, fill = older_male_hetero)) +\n  geom_bar() +\n  labs(\n    x = \"Altersdifferenz (in Jahren)\",\n    y = 'Anzahl der \"Beziehungen\"'\n  ) +\n   scale_fill_manual(\n    name = \"Older partner in couple\",\n    values = c(\"0\" = \"#F8766D\", \"1\" = \"#00BFC4\", \"NA\" = \"grey\"),\n    labels = c(\"0\" = \"Woman\", \"1\" = \"Man\", \"NA\" = \"Same sex couples\")\n  ) +\n  theme_pubr() \n\n\n\n\n\n\nÜberprüfung durch Modellierung\n\nmdl &lt;- lm(age_difference ~ release_year + older_male_hetero, data = age_gaps_consistent)\n\n# Output\nmdl %&gt;% parameters::parameters()\n\nParameter             | Coefficient |    SE |           95% CI | t(1151) |      p\n---------------------------------------------------------------------------------\n(Intercept)           |      204.26 | 28.14 | [149.04, 259.47] |    7.26 | &lt; .001\nrelease year          |       -0.10 |  0.01 | [ -0.13,  -0.07] |   -7.08 | &lt; .001\nolder male hetero [1] |        6.03 |  0.61 | [  4.83,   7.23] |    9.87 | &lt; .001\n\nmdl %&gt;% performance::model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n8058.980 | 8059.015 | 8079.184 | 0.126 |     0.125 | 7.920 | 7.930\n\nmdl %&gt;% report::report()\n\nWe fitted a linear model (estimated using OLS) to predict age_difference with\nrelease_year and older_male_hetero (formula: age_difference ~ release_year +\nolder_male_hetero). The model explains a statistically significant and weak\nproportion of variance (R2 = 0.13, F(2, 1151) = 83.25, p &lt; .001, adj. R2 =\n0.12). The model's intercept, corresponding to release_year = 0 and\nolder_male_hetero = 0, is at 204.26 (95% CI [149.04, 259.47], t(1151) = 7.26, p\n&lt; .001). Within this model:\n\n  - The effect of release year is statistically significant and negative (beta =\n-0.10, 95% CI [-0.13, -0.07], t(1151) = -7.08, p &lt; .001; Std. beta = -0.20, 95%\nCI [-0.25, -0.14])\n  - The effect of older male hetero [1] is statistically significant and positive\n(beta = 6.03, 95% CI [4.83, 7.23], t(1151) = 9.87, p &lt; .001; Std. beta = 0.71,\n95% CI [0.57, 0.85])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "course-assignments.html",
    "href": "course-assignments.html",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g. different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (👥) or individual (👤) work and are marked as such. The expected group size is 2-3 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nTotal\n100 Pts\n\n\n\n\n👥 Presentation\n20 Pts\n\n\n👥 Project topic idea(s)\n10 Pts\n\n\n👥 Project proposal (draft report)\n15 Pts\n\n\n👤 Peer Review\n15 Pts\n\n\n👥 Written short report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g. which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g. the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).\nIf the final structure of the course contains several small projects, the listed assignments only have to be completed for one of the projects."
  },
  {
    "objectID": "course-assignments.html#sec-portfolio",
    "href": "course-assignments.html#sec-portfolio",
    "title": "Assingments",
    "section": "",
    "text": "In order to be able to adapt the assignments flexibly to the requirements of the course (e.g. different number of students, projects changing from semester to semester or several projects in one semester), this seminar uses a portfolio assessment. Even though the seminar aims at testing new forms of assignments due to its practical and empirical orientation, traditional assignments such as presentations or written reports will still be an integral part. However, these will be expanded to include the special features of working with digital behavioral data.\nAssignments can be group (👥) or individual (👤) work and are marked as such. The expected group size is 2-3 persons.\nThe following list contains an overview of the assignments to be completed in the course:\n\n\nBreakdown of the final grade\n\n\nTotal\n100 Pts\n\n\n\n\n👥 Presentation\n20 Pts\n\n\n👥 Project topic idea(s)\n10 Pts\n\n\n👥 Project proposal (draft report)\n15 Pts\n\n\n👤 Peer Review\n15 Pts\n\n\n👥 Written short report\n40 Pts\n\n\n\n\n\n\n\n\n\nImportant Disclaimer\n\n\n\n\nThe final structure of the course (e.g. which and how many small projects) will be discussed and determined together with the students in the first session. Accordingly, it is possible that the portfolio or the assignments contained in it will be adjusted subsequently (e.g. the scope, content or deadline of assignments could be changed or even entire assignments could be dropped).\nIf the final structure of the course contains several small projects, the listed assignments only have to be completed for one of the projects."
  },
  {
    "objectID": "course-assignments.html#sec-presentation",
    "href": "course-assignments.html#sec-presentation",
    "title": "Assingments",
    "section": "👥 Presentation (20 Pts)",
    "text": "👥 Presentation (20 Pts)\nThe topics for the presentation will be assigned at the beginning of the course. The focus of the presentation is either on presenting the theoretical basis of the project (📚) or the data source/collection (📦). The presentation should be 20 to 30 minutes long (including time for questions and discussion). You will be graded based on your individual contribution, so please clarify which slides are presented by whom (e.g., by adding the initials of the presenting individual in the footnote of the slide).\nLiterature for the preparation of the presentation will be provided. The texts relevant for the respective presentation can be found in the information on the preparation (📖) of the respective session. All texts listed in the section “Mandatory literature” constitute the presentation literature for your respective presentation. Please include all texts on your topic in your presentation, but feel free to set your own priorities in the presentation. You may also cite other sources, provided they enrich the subject matter.\nThe aim of the presentation is to give the course participants an overview on your topic, e.g., central terms, definitions and features of the respective platform, method and/or tool. The presentation of the state of research (what is the goal of the studies and what do they show?) plays a subordinate role.\nAdditionally, presenters are required to meet with the instructor in the week before their presentation for a mandatory feedback. My office hours are directly after the session, on Wednesdays from 13:15 to 14:15. If you have scheduling conflicts, we can arrange another meeting time. Meetings can take place in person or via Zoom.\nIn advance of the feedback meeting, a first complete draft of the presentation must be submitted as a PowerPoint or PDF file via mail at the latest 24 hours before the meeting, one week before the presentation. During the feedback meeting, students will receive detailed feedback and tips on how to revise their presentation. The revised presentations are then given in presence in the respective sessions. Afterwards, a PDF of the slides is made available to the seminar.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nAt the latest 24 hours before the pre-meeting: Send the first complete draft of the presentation slides by e-mail to christoph.adrian@fau.de\nOne week before your presentation: Pre-meeting to discuss the presentation draft during the office hours. Please arrange an alternative date in good time if you are unable to attend the scheduled pre-meeting.\nUntil 09:00 of the day of the presentation: Send the final draft of the presentation by e-mail to christoph.adrian@fau.de"
  },
  {
    "objectID": "course-assignments.html#sec-topic-ideas",
    "href": "course-assignments.html#sec-topic-ideas",
    "title": "Assingments",
    "section": "👥 Project topic idea(s) (10 Pts)",
    "text": "👥 Project topic idea(s) (10 Pts)\nShort or ad-hoc presentation (“pitch”) of the status or result of your groups project work. The pitch should be about 5 to 10 minutes long, consist of max. 5 slides and be held by one group member only.\nThe aim of the pitch is to give the course and update about your projects progress as well as a basis for discussion and feedback. Therefore, you should\n\nfirst give a short overview of your topic, research question or motivation and selected data (sub-)sample (2 slides),\ndescribe your method and analysis (1 slide) and\nshow results and/or a challenge you face, that should be discussed by the course (2 slide).\n\nAfter the pitch there will be time for questions, either from the group to the course or vice versa.\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nPrepare a short presentation before and present you pitch in every “Presentation & Discussion (📊)” session, using the Google Slides templates provided."
  },
  {
    "objectID": "course-assignments.html#sec-proposal",
    "href": "course-assignments.html#sec-proposal",
    "title": "Assingments",
    "section": "👥 Project proposal (15 pts)",
    "text": "👥 Project proposal (15 pts)\nThe project proposal is basically the frist written draft of your final short report, consisting of the write-up of your project topic idea and/or associated feedback. The idea is for you to think about the specific analysis strategy early on, as well as present initial results or evaluation. The project proposal is the basis for the peer review.\nThe project proposal should be at least 500 words and include the following points:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore and/or your hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data (sub-)sample you selected. This includes\n\ndescription of the “observations” in the data set,\ndescription of how the data was “originally” collected.\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the central method as well as necessary pre-processing steps of the data.\nDescription, visualization and/or summary statistics of the central variables.\n(Intended) analysis strategy with which you want to answer the research question/hypothesis (with initial results, if available)\n\nThe grading of your proposal will be as followd:\n\nProject proposal grading breakdown\n\n\nTotal\n15 pts\n\n\n\n\nIntroduction\n4 pts\n\n\nData description\n3 pts\n\n\nAnalysis plan\n7 pts\n\n\nOrganization + formatting\n1 pts\n\n\n\n\n\n\n\n\n\nImportant information, tasks & deadlines\n\n\n\n\nSubmit the draft project proposal using the provided Google Docs template no later than two weeks after the associated “Presentation & Discussion” session."
  },
  {
    "objectID": "course-assignments.html#sec-peer-review",
    "href": "course-assignments.html#sec-peer-review",
    "title": "Assingments",
    "section": "👤 Peer Review (15 pts)",
    "text": "👤 Peer Review (15 pts)\nCritically reviewing others’ work is a crucial part of the scientific process. Therefore, each group will be assigned one other group’s project proposal to review before the final submission. This way, you will be able to get additional feedback before handing in your final report.\nDuring the peer review process, you will be provided read-only access to your partner groups’ report via Google Docs. Provide your review in the form of another Google Doc using the template provided.\n\nProcess and questions\n\nOpen the Google Doc of the team you’re reviewing and read their project draft.\nThen, go to Peer Review Google Doc template, fill out the relevant fields and answer the following questions:\n\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the group might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?\n\nThe peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nProvide feedback using the provided Google Docs template until two weeks after you received the document for peer-reviewing."
  },
  {
    "objectID": "course-assignments.html#sec-written-report",
    "href": "course-assignments.html#sec-written-report",
    "title": "Assingments",
    "section": "👥 Written short report (40 pts)",
    "text": "👥 Written short report (40 pts)\nThe goal of the written short report is for each group to use at least one of the method or data presented to explore a topic of your own choosing. Choose both data and topic based on your group’s interests, experience or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis. You are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the word limit.\nThe written report should be 750 to 1000 words per person. However, when written as a group report, the number or words scale with a factor of 0.8 per person (e.g., a group of two should write 1200 to 1600 words, a group of three 1800 to 2400 words). All analyses as well as the written report must be done in RStudio and all components of the project must be reproducible. The mandatory components of the reports are: Introduction, Data/Methodology, Results, Discussion & Conclusion. You are free to add additional sections as necessary. You will be graded based on your individual contribution, so please clarify which part of the report was written by whom (e.g., by adding the initials of the author in the header of the section).\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\n\n\n\n\n\n\nImportant tasks & deadlines\n\n\n\n\nFinal submission of the revised written report is due 01.03.2024, 23:591."
  },
  {
    "objectID": "course-assignments.html#footnotes",
    "href": "course-assignments.html#footnotes",
    "title": "Assingments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that this is a temporary deadline. The final deadline will be adjusted in the course in consultation with the students if necessary.↩︎"
  }
]