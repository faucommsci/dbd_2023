---
title: "Showcase 09: üî® Text as data in R "
subtitle: "Digital disconnection on Twitter"
format: 
  html:
    toc: true
    toc-depth: 4
    fig-width: 16
    fig-height: 9
callout-appearance: simple
execute: 
  cache: true
  eval: true
  echo: true
  message: false
  warning: false
highlight-style: atom-one
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

::: {.callout-tip icon="false"}
[![Quarto Slides](/img/badge-quarto-slide.svg)]((/slides/slides-09.html)) Open session slides
:::

## Background

::: {.callout-note appearance="minimal"}
## Definition

::: notes
‚Äû**Digital disconnection** is a **deliberate** (i.e., chosen by the individual) form of **non-use** of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of **restoring or improving one‚Äôs perceived** overuse, social interactions, psychological **well-being**, productivity, privacy and/or perceived usefulness‚Äú. [@nassen2023]
:::
:::

-   Increasing trend towards **more conscious use of digital media (devices)**, including **(deliberate) non-use** with the aim to **restore or improve psychological well-being** (among other factors)

-   But how do "we" talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?

::: callout-important
## Todays's data basis: Twitter dataset

-   Collection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)
-   Initial query is searching for "digital detox", "#digitaldetox", "digital_detox"
-   Access via official Academic-Twitter-API via academictwitteR [@barrie2021] at the beginning of last year
:::

## Preparation

```{r load-packages}
source(here::here("slides/schedule.R"))

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here, qs, # file management
  magrittr, janitor, # data wrangling
  easystats, sjmisc, # data analysis
  tidytext, textdata, widyr, # text processing
  ggpubr, ggwordcloud, # visualization
  gt, gtExtras, # fancy tables
  tidyverse # load last to avoid masking issues
  )
```

## Import and process the data

```{r import-data}
# Import raw data from local
tweets <- qs::qread(here("local_data/tweets-digital_detox.qs"))$raw %>% 
  janitor::clean_names()

# Initial data processing
tweets_correct <- tweets %>% 
  mutate(
    # reformat and create datetime variables
    across(created_at, ~ymd_hms(.)), # convert to dttm format
    year = year(created_at), 
    month = month(created_at), 
    day = day(created_at), 
    hour = hour(created_at),
    minute = minute(created_at),
    # create addtional variables
    retweet_dy = str_detect(text, "^RT"), # identify retweets
    detox_dy = str_detect(text, "#digitaldetox") 
  ) %>% 
  distinct(tweet_id, .keep_all = TRUE)

# Filter relevant tweets
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )
```


## Digital disconnection on {{< fa brands twitter >}} (not ùïè) 
### The structure of the data set & available variables

```{r table-initial-overview}
tweets_correct %>% glimpse()
```

### Bonus: Distribution statistics

```{r table-location-parameter}
tweets_correct %>% skimr::skim()
```

### Distribution of tweets with reference to digital detox over time

```{r figure-tweets-by-year}
tweets_correct %>% 
  ggplot(aes(x = as.factor(year), fill = retweet_dy)) +
    geom_bar() +
    labs(
      x = "",
      y = "Number of tweets", 
      fill = "Is the tweet a retweet?"
     ) +
    scale_fill_manual(values = c("#1DA1F2", "#004389")) +
    theme_pubr() +    
    # add annotations
    annotate(
      "text", 
      x = 14, y = 55000,
      label = "Increase of character limit from 140 to 280") + 
    geom_curve(
      data = data.frame(
        x = 14.2965001234837,y = 53507.2283841571,
        xend = 11.5275706534335, yend = 45412.4966032138),
      mapping = aes(x = x, y = y, xend = xend, yend = yend),
      angle = 127L,
      curvature = 0.28,
      arrow = arrow(30L, unit(0.1, "inches"), "last", "closed"),
      inherit.aes = FALSE)
```

### Tweets with reference to ditigal detox by (participating) users 

```{r table-tweets-by-user}
tweets_correct %>% 
    group_by(author_id) %>% 
    summarize(n = n()) %>% 
    mutate(
        n_grp = case_when(
                     n <=    1 ~ 1,
            n >   1 & n <=  10 ~ 2,
            n >  10 & n <=  25 ~ 3,
            n >  25 & n <=  50 ~ 4,
            n >  50 & n <=  75 ~ 5,
            n >  75 & n <= 100 ~ 6,
            n > 100 ~ 7
        ), 
        n_grp_fct = factor(
            n_grp,
            levels = c(1:7),
            labels = c(
            " 1",
            " 2 - 10", "11 - 25",
            "26 - 50", "50 - 75",
            "75 -100", "> 100")
        )
    ) %>% 
    sjmisc::frq(n_grp_fct) %>% 
    as.data.frame() %>% 
    select(val, frq:cum.prc) %>% 
    # create table
    gt() %>% 
    # tab_header(
    #     title = "Tweets by users with a least on tweet"
    #    ) %>% 
    cols_label(
      val = "Number of tweets by user",
      frq = "n",
      raw.prc = html("%<sub>raw</sub>"),
      cum.prc = html("%<sub>cum</sub>")
  ) %>% 
  gt_theme_538()
```

### Tweets with reference to ditigal detox by language

```{r table-tweets-by-language}
tweets_correct %>% 
    sjmisc::frq(
        lang, 
        sort.frq = c("desc"), 
        min.frq = 2000
    ) %>% 
    as.data.frame() %>% 
    select(val, frq:cum.prc) %>% 
    # create table
    gt() %>% 
    # tab_header(
    #     title = "Language of the collected tweets"
    #   ) %>% 
    cols_label(
      val = "Twitter language code",
      frq = "n",
      raw.prc = html("%<sub>raw</sub>"),
      cum.prc = html("%<sub>cum</sub>")
  ) %>% 
  gt_theme_538()
```

## Text as data in R (Part I)
### Build a subsample: Tweets containing #digitaldetox

```{r recode-create-subsample}
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )
```

```{r figure-subsample-distribution-over-time}
#| code-fold: true
#| code-summary: "Expand for full code"
#| fig-align: "center"

tweets_correct %>% 
  ggplot(aes(x = as.factor(year), fill = detox_dy)) +
    geom_bar() +
    labs(
      x = "",
      y = "Number of tweets", 
      fill = "#digitaldetox used in tweet?"
     ) +
    scale_fill_manual(values = c("#A0ACBD50", "#1DA1F2")) +
    theme_pubr() 
```

### Bonus: distribution statistics

```{r table-location-parameter-subsample}
tweets_detox %>% skimr::skim()
```

### Transform data to tidy text

```{r output-tokenization}
# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
    # Remove HTML entities
    mutate(text = str_remove_all(text, remove_reg)) %>% 
    # Tokenization
    tidytext::unnest_tokens("text", text) %>% 
    # Remove stopwords
    filter(!text %in% tidytext::stop_words$word)

# Preview
tweets_tidy %>% 
    select(tweet_id, user_username, text) %>% 
    print(n = 5)
```

### Summarize all tokens over all tweets

```{r output-summarization}
#| output-location: column

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(text, sort = TRUE) 

# Preview Top 15 token
tweets_summarized %>% 
    print(n = 15)
```

### Visualization of Top 100 token

```{r figure-wordcloud}
#| fig-align: "center"

tweets_summarized %>% 
    top_n(100) %>% 
    ggplot(aes(label = text, size = n)) +
    ggwordcloud::geom_text_wordcloud() +
    scale_size_area(max_size = 30) +
    theme_minimal()
```


## Modeling realtionships between words

### Count word pairs within tweets

```{r output-word-pairs-count}
#| output-location: column

# Create word paris
tweets_word_pairs <- tweets_tidy %>% 
    widyr::pairwise_count(
        text,
        tweet_id,
        sort = TRUE)

# Preview
tweets_word_pairs %>% 
    print(n = 15)
```

### Summarize and correlate tokens within tweets

```{r ouptput-word-pairs-correlation}
#| output-location: column

# Create word correlation
tweets_pairs_corr <- tweets_tidy %>% 
    group_by(text) %>% 
    filter(n() >= 300) %>% 
    pairwise_cor(
        text, 
        tweet_id, 
        sort = TRUE)

# Preview
tweets_pairs_corr %>% 
    print(n = 15)
```

### Display correlates for specific token

```{r figure-word-pairs-correlates}
#| output-location: column
#| fig-height: 7
#| fig-width: 8

tweets_pairs_corr %>% 
  filter(
    item1 %in% c("detox", "digital")
    ) %>% 
  group_by(item1) %>% 
  slice_max(correlation, n = 5) %>% 
  ungroup() %>% 
  mutate(
    item2 = reorder(item2, correlation)
    ) %>% 
  ggplot(
    aes(item2, correlation, fill = item1)
    ) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_fill_manual(
    values = c("#C50F3C", "#04316A")) +
  theme_pubr()
```

## Dictionary based approach: Sentiment analysis

### Most commmon positive and negative words

```{r figure-sentiment-most-frequent-words}
#| output-location: column
#| fig-height: 7
#| fig-width: 7

tweets_sentiment_count <- tweets_tidy %>% 
  inner_join(
     get_sentiments("bing"),
     by = c("text" = "word"),
     relationship = "many-to-many") %>% 
  count(text, sentiment)
  
# Preview
tweets_sentiment_count %>% 
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>% 
  mutate(text = reorder(text, n)) %>%
  ggplot(aes(n, text, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(
    ~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) +
  scale_fill_manual(
    values = c("#C50F3C", "#007D29")) +
  theme_pubr()
```

#### Link word sentiment to tidy data

```{r output-sentiment-aggregated-by-tweet}
tweets_sentiment <- tweets_tidy %>% 
  inner_join(
     get_sentiments("bing"),
     by = c("text" = "word"),
     relationship = "many-to-many") %>% 
  count(tweet_id, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
  
# Check
tweets_sentiment 
```


#### Overall distribution sentiment by tweets

```{r figure-sentiment-distribution}
#| fig-align: center

tweets_sentiment %>% 
  ggplot(aes(as.factor(sentiment))) +
  geom_bar(fill = "#1DA1F2") +
  labs(
    x = "Sentiment (sum) of tweet", 
    y = "Number of tweets"
  ) +
  theme_pubr()
```


#### Development of tweet sentiment over the years

```{r figure-sentiment-distribution-over-time}
#| code-fold: true
#| code-summary: "Expand for full code"
#| fig-height: 8
#| fig-width: 12
#| fig-align: center

# Create first graph
g1 <- tweets_correct %>% 
  filter(tweet_id %in% tweets_sentiment$tweet_id) %>% 
  left_join(tweets_sentiment) %>% 
  sjmisc::rec(
    sentiment,
    rec = "-8:-1=negative; 0=neutral; 1:8=positive") %>%
  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +
    geom_bar() +
    labs(
      x = "",
      y = "Number of tweets", 
      fill = "Sentiment (sum) of tweet") +
    scale_fill_manual(values = c("#C50F3C", "#90A0AF", "#007D29")) +
    theme_pubr() 
    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Create second graph
g2 <- tweets_correct %>% 
  filter(tweet_id %in% tweets_sentiment$tweet_id) %>% 
  left_join(tweets_sentiment) %>% 
  sjmisc::rec(
    sentiment,
    rec = "-8:-1=negative; 0=neutral; 1:8=positive") %>%
  ggplot(aes(x = as.factor(year), fill = as.factor(sentiment_r))) +
    geom_bar(position = "fill") +
    labs(
      x = "",
      y = "Proportion of tweets", 
      fill = "Sentiment (sum) of tweet") +
    scale_fill_manual(values = c("#C50F3C", "#90A0AF", "#007D29")) +
    theme_pubr() 

# COMBINE GRPAHS
ggarrange(g1, g2,
          nrow = 1, ncol = 2, 
          align = "hv",
          common.legend = TRUE) 
```