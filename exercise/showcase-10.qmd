---
title: "Showcase 10: üî® Automatic analysis of text in R "
subtitle: "Digital disconnection on Twitter"
format: 
  html:
    fig-width: 16
    fig-height: 9
date: last-modified
date-format: "DD.MM.YYYY"
---

::: {.callout-tip icon="false"}
[![Quarto Slides](/img/badge-quarto-slide.svg)](https://faucommsci.github.io/dbd_2023/slides/slides-10.html) Open session slides
:::

## Background

::: {.callout-note appearance="minimal"}
## Definition

::: notes
‚Äû**Digital disconnection** is a **deliberate** (i.e., chosen by the individual) form of **non-use** of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of **restoring or improving one‚Äôs perceived** overuse, social interactions, psychological **well-being**, productivity, privacy and/or perceived usefulness‚Äú. [@nassen2023]
:::
:::

-   Increasing trend towards **more conscious use of digital media (devices)**, including **(deliberate) non-use** with the aim to **restore or improve psychological well-being** (among other factors)

-   But how do "we" talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?

::: callout-important
## Todays's data basis: Twitter dataset

-   Collection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)
-   Initial query is searching for "digital detox", "#digitaldetox", "digital_detox"
-   Access via official Academic-Twitter-API via academictwitteR [@barrie2021] at the beginning of last year
:::

## Preparation

```{r load-packages}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here, qs, # file management
  magrittr, janitor, # data wrangling
  easystats, sjmisc, # data analysis
  ggpubr, ggwordcloud, # visualization
  gt, gtExtras, # fancy tables
  tidytext, textdata, widyr, # tidy text processing
  quanteda, # quanteda text processing
  quanteda.textplots, 
  topicmodels, stm, 
  tidyverse # load last to avoid masking issues
  )
```

## Import and process the data

```{r import-data}
# Import raw data from local
tweets <- qs::qread(here("local_data/tweets-digital_detox.qs"))$raw %>% 
  janitor::clean_names()

# Initial data processing
tweets_correct <- tweets %>% 
  mutate(
    # reformat and create datetime variables
    across(created_at, ~ymd_hms(.)), # convert to dttm format
    year = year(created_at), 
    month = month(created_at), 
    day = day(created_at), 
    hour = hour(created_at),
    minute = minute(created_at),
    # create addtional variables
    retweet_dy = str_detect(text, "^RT"), # identify retweets
    detox_dy = str_detect(text, "#digitaldetox") 
  ) %>% 
  distinct(tweet_id, .keep_all = TRUE)

# Filter relevant tweets
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )

# Topic models 
stm_k0 <- qs::qread(here("local_data/stm_k0.qs"))
stm_exploration <- qs::qread(here("local_data/stm_exploration.qs"))
stm_results <- qs::qread(here("local_data/stm_results.qs"))
```

## Text as data in R (Part II)

### Step-by-step DTM creation

::: panel-tabset
#### tidytext

```{r create-dtm-tidytext}
# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dfm <- tweets_summarized %>% 
  cast_dfm(tweet_id, text, n)

# Preview
tweets_dfm
```

#### quanteda

```{r create-dfm-quanteda}
# Create corpus
quanteda_corpus <- tweets_detox %>% 
  quanteda::corpus(
    docid_field = "tweet_id", 
    text_field = "text"
  )

# Tokenize
quanteda_token <- quanteda_corpus %>% 
  quanteda::tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE, 
    remove_numbers = TRUE, 
    remove_url = TRUE, 
    split_tags = FALSE # keep hashtags and mentions
  ) %>% 
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(
    pattern = stopwords("en")
    )

# Convert to Document-Feature-Matrix (DFM)
quanteda_dfm <- quanteda_token %>% 
  quanteda::dfm()

# Preview
quanteda_dfm
```
:::

### Network of hashtags

::: panel-tabset
#### tidytext

```{r figure-hashtag-cooccurence-tidytext}

# Extract hashtags
tweets_hashtags <- tweets_detox %>% 
  mutate(hashtags = str_extract_all(text, "#\\S+")) %>%
  unnest(hashtags) 

# Extract most common hashtags
top50_hashtags_tidy <- tweets_hashtags %>% 
  count(hashtags, sort = TRUE) %>% 
  slice_head(n = 50) %>% 
  pull(hashtags)

# Visualize
tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```

#### quanteda

```{r figure-hashtag-cooccurence-quanteda}

# Extract DFM with only hashtags
quanteda_dfm_hashtags <- quanteda_dfm %>% 
  quanteda::dfm_select(pattern = "#*") 

# Extract most common hashtags 
top50_hashtags_quanteda <- quanteda_dfm_hashtags %>% 
  topfeatures(50) %>% 
  names()

# Construct feature-occurrence matrix of hashtags
quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  ) 
```
:::

### Topic modeling

#### Preparation

##### Pruning

```{r create-quanteda-stm}
quanteda_dfm_trim <- quanteda_dfm %>% 
  dfm_trim(
    min_docfreq = 0.0001, 
    max_docfreq = .099, 
    docfreq_type = "prop"
  )

# Convert for stm topic modeling
quanteda_stm <- quanteda_dfm_trim %>% 
    convert(to = "stm")
```

::: callout-warning
Converting the trimmmed DFM to an stm object will result in an errormessage

> *Warning message: \
> In dfm2stm(x, docvars, omit_empty = TRUE) : Dropped 46,670 empty document(s)*

This is due to the fact, that some tweets are "empty" or do not match with any feature after the pruning. To successfully match the stm results with the original data, the "empty" tweets need to be dropped. To identify the empty cases, run

```{r error-fix-export}

# Check if tweet contains feature
empty_docs <- Matrix::rowSums(as(quanteda_dfm_trim, "Matrix")) == 0 

# Create vector for empty tweet identification 
empty_docs_ids <- quanteda_dfm_trim@docvars$docname[empty_docs]
```


```{r error-fix-output}
#| eval: false

# Optional: Print indices of empty documents
if (any(empty_docs)) {
  cat("Indices of empty documents:", which(empty_docs), "\n")
  
  # Print corresponding docnames
  cat("Docnames of empty documents:", quanteda_dfm_trim@docvars$docname[empty_docs], "\n")
}
```

:::

#### Select model

##### based on k = 0

```{r model-stm-k0-estimation}
#| eval: false

# Estimate model
stm_k0 <- stm(
    quanteda_stm$documents, 
    quanteda_stm$vocab, 
    K = 0, 
    max.em.its = 50,
    init.type = "Spectral", 
    seed = 42 
  )
```

```{r model-stm-k0-preview}
# Preview
stm_k0
```

##### based on different model diagnostics

```{r model-stm-exploration-estimation}
#| eval: false

# Set up parallel processing using furrr
future::plan(future::multisession()) # use multiple sessions 

# Estimate multiple models
stm_exploration <- tibble(k = seq(from = 5, to = 85, by = 5)) %>% 
    mutate(mdl = furrr::future_map(k, ~stm::stm(
      documents =  quanteda_stm$documents,
      vocab = quanteda_stm$vocab, 
      K = ., 
      seed = 42,
      max.em.its = 1000,
      init.type = "Spectral",
      verbose = FALSE),
      .options = furrr::furrr_options(seed = 42))
  )
```

```{r model-stm-exploration-preview}
stm_exploration$mdl
```

#### Exploration

```{r create-model-diagnostics-stm-exploration}
#| eval: false

# Create heldout
heldout <- make.heldout(
  quanteda_stm$documents,
  quanteda_stm$vocab,
  seed = 42)

# Create model diagnostics
stm_results <- stm_exploration %>%
  mutate(exclusivity = map(mdl, exclusivity),
         semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),
         eval_heldout = map(mdl, eval.heldout, heldout$missing),
         residual = map(mdl, checkResiduals, quanteda_stm$documents),
         bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),
         lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))
```

```{r figure-model-diagnostics-stm-exploration}
# Preview 
stm_results

# Visualize
stm_results %>%
  transmute(
    k,
    `Lower bound` = lbound,
    Residuals = map_dbl(residual, "dispersion"),
    `Semantic coherence` = map_dbl(semantic_coherence, mean),
    `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -k) %>%
  ggplot(aes(k, Value, color = Metric)) +
    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
    geom_point(size = 3) +
    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +
    facet_wrap(~Metric, scales = "free_y") +
    labs(x = "K (number of topics)",
         y = NULL,
         title = "Model diagnostics by number of topics"
    ) +
    theme_pubr() +
    # add highlights 
    geom_vline(aes(xintercept =  5), color = "#00BFC4", alpha = .5) +
    geom_vline(aes(xintercept = 10), color = "#C77CFF", alpha = .5) +
    geom_vline(aes(xintercept = 40), color = "#C77CFF", alpha = .5) 
```

```{r figure-exclusivity-stm-exploration}
# Models for comparison
models_for_comparison = c(5, 10, 40)

# Create figures
fig_excl <- stm_results %>% 
  # Edit data
  select(k, exclusivity, semantic_coherence) %>%
  filter(k %in% models_for_comparison) %>%
  unnest(cols = c(exclusivity, semantic_coherence))  %>%
  mutate(k = as.factor(k)) %>%
  # Build graph
  ggplot(aes(semantic_coherence, exclusivity, color = k)) +
    geom_point(size = 2, alpha = 0.7) +
    labs(
      x = "Semantic coherence",
      y = "Exclusivity"
      # title = "Comparing exclusivity and semantic coherence",
      # subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity"
      ) +
      theme_pubr() 

# Create plotly
fig_excl %>% plotly::ggplotly()
```

#### Understanding

##### Select model for further analysis

```{r create-tpm}
n_topics <- 10

tpm <- stm_results |>
   filter(k == n_topics) |> 
   pull(mdl) %>% .[[1]]
```

##### Overview

```{r figure-stm-mdl-overview-1}
tpm %>% plot(type = "summary")
tpm %>% plot(type = "hist")
```

##### Document/Word-topic relations

```{r table-topic-word-gamma}
# Create data
top_gamma <- tpm %>%
  tidy(matrix = "gamma") %>% 
  dplyr::group_by(topic) %>%
  dplyr::summarise(gamma = mean(gamma), .groups = "drop") %>%
  dplyr::arrange(desc(gamma))

top_beta <- tpm %>%
  tidytext::tidy(.) %>% 
  dplyr::group_by(topic) %>%
  dplyr::arrange(-beta) %>%
  dplyr::top_n(10, wt = beta) %>% 
  dplyr::select(topic, term) %>%
  dplyr::summarise(terms_beta = toString(term), .groups = "drop")

top_topics_terms <- top_beta %>% 
  dplyr::left_join(top_gamma, by = "topic") %>%
  dplyr::mutate(
          topic = paste0("Topic ", topic),
          topic = reorder(topic, gamma)
      )

# Preview
top_topics_terms %>%
  mutate(across(gamma, ~round(.,3))) %>% 
  dplyr::arrange(-gamma) %>% 
  gt()
```

##### Document-topic-relations

```{r create-tweets-detox-topics}
# Prepare for merging
topic_gammas <- tpm %>%
  tidy(matrix = "gamma") %>% 
  dplyr::group_by(document) %>% 
  tidyr::pivot_wider(
    id_cols = document, 
    names_from = "topic", 
    names_prefix = "gamma_topic_",
    values_from = "gamma")
      
gammas <- tpm %>%
  tidytext::tidy(matrix = "gamma") %>% 
  dplyr::group_by(document) %>% 
  dplyr::slice_max(gamma) %>% 
  dplyr::mutate(main_topic = ifelse(gamma > 0.5, topic, NA)) %>% 
  rename(
    top_topic = topic,
    top_gamma = gamma) %>% 
  ungroup() %>% 
  left_join(., topic_gammas, by = join_by(document))

# Merge with original data
tweets_detox_topics <- tweets_detox %>% 
  filter(!(tweet_id %in% empty_docs_ids)) %>% 
  bind_cols(gammas) %>% 
  select(-document)

# Preview
tweets_detox_topics 
```

##### Topic distribution

```{r figure-topic-distribution}
tweets_detox_topics %>% 
  mutate(across(top_topic, as.factor)) %>% 
  ggplot(aes(top_topic)) +
  geom_bar() +
  theme_pubr()
```

##### Topic distribution over time

```{r figure-topics-distribution-over-time}
tweets_detox_topics %>% 
  mutate(across(top_topic, as.factor)) %>% 
  ggplot(aes(year, fill = top_topic)) +
  geom_bar() +
  theme_pubr()

tweets_detox_topics %>% 
  mutate(across(top_topic, as.factor)) %>% 
  ggplot(aes(year, fill = top_topic)) +
  geom_bar(position = "fill") +
  theme_pubr() 
```

##### Focus on specific topics

```{r table-topic-exploration-top-tweets}
tweets_detox_topics %>% 
  filter(top_topic == 10) %>% 
  arrange(-top_gamma) %>% 
  slice_head(n = 10) %>% 
  select(tweet_id, user_username, created_at, text, top_gamma) %>% 
  gt()
```

##### Top Users

```{r table-topic-exploration-top-users}
tweets_detox_topics %>% 
  filter(top_topic == 10) %>% 
  count(user_username, sort = TRUE) %>% 
  mutate(prop = round(n/sum(n)*100, 2)) %>% 
  slice_head(n = 15)
```