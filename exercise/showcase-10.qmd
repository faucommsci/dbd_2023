---
title: "Showcase 10: üî® Automatic analysis of text in R "
subtitle: "Digital disconnection on Twitter"
format: 
  html:
    toc: true
    toc-depth: 4
    fig-width: 16
    fig-height: 9
callout-appearance: simple
execute: 
  cache: true
  eval: false
  echo: true
  message: false
  warning: false
highlight-style: atom-one
editor_options: 
  chunk_output_type: console
bibliography: references_exercise.bib
---

::: {.callout-tip icon="false"}
<!-- TODO Update link to slides -->

[![Quarto Slides](/img/badge-quarto-slide.svg)]((../slides/slides-10.html)) Open session slides
:::

## Background

::: {.callout-note appearance="minimal"}
## Definition

::: notes
‚Äû**Digital disconnection** is a **deliberate** (i.e., chosen by the individual) form of **non-use** of devices, platforms, features, interactions, and/or messages that occurs with higher or lower frequencies, and for shorter or longer periods of time, after the initial adoption of these technologies, and with the aim of **restoring or improving one‚Äôs perceived** overuse, social interactions, psychological **well-being**, productivity, privacy and/or perceived usefulness‚Äú. [@nassen2023]
:::
:::

-   Increasing trend towards **more conscious use of digital media (devices)**, including **(deliberate) non-use** with the aim to **restore or improve psychological well-being** (among other factors)

-   But how do "we" talk about digital detox/disconnection: üíä drug, üëπ demon or üç© donut?

::: callout-important
## Todays's data basis: Twitter dataset

-   Collection of all tweets up to the beginning of 2023 that mention or discuss digital detox (and similar terms) on Twitter (not ùïè)
-   Initial query is searching for "digital detox", "#digitaldetox", "digital_detox"
-   Access via official Academic-Twitter-API via academictwitteR [@barrie2021] at the beginning of last year
:::

## Preparation

```{r load-packages}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here, qs, # file management
  magrittr, janitor, # data wrangling
  easystats, sjmisc, # data analysis
  ggpubr, ggwordcloud, # visualization
  gt, gtExtras, # fancy tables
  tidytext, textdata, widyr, # tidy text processing
  quanteda, # quanteda text processing
  quanteda.textplots, 
  topicmodels, stm, 
  tidyverse # load last to avoid masking issues
  )
```

## Import and process the data

```{r import-data}
# Import raw data from local
tweets <- qs::qread(here("local_data/tweets-digital_detox.qs"))$raw %>% 
  janitor::clean_names()

# Initial data processing
tweets_correct <- tweets %>% 
  mutate(
    # reformat and create datetime variables
    across(created_at, ~ymd_hms(.)), # convert to dttm format
    year = year(created_at), 
    month = month(created_at), 
    day = day(created_at), 
    hour = hour(created_at),
    minute = minute(created_at),
    # create addtional variables
    retweet_dy = str_detect(text, "^RT"), # identify retweets
    detox_dy = str_detect(text, "#digitaldetox") 
  ) %>% 
  distinct(tweet_id, .keep_all = TRUE)

# Filter relevant tweets
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )
```

## Text as data in R (Part II)

### Step-by-step DTM creation

::: panel-tabset
#### tidytext

```{r create-dtm-tidytext}
# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dfm <- tweets_summarized %>% 
  cast_dfm(tweet_id, text, n)

# Preview
tweets_dfm
```

#### quanteda

```{r}
# Create corpus
quanteda_corpus <- tweets_detox %>% 
  quanteda::corpus(
    docid_field = "tweet_id", 
    text_field = "text"
  )

# Tokenize
quanteda_token <- quanteda_corpus %>% 
  quanteda::tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE, 
    remove_numbers = TRUE, 
    remove_url = TRUE, 
    split_tags = FALSE # keep hashtags and mentions
  ) %>% 
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(
    pattern = stopwords("en")
    )

# Convert to Document-Feature-Matrix (DFM)
quanteda_dfm <- quanteda_token %>% 
  quanteda::dfm()

# Preview
quanteda_dfm
```
:::

### Co-occurence of hashtags

::: panel-tabset
#### tidytext

```{r}

# Extract hashtags
tweets_hashtags <- tweets_detox %>% 
  mutate(hashtags = str_extract_all(text, "#\\S+")) %>%
  unnest(hashtags) 

# Extract most common hashtags
top50_hashtags_tidy <- tweets_hashtags %>% 
  count(hashtags, sort = TRUE) %>% 
  slice_head(n = 50) %>% 
  pull(hashtags)

# Visualize
tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```

#### quanteda

```{r}
# Extract DFM with only hashtags
quanteda_dfm_hashtags <- quanteda_dfm %>% 
  quanteda::dfm_select(pattern = "#*") 

# Extract most common hashtags 
top50_hashtags_quanteda <- quanteda_dfm_hashtags %>% 
  topfeatures(50) %>% 
  names()

# Construct feature-occurrence matrix of hashtags
quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  ) 
```
:::

### Topic modeling

#### Preparation

##### Pruning

```{r}
quanteda_dfm_trim <- quanteda_dfm %>% 
  dfm_trim(
    min_docfreq = 0.0001, 
    max_docfreq = .099, 
    docfreq_type = "prop"
  )

quanteda_stm <- quanteda_dfm_trim %>% 
    convert(to = "stm")
```

::: callout-warning
Converting the trimmmed DFM to an stm object will result in an errormessage

> *Warning message: In dfm2stm(x, docvars, omit_empty = TRUE) : Dropped 46,670 empty document(s)*

This is due to the fact, that some tweets are "empty" or do not match with any feature after the pruning. 
To successfully match the stm results with the original data, the "empty" tweets need to be dropped.
To identify the empty cases, run 

```{r}
#| ouptut: false

# Check if tweet contains feature
empty_docs <- Matrix::rowSums(as(quanteda_dfm_trim, "Matrix")) == 0 

# Print indices of empty documents
if (any(empty_docs)) {
  cat("Indices of empty documents:", which(empty_docs), "\n")
  
  # Print corresponding docnames
  cat("Docnames of empty documents:", quanteda_dfm_trim@docvars$docname[empty_docs], "\n")
}

# Create vector for empty tweet identification 
empty_docs_ids <- quanteda_dfm_trim@docvars$docname[empty_docs]
```
::: 

#### Select model
##### based on k = 0
```{r}
#|¬†eval: false

stm_k0 <- stm(
    quanteda_stm$documents, 
    quanteda_stm$vocab, 
    K = 0, 
    max.em.its = 50,
    init.type = "Spectral", 
    seed = 42 
  )

qs::qsave(stm_k5, file = here("local_data/stm_k5.qs"))
```

##### based on different model diagnostics

```{r}
#| eval: false

# Set up parallel processing using furrr
future::plan(future::multisession()) # use multiple sessions 

# Estimate multiple models
stm_exploration <- tibble(K = seq(from = 5, to = 85, by = 5)) %>% 
    mutate(mdl = furrr::future_map(k, ~stm::stm(
      documents =  quanteda_stm$documents,
      vocab = quanteda_stm$vocab, 
      K = ., 
      seed = 42,
      max.em.its = 1000,
      init.type = "Spectral",
      verbose = FALSE),
      .options = furrr::furrr_options(seed = 42))
  )

qs::qsave(stm_exploration, file = here("local_data/stm_exploration.qs"))
```


```{r}
#| eval: true
#| echo: false

stm_k5 <- qs::qread(file = here("local_data/stm_k5.qs"))
```

#### Exploration
##### Top Topics
```{r}
stm_k5 %>% plot(type = "summary")
stm_k5 %>% plot(type = "hist")
stm_k5 %>% labelTopics()
stm_k5 %>% findThoughts(topics = 2)
stm_k5 %>% 
  topicCorr(method = "huge") %>% 
  plot()

```


##### Create prevalence

```{r}
# Create document-topic matrix (gamma)
td_gamma <- stm_k5 %>%
  tidytext::tidy(matrix = "gamma")

gamma_dummies <- td_gamma %>%
  dplyr::group_by(document) %>% 
  tidyr::pivot_wider(
    id_cols = document, 
    names_from = "topic", 
    names_prefix = "gamma_topic_",
    values_from = "gamma") %>% 
  rename(doc_id_gamma = document)
      
top_gamma <- td_gamma %>% 
  dplyr::group_by(document) %>% 
  dplyr::slice_max(gamma) %>% 
  dplyr::mutate(main_topic = ifelse(gamma > 0.5, topic, NA)) %>% 
  rename(
    top_topic = topic,
    top_gamma = gamma)

tweets_detox_topics <- tweets_detox %>% 
  filter(!(tweet_id %in% empty_docs_ids)) %>% 
  bind_cols(top_gamma)
  

```

##### Distribution of topics over time

```{r}
tweets_detox_topics %>% 
  mutate(across(top_topic, as.factor)) %>% 
  ggplot(aes( top_topic, fill = top_topic)) +
  geom_bar()+ 
  facet_grid(rows = vars(year)) +
  theme_minimal()
```

##### Focus on specific topics

```{r}
tweets_detox_topics %>% 
  filter(top_topic == 3) %>% 
  arrange(-top_gamma) %>% 
  slice_head(n = 10) %>% 
  select(tweet_id, user_username, created_at, text, top_gamma) %>% 
  gt()

```

##### Top Users

```{r}
tweets_detox_topics %>% 
  filter(top_topic == 3) %>% 
  sjmisc::frq(user_username)

```



<!-- TODO Exercise; Create FCM without #digitaldetox -->


<!-- TODO Slides: Dispaly "feature" reduction -->

<!-- TODO Exercise Re-Run analysis with deduplication -->