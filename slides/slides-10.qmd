---
title: "üî® Automatic text analysis in R"
subtitle: "Session 10"
date: 17 01 2024
date-format: "DD.MM.YYYY"
filters:
  - roughnotation
  - line-highlight
bibliography: references_slides.bib
editor_options: 
  chunk_output_type: console
---

# `print("Hello (again)!")` {background-image="img/slide_bg-section.png"}

üë®‚Äçüíª **Christoph Adrian**

-   PhD student \@ [Chair of Communication Science](https://www.kowi.rw.fau.de/team/christoph-adrian/)
-   Text as Data & Social Media Usage (Effects)
-   Conversational in R, Beginner in Python, SPSS & Stata

## Schedule

```{r setup-slide-session}
#| echo: false

# Load packages
source(here::here("slides/schedule.R"))

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here, qs, # file management
  magrittr, janitor, # data wrangling
  easystats, sjmisc, # data analysis
  ggpubr, ggwordcloud, # visualization
  gt, gtExtras, # fancy tables
  tidytext, textdata, widyr, # tidy text processing
  quanteda, # quanteda text processing
  quanteda.textplots, 
  topicmodels, stm, 
  tidyverse # load last to avoid masking issues
  )
```

```{r table-schedule-b}
#| echo: false 

new_schedule_d %>% 
    gt::gt() %>% 
    gt::fmt_markdown() %>% 
    gt::tab_options(
        table.width = gt::pct(75), 
        table.font.size = "12px"
    ) %>% 
    gtExtras::gt_theme_nytimes() %>% 
    # mark session
    gtExtras::gt_highlight_rows(
        rows = 15,
        fill = "#C50F3C", alpha = 0.15,
        bold_target_only = TRUE,
        target_col = Topic
    ) %>%
    # mark past Session
    gt::tab_style(
        style = cell_text(
            style = "italic", 
            color = "grey"),
        location = cells_body(
            columns = everything(), 
            rows = c(1:12, 14)
        )
    )
```

# Agenda {background-image="img/slide_bg-agenda.png"}

1.  [What we did so far](#intro)
2.  [Text as data in R (Part II)](#examples)
3.  [üìã Hands on working with R](#exercises)

# What we did so far {#intro background-image="img/slide_bg-section.png"}

Analysis of tweets about digital disconnection on {{< fa brands twitter >}} (not ùïè)

```{r import-data-silent}
#| echo: false

# Import data from URL
tweets <- qs::qread(here("local_data/tweets-digital_detox.qs"))$raw %>% 
  janitor::clean_names()

# Correct raw data
tweets_correct <- tweets %>% 
  mutate(
    # reformat and create datetime variables
    across(created_at, ~ymd_hms(.)), # convert to dttm format
    year = year(created_at), 
    month = month(created_at), 
    day = day(created_at), 
    hour = hour(created_at),
    minute = minute(created_at),
    # create addtional variables
    retweet_dy = str_detect(text, "^RT"), # identify retweets
    detox_dy = str_detect(text, "#digitaldetox") 
  ) %>% 
  distinct(tweet_id, .keep_all = TRUE)

# Create subsample 
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )

# TPM 
stm_k0 <- qs::qread(here("local_data/stm_k0.qs"))
stm_exploration <- qs::qread(here("local_data/stm_exploration.qs"))
stm_results <- qs::qread(here("local_data/stm_results.qs"))

# Base data with topics 
tweets_detox_topics <- qs::qread(here("local_data/tweets-digital-detox-topics.qs"))
```

## Social Media as üíä, üëπ or üç© ?

#### Discussion about digital disconnection on twitter

-   Increasing trend towards **more conscious use of digital media (devices)**, including **(deliberate) non-use** with the aim to **restore or improve psychological well-being** (among other factors)

-   Collect **all tweets (until 31.12.2022)** via *Twitter Academic Research Product Track v2 API* & `academictwitteR` package [@barrie2021] that **mention or discuss digital detox** (and similar terms)

-   Dataset for session is a subsample (n = `r nrow(tweets_detox)`) with only tweets that contain `#digitaldetox`.

## The tidy text format pipeline basics

#### Focus on single words and their relationship documents & sentiments

::: r-stack
![](https://www.tidytextmining.com/images/tmwr_0101.png){fig-align="center"}

![](https://www.tidytextmining.com/images/tmwr_0201.png){.fragment fig-align="center"}
:::

::: {style="text-align: center"}
@silge2017
:::

# Text as data in R {#examples background-image="img/slide_bg-example.png"}

**Part II:** Document-Term-Matrices & Unsupervised Text Analysis (Topic Modeling)

## Expansion of the pipeline

#### Focus on modeling the realtionships between words & documents

![](https://www.tidytextmining.com/images/tmwr_0501.png){fig-align="center"}

::: {style="text-align: center"}
@silge2017
:::

## Quick recap on [Document-Term Matrix](https://en.wikipedia.org/wiki/Document-term_matrix) \[DTM\]

#### Most common structure for (classic) text mining

::: columns
::: {.column width="50%"}
A matrix where:

-   **each row represents one document** (such as a tweet),

-   **each column represents one term,** and

-   **each value** (typically) contains the **number of appearances of that term** in that document.
:::

::: {.column width="50%"}
![](https://www.oreilly.com/api/v2/epubs/9781491953235/files/assets/feml_0405.png){fig-align="center"}

::: {style="text-align: center"}
[@zheng2018]
:::
:::
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: [Tokenize]{.underline}

::: {style="font-size: smaller"}
```{r create-dtm-tidytext-1}
#| output-location: column

# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Preview
tweets_tidy %>% 
  select(tweet_id, user_name, text) %>% 
  print(n = 15)
```
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: Tokenize ‚ñ∂Ô∏è [Summarize]{.underline}

::: {style="font-size: smaller"}
```{r create-dtm-tidytext-2}
#| output-location: column
#| code-line-numbers: "11-17"


# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Preview 
tweets_summarized %>% 
  print(n = 15)
```
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: Tokenize ‚ñ∂Ô∏è Summarize ‚ñ∂Ô∏è [DTM]{.underline}

::: {style="font-size: smaller"}
```{r create-dtm-tidytext-3}
#| output-location: column
#| code-line-numbers: "15-20"


# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dtm <- tweets_summarized %>% 
  cast_dtm(tweet_id, text, n)

# Preview
tweets_dtm
```
:::

## Choose or combine styles

#### Simple with tidytext, precise with quanteda

::: {style="font-size: smaller"}
::: columns
::: {.column width="50%"}
```{r create-dtm-tidyverse-comparison}
#| eval: false

# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dtm <- tweets_summarized %>% 
  cast_dtm(tweet_id, text, n)

# Preview
tweets_dtm
```
:::

::: {.column width="50%"}
```{r create-dfm-quanteda-comparison}
#| output: false

# Create corpus
quanteda_corpus <- tweets_detox %>% 
  quanteda::corpus(
    docid_field = "tweet_id", 
    text_field = "text"
  )

# Tokenize
quanteda_token <- quanteda_corpus %>% 
  quanteda::tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE, 
    remove_numbers = TRUE, 
    remove_url = TRUE, 
    split_tags = FALSE # keep hashtags and mentions
  ) %>% 
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(
    pattern = stopwords("en")
    )

# Convert to Document-Feature-Matrix (DFM)
quanteda_dfm <- quanteda_token %>% 
  quanteda::dfm()
```
:::
:::
:::

## An example: Network of hashtags {auto-animate="true"}

#### Comparison between [tidytext]{.underline} & quanteda

::: {style="font-size: smaller"}
```{r figure-hashtags-cooccurence-tidytext}
#| output-location: column
#| fig-height: 9
#| fig-width: 9

# Extract hashtags
tweets_hashtags <- tweets_detox %>% 
  mutate(hashtags = str_extract_all(
    text, "#\\S+")) %>%
  unnest(hashtags) 

# Extract most common hashtags
top50_hashtags_tidy <- tweets_hashtags %>% 
  count(hashtags, sort = TRUE) %>% 
  slice_head(n = 50) %>% 
  pull(hashtags)

# Visualize
tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```
:::

## An example: Network of hashtags {auto-animate="true"}

#### Comparison between tidytext & [quanteda]{.underline}

::: {style="font-size: smaller"}
```{r figure-hashtags-cooccurence-quanteda}
#| output-location: column
#| fig-height: 9
#| fig-width: 9

# Extract DFM with only hashtags
quanteda_dfm_hashtags <- quanteda_dfm %>% 
  quanteda::dfm_select(pattern = "#*") 

# Extract most common hashtags 
top50_hashtags_quanteda <- quanteda_dfm_hashtags %>% 
  topfeatures(50) %>% 
  names()

# Construct feature-occurrence matrix of hashtags
quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  ) 
```
:::

## An example: Network of hashtags

#### Comparison between tidytext & quanteda

::: columns
::: {.column width="50%"}
```{r figure-hashtag-comparison-tidytext}
#| code-fold: true
#| code-summary: "Expand for full code"
#| fig-height: 9
#| fig-width: 9

tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```
:::

::: {.column width="50%"}
```{r figure-hashtag-comparison-quanteda}
#| code-fold: true 
#| code-summary: "Expand for full code"
#| fig-height: 9
#| fig-width: 9

quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  )
```
:::
:::

## A new input in the pipeline

#### Unsupervised learning example: *Topic modeling*

![](https://www.tidytextmining.com/images/tmwr_0601.png){fig-align="center"}

::: {style="text-align: center"}
@silge2017
:::

## Building a shared vocabulary ... again

#### Important terms and definitions

-   **`Topic Modeling`**: Form of unsupervised machine learning method used to exploratively identify topics in a corpus. Often, these are so-called **mixed-membership models**.
-   **`K`**: Number of topics to be calculated for a given a topic model.
-   **`Word-Topic-Matrix`**: Matrix describing the conditional probability (**`beta`**) with which a feature is prevalent in a given topic.
-   **`Document-Topic-Matrix`**: Matrix describing the conditional probability (**`gamma`**) with which a topic is prevalent in a given document.

## Beyond LDA

#### Different topic modeling approaches

-   *Latent Dirichlet Allocation \[`LDA`\]* [@blei2003] is a probabilistic generative model that assumes each document in a corpus is a mix of topics and each word in the document is attributable to one of the document's topics.
-   **Structural Topic Modeling \[`STM`\]** [@roberts2016; @roberts2019] extends LDA by incorporating document-level covariates, allowing for the modeling of how external factors influence topic prevalence.
-   *Word embeddings* (`Word2Vec` [@mikolov2013] , `Glove` [@pennington2014]) represent words as continuous vectors in a high-dimensional space, capturing semantic relationships between words based on their context in the data.
-   *Topic Modeling with Neural Networks* (`BERTopic`[@devlin2019], `Doc2Vec`[@le2014]) leverages deep learning architectures to automatically learn latent topics from textual data.

::: notes
-   specify the presumed number of topics K thatyou expect to find in a corpus (e.g., K = 5, i.e., 5 topics)
-   the model then tries to inductively identify 5 topics in the corpus based on the distribution of frequently co-occurring features.
-   an algorithm is used for this purpose, which is why topic modeling is a type of ‚Äúmachine learning‚Äù.
:::

## Preparation is everything

#### Suggested pre-processing steps (based on @maier2018)

::: columns
::: {.column width="50%"}
1.  ‚ö†Ô∏è Deduplication;
2.  ‚úÖ tokenization;
3.  ‚úÖ transforming all characters to lowercase;
4.  ‚úÖ removing punctuation and special characters;
5.  ‚úÖ Removing stop-words;
6.  ‚ö†Ô∏è term unification (lemmatizing or stemming);
7.  üèóÔ∏è relative pruning (attributed to [Zipf‚Äôs law](https://en.wikipedia.org/wiki/Zipf%27s_law));
:::

::: {.column width="50%"}
```{r create-stm-data}
#| eval: false

# Pruning
quanteda_dfm_trim <- quanteda_dfm %>% 
  dfm_trim( 
    min_docfreq = 10/nrow(tweets_detox),
    max_docfreq = 0.99, 
    docfreq_type = "prop")

# Convert for stm topic modeling
quanteda_stm <- quanteda_dfm_trim %>% 
   convert(to = "stm")
```
:::
:::

::: notes
Zipf‚Äôs law states that the frequency that a word appears is inversely proportional to its rank.
:::

## How to find K

#### The most important question of model selection

-   The **choice of K** (whether the model is instructed to identify 5, 15, or 100 topics), has a **substantial impact on results**:
    -   The smaller K, the more fine-grained and usually the more exclusive topics;
    -   the larger K, the more clearly topics identify individual events or issues.
-   The **stm** package [@roberts2019] has two build in solution to find the optimal K
    -   `searchK()` function
    -   setting `K = 0` when estimating the model
-   ***Recommendation for `stm`: (Manual) training and evaluation!***

::: notes
However, with a larger K topics are oftentimes less exclusive, meaning that they somehow overlap.
:::

## Train and evaluate topic models

#### Better than `searchK()`: Manual exploration

::: columns
::: {.column width="50%"}
```{r model-stm-exploration-estimation}
#| eval: false

# Set up parallel processing using furrr
future::plan(future::multisession()) 

# Estimate multiple models
stm_exploration <- tibble(
  k = seq(from = 5, to = 85, by = 5)
  ) %>%
  mutate(
    mdl = furrr::future_map(
      k, 
      ~stm::stm(
        documents = quanteda_stm$documents,
        vocab = quanteda_stm$vocab, 
        K = ., 
        seed = 42,
        max.em.its = 1000,
        init.type = "Spectral",
        verbose = FALSE),
    .options = furrr_options(seed = 42))
  )
```
:::

::: {.column width="50%"}
```{r model-stm-exploration-preview}
stm_exploration$mdl
```
:::
:::

## Semantic coherence as the key

#### Different model statistics for evaluation

```{r figure-model-diagnostics-stm-exploration}
#| code-fold: true
#| code-summary: "Expand for full code"
#| fig-height: 6
#| fig-width: 11
#| fig-align: center

stm_results %>%
  transmute(
    k,
    `Lower bound` = lbound,
    Residuals = map_dbl(residual, "dispersion"),
    `Semantic coherence` = map_dbl(semantic_coherence, mean),
    `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -k) %>%
  ggplot(aes(k, Value, color = Metric)) +
    geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
    geom_point(size = 3) +
    scale_x_continuous(breaks = seq(from = 5, to = 85, by = 5)) +
    facet_wrap(~Metric, scales = "free_y") +
    labs(x = "K (number of topics)",
         y = NULL,
         title = "Model diagnostics by number of topics"
    ) +
    theme_pubr() +
    # add highlights 
    geom_vline(aes(xintercept =  5), color = "#00BFC4", alpha = .5) +
    geom_vline(aes(xintercept = 10), color = "#C77CFF", alpha = .5) +
    geom_vline(aes(xintercept = 40), color = "#C77CFF", alpha = .5) 
```

::: notes
Semantic Coherence: tells you how coherent topics are, i.e., how often features describing a topic co-occur and topics thus appear to be internally coherent.

Exclusivity: tells you how exclusive topics are, i.e., how much they differ from each other and topics thus appear to describe different things.
:::

## Finding the best trade-off

#### Comparison of selected models based on exclusivty and semantic coherence

```{r figure-exclusivity-stm-exploration}
#| code-fold: true
#| code-summary: "Expand for full code"
#| fig-align: center

# Models for comparison
models_for_comparison = c(5, 10, 40)

# Create figures
stm_results %>% 
  # Edit data
  select(k, exclusivity, semantic_coherence) %>%
  filter(k %in% models_for_comparison) %>%
  unnest(cols = c(exclusivity, semantic_coherence))  %>%
  mutate(k = as.factor(k)) %>%
  # Build graph
  ggplot(aes(semantic_coherence, exclusivity, color = k)) +
    geom_point(size = 2, alpha = 0.7) +
    labs(
      x = "Semantic coherence",
      y = "Exclusivity"
      # title = "Comparing exclusivity and semantic coherence",
      # subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity"
      ) +
    theme_pubr() 
```

## A first overview

#### Understanding the 'final' model (k = 10)

```{r create-tpm}
#| echo: false
n_topics <- 10

tpm <- stm_results |>
   filter(k == n_topics) |> 
   pull(mdl) %>% .[[1]]
```

```{r figure-stm-mdl-overview-1}
tpm %>% plot(type = "summary")
```

## A more detailed overview

#### Understanding the 'final' model (k = 10)

```{r table-topic-word-gamma}
#| code-fold: true
#| code-summary: "Expand for full code"

# Create data
top_gamma <- tpm %>%
  tidy(matrix = "gamma") %>% 
  dplyr::group_by(topic) %>%
  dplyr::summarise(gamma = mean(gamma), .groups = "drop") %>%
  dplyr::arrange(desc(gamma))

top_beta <- tpm %>%
  tidytext::tidy(.) %>% 
  dplyr::group_by(topic) %>%
  dplyr::arrange(-beta) %>%
  dplyr::top_n(10, wt = beta) %>% 
  dplyr::select(topic, term) %>%
  dplyr::summarise(terms_beta = toString(term), .groups = "drop")

top_topics_terms <- top_beta %>% 
  dplyr::left_join(top_gamma, by = "topic") %>%
  dplyr::mutate(
          topic = reorder(topic, gamma)
      )

# Preview
top_topics_terms %>%
  mutate(across(gamma, ~round(.,3))) %>% 
  dplyr::arrange(-gamma) %>% 
  gt() %>% 
  cols_label(
    topic = "Topic", 
    terms_beta = "Top Terms (based on beta)",
    gamma = "Gamma"
  ) %>% 
  gtExtras::gt_theme_538()
```

## Results in a different context

#### Merge back with original data for further analysis and comparison

```{r figure-topic-distribution}
#| code-fold: true
#| code-summary: "Expand for full code"
#| layout-ncol: 2
#| fig-height: 8


top_gamma %>% 
  ggplot(aes(as.factor(topic), gamma)) +
  geom_col(fill = "#F57350") +
  labs(
    x = "Topic",
    y = "Mean gamma"
  ) +
  coord_flip() +
  scale_y_reverse() +
  scale_x_discrete(position = "top") +
  theme_pubr()

tweets_detox_topics %>% 
  mutate(across(top_topic, as.factor)) %>% 
  ggplot(aes(top_topic)) +
  geom_bar(fill = "#1DA1F2") +
  labs(
    x = "", 
    y = "Number of tweets"
  ) +
  coord_flip() +
  theme_pubr()
```

## Most representative tweets for Topic 10

#### Check interpretability and relevance of topics

```{r table-topic-exploration-top-tweets}
#| code-fold: true
#| code-summary: "Expand for full code"

tweets_detox_topics %>% 
  filter(top_topic == 10) %>% 
  arrange(-top_gamma) %>% 
  slice_head(n = 10) %>% 
  select(tweet_id, user_username, created_at, text, top_gamma) %>% 
  gt() %>% 
  gtExtras::gt_theme_538() %>% 
  gt::tab_options(table.font.size = "10px")

```

## Users with most tweets about Topic 10

#### Check interpretability and relevance of topics

```{r table-topic-exploration-top-users}
#| output-location: column

tweets_detox_topics %>% 
  filter(top_topic == 10) %>% 
  count(user_username, sort = TRUE) %>% 
  mutate(
    prop = round(n/sum(n)*100, 2)) %>% 
  slice_head(n = 10) %>% 
  gt() %>% 
  gtExtras::gt_theme_538() 
```

## Validate, validate, validate!

#### Things to remember about topic models

-   topic models are a **useful tool for automated content analysis**, both when exploring a large amount of data and when it comes to systematically identifying relationships between topics and other variables

-   **certain prerequisites** such as minimum size and variety of the corpus (namely on the level of words and documents and their relation to each other) **need to be met for a conclusive model**

-   everything above a certain degree of word frequencies is considered a ‚Äútopic,‚Äù even if it is not a topic in human interpretation

-   Reading the tea leaves [@chang2009] or (again): validate, validate, validate (e.g. with [`oolong`](https://github.com/gesistsa/oolong) package [@chan2020])

# üìã Hands on working with R {#group-activity background-image="img/slide_bg-group_activity.png"}

Various exercises on the content of today's session

## üß™ And now ... you: Model away!

::: {.callout-caution appearance="simple"}
## Objective of this exercise

-   Brief review of the contents of the last session
-   Teaching the basic steps for creating and analyzing document feature matrices and stm topic models
:::

#### Next steps

-   Download files provided on StudOn or shared drives for the sessions
-   [**`Unzip`**]{.underline} the archive at a destination of your choice.
-   Double click on the `Exercise-Automatic_text_analysis.Rproj` to open the RStudio project. This ensures that all dependencies are working correctly.
-   Open the `exercise.qmd` file and follow the instructions.
-   ***Tip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs).***

# Time for questions {background-image="img/slide_bg-question.png"}

# Thank you! {background-image="img/slide_bg-end_session.png"}

## References

::: {#refs}
:::