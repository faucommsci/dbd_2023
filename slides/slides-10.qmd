---
title: "üî® Automatic text analysis in R"
subtitle: "Session 10"
date: 17 01 2024
date-format: "DD.MM.YYYY"
filters:
  - roughnotation
  - line-highlight
bibliography: references_slides.bib
editor_options: 
  chunk_output_type: console
---

# `print("Hello (again)!")` {background-image="img/slide_bg-section.png"}

üë®‚Äçüíª **Christoph Adrian**

-   PhD student \@ [Chair of Communication Science](https://www.kowi.rw.fau.de/team/christoph-adrian/)
-   Text as Data & Social Media Usage (Effects)
-   Conversational in R, Beginner in Python, SPSS & Stata

## Schedule

```{r setup-slide-session}
#| echo: false

# Load packages
source(here::here("slides/schedule.R"))

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  here, qs, # file management
  magrittr, janitor, # data wrangling
  easystats, sjmisc, # data analysis
  ggpubr, ggwordcloud, # visualization
  gt, gtExtras, # fancy tables
  tidytext, textdata, widyr, # tidy text processing
  quanteda, # quanteda text processing
  quanteda.textplots, 
  topicmodels, stm, 
  tidyverse # load last to avoid masking issues
  )
```

```{r table-schedule-b}
#| echo: false 

new_schedule_d %>% 
    gt::gt() %>% 
    gt::fmt_markdown() %>% 
    gt::tab_options(
        table.width = gt::pct(75), 
        table.font.size = "12px"
    ) %>% 
    gtExtras::gt_theme_nytimes() %>% 
    # mark session
    gtExtras::gt_highlight_rows(
        rows = 15,
        fill = "#C50F3C", alpha = 0.15,
        bold_target_only = TRUE,
        target_col = Topic
    ) %>%
    # mark past Session
    gt::tab_style(
        style = cell_text(
            style = "italic", 
            color = "grey"),
        location = cells_body(
            columns = everything(), 
            rows = c(1:12, 14)
        )
    )
```

# Agenda {background-image="img/slide_bg-agenda.png"}

1.  [Quick recap}](#intro)
2.  [Text as data in R (Part II)](#examples)
3.  [üìã Hands on working with R](#exercises)

# Quick recap {#intro background-image="img/slide_bg-section.png"}

Analysis of tweets about digital disconnection on {{< fa brands twitter >}} (not ùïè)

```{r import-data-silent}
#| echo: false

# Import data from URL
tweets <- qs::qread(here("local_data/tweets-digital_detox.qs"))$raw %>% 
  janitor::clean_names()

# Correct raw data
tweets_correct <- tweets %>% 
  mutate(
    # reformat and create datetime variables
    across(created_at, ~ymd_hms(.)), # convert to dttm format
    year = year(created_at), 
    month = month(created_at), 
    day = day(created_at), 
    hour = hour(created_at),
    minute = minute(created_at),
    # create addtional variables
    retweet_dy = str_detect(text, "^RT"), # identify retweets
    detox_dy = str_detect(text, "#digitaldetox") 
  ) %>% 
  distinct(tweet_id, .keep_all = TRUE)

# Create subsample 
tweets_detox <- tweets_correct %>% 
  filter(
    detox_dy == TRUE, # only tweets with #digitaldetox
    retweet_dy == FALSE, # no retweets
    lang == "en" # only english tweets
    )
```

## Social Media as üíä, üëπ or üç© ?

#### Discussion about digital disconnection on twitter

-   Increasing trend towards **more conscious use of digital media (devices)**, including **(deliberate) non-use** with the aim to **restore or improve psychological well-being** (among other factors)

-   Collect **all tweets (until 31.12.2022)** via *Twitter Academic Research Product Track v2 API* & `academictwitteR` package [@barrie2021] that **mention or discuss digital detox** (and similar terms)

-   Dataset for session is a subsample (n = `r nrow(tweets_detox)`) with only tweets that contain `#digitaldetox`.

## The tidy text format pipeline basics

#### Focus on single words and their relationship to sentiments & documents

::: r-stack
![](https://www.tidytextmining.com/images/tmwr_0101.png){fig-align="center"}

![](https://www.tidytextmining.com/images/tmwr_0201.png){.fragment fig-align="center"}
:::

::: {style="text-align: center"}
@silge2017
:::

# Text as data in R {#examples background-image="img/slide_bg-example.png"}

**Part II:** Document-Term-Matrices & Topic Modeling

## Focus on relationships

#### Modeling realtionships between words & documents

-   **Three variants of automatic text analysis:**

    -   *Dictionary Approaches (e.g. Sentiment Analysis)*

    -   üîé **Unsupervised Text Analysis (e.g. Topic Modeling)**

    -   Supervised Text Analysis (e.g. ML Classifier)

## Expansion of the pipeline

#### Focus on the word-document relationship

![](https://www.tidytextmining.com/images/tmwr_0501.png){fig-align="center"}

## **Tidying a document-term matrix**

#### Quick recap on the DTM fundamentals

One of the most common structures that text mining packages work with is the [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (or DTM).

::: columns
::: {.column width="50%"}
This is a matrix where:

-   each row represents one document (such as a tweet),

-   each column represents one term, and

-   each value (typically) contains the number of appearances of that term in that document.
:::

::: {.column width="50%"}
![[@zheng2018]](https://www.oreilly.com/api/v2/epubs/9781491953235/files/assets/feml_0405.png){fig-align="center"}
:::
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: [Tokenize]{.underline}

::: {style="font-size: smaller"}
```{r}
#| output-location: column

# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Preview
tweets_tidy %>% 
  select(tweet_id, user_name, text) %>% 
  print(n = 15)
```
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: Tokenize ‚ñ∂Ô∏è [Summarize]{.underline}

::: {style="font-size: smaller"}
```{r}
#| output-location: column
#| code-line-numbers: "11-17"


# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Preview 
tweets_summarized %>% 
  print(n = 15)
```
:::

## Step-by-step DTM creation {auto-animate="true"}

#### Along the tidy text pipeline: Tokenize ‚ñ∂Ô∏è Summarize ‚ñ∂Ô∏è [DTM]{.underline}

::: {style="font-size: smaller"}
```{r}
#| output-location: column
#| code-line-numbers: "15-20"


# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dtm <- tweets_summarized %>% 
  cast_dtm(tweet_id, text, n)

# Preview
tweets_dtm
```
:::

## Choose or combine styles

#### Simple with tidytext, precise with quanteda

::: {style="font-size: smaller"}
::: columns
::: {.column width="50%"}
```{r}
#| eval: false

# Common HTML entities
remove_reg <- "&amp;|&lt;|&gt;"

# Create tidy data
tweets_tidy <- tweets_detox %>% 
  mutate(
    text = str_remove_all(text, remove_reg)) %>% 
    tidytext::unnest_tokens("text", text) %>% 
    filter(!text %in% tidytext::stop_words$word)

# Create summarized data
tweets_summarized <- tweets_tidy %>% 
  count(tweet_id, text) 

# Create DTM
tweets_dtm <- tweets_summarized %>% 
  cast_dtm(tweet_id, text, n)

# Preview
tweets_dtm
```
:::

::: {.column width="50%"}
```{r}
#| output: false

# Create corpus
quanteda_corpus <- tweets_detox %>% 
  quanteda::corpus(
    docid_field = "tweet_id", 
    text_field = "text"
  )

# Tokenize
quanteda_token <- quanteda_corpus %>% 
  quanteda::tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE, 
    remove_numbers = TRUE, 
    remove_url = TRUE, 
    split_tags = FALSE # keep hashtags and mentions
  ) %>% 
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(
    pattern = stopwords("en")
    )

# Convert to Document-Feature-Matrix (DFM)
quanteda_dfm <- quanteda_token %>% 
  quanteda::dfm()
```
:::
:::
:::

## An exmaple: Co-occurence of hashtags {auto-animate="true"}

#### Comparison between tidytext & quanteda

::: {style="font-size: smaller"}
```{r}
#| output-location: column
#| fig-height: 9
#| fig-width: 9

# Extract hashtags
tweets_hashtags <- tweets_detox %>% 
  mutate(hashtags = str_extract_all(text, "#\\S+")) %>%
  unnest(hashtags) 

# Extract most common hashtags
top50_hashtags_tidy <- tweets_hashtags %>% 
  count(hashtags, sort = TRUE) %>% 
  slice_head(n = 50) %>% 
  pull(hashtags)

# Visualize
tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```
:::

## An exmaple: Network of hashtags {auto-animate="true"}

#### Comparison between tidytext & quanteda

::: {style="font-size: smaller"}
```{r}
#| output-location: column
#| fig-height: 9
#| fig-width: 9


# Extract DFM with only hashtags
quanteda_dfm_hashtags <- quanteda_dfm %>% 
  quanteda::dfm_select(pattern = "#*") 

# Extract most common hashtags 
top50_hashtags_quanteda <- quanteda_dfm_hashtags %>% 
  topfeatures(50) %>% 
  names()

# Construct feature-occurrence matrix of hashtags
quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  ) 
```
:::

## An exmaple: Network of hashtags

#### Comparison between tidytext & quanteda

::: columns
::: {.column width="50%"}
```{r}
#| code-fold: true
#| fig-height: 9
#| fig-width: 9

tweets_hashtags %>% 
  count(tweet_id, hashtags, sort = TRUE) %>% 
  cast_dfm(tweet_id, hashtags, n) %>% 
  quanteda::fcm() %>% 
  quanteda::fcm_select(
    pattern = top50_hashtags_tidy,
    case_insensitive = FALSE
    ) %>% 
  quanteda.textplots::textplot_network(
    edge_color = "#04316A"
  )
```
:::

::: {.column width="50%"}
```{r}
#| code-fold: true 
#| fig-height: 9
#| fig-width: 9

quanteda_dfm_hashtags %>% 
  fcm() %>% 
  fcm_select(pattern = top50_hashtags_quanteda) %>% 
  textplot_network(
    edge_color = "#C50F3C"
  )
```
:::
:::

## Let's do some modeling

#### Unsupervised learning example: Topic modeling

![](https://www.tidytextmining.com/images/tmwr_0601.png){fig-align="center"}

::: {style="text-align: center"}
@silge2017
:::

## Beyond LDA

#### Different topic modeling approaches

-   *Latent Dirichlet Allocation [`LDA`]* [@blei2003]is a probabilistic generative model that assumes each document in a corpus is a mix of topics and each word in the document is attributable to one of the document's topics.
-   **Structural Topic Modeling [`STM`]** [@roberts2016; @roberts2019] extends LDA by incorporating document-level covariates, allowing for the modeling of how external factors influence topic prevalence.
-   *Word embeddings* (`Word2Vec` [@mikolov2013] , `Glove` [@pennington2014]) represent words as continuous vectors in a high-dimensional space, capturing semantic relationships between words based on their context in the data.
-   *Topic Modeling with Neural Networks* (`BERTopic`[@devlin2019], `Doc2Vec`[@le2014]) leverages deep learning architectures to automatically learn latent topics from textual data.

::: notes
-  specify the presumed number of topics K thatyou expect to find in a corpus (e.g., K = 5, i.e., 5 topics)
-  the model then tries to inductively identify 5 topics in the corpus based on the distribution of frequently co-occurring features. 
-  an algorithm is used for this purpose, which is why topic modeling is a type of ‚Äúmachine learning‚Äù.
:::

## Preparation is everything

#### Suggested pre-processing steps (based on MAIER)

::: columns
::: {.column width="50%"}
1.  ‚ö†Ô∏è Deduplication;
2.  ‚úÖ tokenization;
3.  ‚úÖ transforming all characters to lowercase;
4.  ‚úÖ removing punctuation and special characters;
5.  ‚úÖ Removing stop-words;
6.  ‚ö†Ô∏è term unification (lemmatizing or stemming);
7.  üèóÔ∏è relative pruning (attributed to [Zipf‚Äôs law](https://en.wikipedia.org/wiki/Zipf%27s_law));
:::

::: {.column width="50%"}
```{r}
# Pruning
quanteda_dfm_trim <- quanteda_dfm %>% 
  dfm_trim( 
    min_docfreq = 10/nrow(tweets_detox),
    max_docfreq = 0.99, 
    docfreq_type = "prop")

# Convert from LDA topic modeling
quanteda_stm <- quanteda_dfm_trim %>% 
   convert(to = "stm")
```
:::
:::

::: notes
Zipf‚Äôs law states that the frequency that a word appears is inversely proportional to its rank.
:::


## How to find K
## The most important question of model selection

-   The choice of K (whether the model is instructed to identify 5, 15, or 100 topics), has a substantial impact on results: 
    -   The smaller K, the more fine-grained and usually the more exclusive topics; 
    -   the larger K, the more clearly topics identify individual events or issues.

-   The stm package has two build in solution to find the optimal K
    -   searchK()-function 
    -   setting K = 0 when estimating the model 

:::

::: notes
 However, with a larger K topics are oftentimes less exclusive, meaning that they somehow overlap. 
:::


## Add title here
#### Multiple statistics 

```{r}
#| eval: false
#| echo: false

tic()
stm_k <- searchK(
    quanteda_stm$documents, 
    quanteda_stm$vocab,
    K = c(5,10,15), 
    init.type = "Spectral",
    heldout.seed = 42)
toc()
```




#### Add text here

<!-- TODO Add LDA infos/best practise -->

```{r}
#| eval: false
#| echo: false


tweets_lda <- quanteda_dtm %>% 
  LDA(
    method = "Gibbs",
    k = 5,
    control = list(
      seed = 42))
```

## Top-Terms of topics

#### Explore TPM results

```{r}
#| eval: false
#| echo: false


tweets_lda %>% 
  terms(10) %>% 
  as.data.frame() %>% 
  gt() %>% 
  gtExtras::gt_theme_538()
```

## Title needed

#### Word-Topic-Probabilities

```{r}
#| eval: false
#| echo: false
#| code-fold: true 


tweets_lda_beta <- tweets_lda %>% 
  tidy(matrix = "beta") 

tweets_lda_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)  %>% 
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  scale_y_reordered() +
  theme_minimal()

```

## Title needed

#### Document-topic-probabilites

```{r}
#| eval: false
#| echo: false


tweets_topics <- tweets_lda %>% 
  topics() %>% 
  as.data.frame() %>% 
  janitor::clean_names() %>% 
  rownames_to_column(var = "tweet_id") %>% 
  rename(topic = x) 

tweets_topics %>% 
  data_tabulate(topic)
```

## Title needed

#### Join with original data

```{r}
#| eval: false
#| echo: false


tweets_lda_gamma <- tweets_lda %>% 
  tidy(matrix = "gamma") 

tweets_topics_full <- tweets_lda_gamma %>% 
  pivot_wider(
    names_from = topic, 
    names_glue = "gamma_t{topic}",
    values_from = gamma
  ) %>% 
  right_join(
    tweets_topics, .,
    by = join_by(tweet_id == document))

tweets_detox_topics <- tweets_topics_full %>% 
  right_join(
    tweets_detox, ., 
    by = join_by(tweet_id))
```

## Title needed

#### Distribution of topics over time

##### Distribution over time

```{r}
#| eval: false
#| echo: false


# Create first graph
g1 <- tweets_detox_topics %>% 
  ggplot(aes(x = as.factor(year), fill = as.factor(topic))) +
    geom_bar() +
    labs(
      x = "",
      y = "Number of tweets", 
      fill = "Topic"
     ) +
    theme_pubr() 

# Create second graph
g2 <- tweets_detox_topics %>% 
  ggplot(aes(x = as.factor(year), fill = as.factor(topic))) +
    geom_bar(position = "fill") +
    labs(
      x = "",
      y = "Number of tweets", 
      fill = "Topic"
     ) +
    theme_pubr() 

# COMBINE GRPAHS
ggarrange(g1, g2,
          nrow = 1, ncol = 2, 
          align = "hv",
          common.legend = TRUE) 
```

## Check out the topics

#### Most prominent tweets

```{r}
#| eval: false
#| echo: false


tweets_detox_topics %>% 
  arrange(-gamma_t1) %>% 
  slice_head(n = 10) %>% 
  select(tweet_id, gamma_t1, created_at, user_username, text) %>% 
  gt()
```

## Finishing remarks

#### Best practise and outlook

# üìã Hands on working with R {#group-activity background-image="img/slide_bg-group_activity.png"}

Various exercises on the content of today's session

## üß™ And now ... you: Clean and repeat!

#### Redo the tidy text analysis pipeline with cleaned data

::: {.callout-caution appearance="simple"}
## Objective of this exercise

-   Brush up basic knowledge of working with R and the tidyverse
-   Get to know the typical steps of tidy text analysis, from tokenisation and summarisation to visualisation.
:::

#### Next steps

-   Download files provided on StudOn or shared drives for the sessions
-   Unzip the archive at a destination of your choice.
-   Double click on the `Exercise-Text_as_data.Rproj` to open the RStudio project. This ensures that all dependencies are working correctly.
-   Open the `exercise.qmd` file and follow the instructions.
-   ***Tip: You can find all code chunks used in the slides in the showcase.qmd (for the raw code) or showcase.html (with rendered outputs).***

# Time for questions {background-image="img/slide_bg-question.png"}

# Thank you and see you next week! {background-image="img/slide_bg-end_session.png"}

## References

::: {#refs}
:::